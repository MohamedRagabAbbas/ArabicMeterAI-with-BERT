{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RS7lEDWlVuI",
        "outputId": "ed477ee7-5107-4010-89e6-af6f023a2e41"
      },
      "id": "3RS7lEDWlVuI",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "298035d4-5673-4977-8393-262f92cc7265",
      "metadata": {
        "id": "298035d4-5673-4977-8393-262f92cc7265"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f236de",
      "metadata": {
        "id": "53f236de"
      },
      "source": [
        "## Setting the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "ceb7b34e",
      "metadata": {
        "id": "ceb7b34e"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 128\n",
        "        self.max_seq_len = 56\n",
        "        self.input_dim = 512\n",
        "        self.d_model = 512\n",
        "        self.num_heads = 8\n",
        "        self.ffn_hidden = 2048\n",
        "        self.num_layers = 6\n",
        "        self.dropout_rate = 0.1\n",
        "        self.learning_rate = 3e-4\n",
        "        self.num_epochs = 1\n",
        "        self.vocab_size = 50304\n",
        "        self.decoder_vocab_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "7b704ec8",
      "metadata": {
        "id": "7b704ec8"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b2d3ac",
      "metadata": {
        "id": "33b2d3ac"
      },
      "source": [
        "## Multihead Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "deac3059",
      "metadata": {
        "id": "deac3059"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q,k,v,config=Config(),mask=None,):\n",
        "    d_k = q.size(-1)\n",
        "    qk = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        qk = qk.permute(1, 0, 2, 3) + mask\n",
        "        qk = qk.permute(1, 0, 2, 3)\n",
        "    qk = F.softmax(qk, dim=-1)\n",
        "    new_qkv = torch.matmul(qk, v)\n",
        "    return new_qkv\n",
        "\n",
        "class Multihead_Self_Attention(nn.Module):\n",
        "    def __init__(self,input_dim, d_model, num_heads, config=Config()):\n",
        "        super(Multihead_Self_Attention, self).__init__()\n",
        "        self.config = config\n",
        "        self.input_dim = input_dim\n",
        "        self.model_dim = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.model_dim // self.num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim, 3 * self.model_dim)\n",
        "        self.concat_layer = nn.Linear(self.model_dim, self.model_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        qkv = self.qkv_layer(x)\n",
        "        a,b,c = qkv.size()\n",
        "        qkv = qkv.view(self.config.batch_size,self.config.max_seq_len,self.num_heads,3*self.head_dim)\n",
        "        qkv = qkv.permute(0,2,1,3)\n",
        "        q,k,v = qkv.chunk(3,dim=-1)\n",
        "        new_qkv = scaled_dot_product_attention(q,k,v,mask)\n",
        "        new_qkv = new_qkv.permute(0,2,1,3)\n",
        "        new_qkv = new_qkv.reshape(self.config.batch_size,self.config.max_seq_len,self.model_dim)\n",
        "        out = self.concat_layer(new_qkv)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a55ca48",
      "metadata": {
        "id": "0a55ca48"
      },
      "source": [
        "## Multihead Cross Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7ee818ea",
      "metadata": {
        "id": "7ee818ea"
      },
      "outputs": [],
      "source": [
        "class Multihead_Cross_Attention(nn.Module):\n",
        "    def __init__(self,input_dim, model_dim, num_heads, config=Config()):\n",
        "        super(Multihead_Cross_Attention, self).__init__()\n",
        "        self.config = config\n",
        "        self.input_dim = input_dim\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = model_dim // num_heads\n",
        "        self.qk_layer = nn.Linear(input_dim, 2 * model_dim)\n",
        "        self.v_layer = nn.Linear(input_dim, model_dim)\n",
        "        self.concat_layer = nn.Linear(model_dim, model_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self,x,y,mask=None):\n",
        "        x_B,x_T,x_C = x.size()\n",
        "        y_B,y_T,y_C = y.size()\n",
        "        qk = self.qk_layer(x)\n",
        "        v = self.v_layer(y)\n",
        "        qk = qk.view(self.config.batch_size,self.config.max_seq_len,self.config.num_heads,2*self.head_dim)\n",
        "        v = v.view(self.config.batch_size,self.config.max_seq_len,self.config.num_heads,self.head_dim)\n",
        "        qk = qk.permute(0,2,1,3)\n",
        "        v = v.permute(0,2,1,3)\n",
        "        q,k = qk.chunk(2,dim=-1)\n",
        "        new_qkv = scaled_dot_product_attention(q,k,v,mask)\n",
        "        new_qkv = new_qkv.permute(0,2,1,3)\n",
        "        new_qkv = new_qkv.reshape(self.config.batch_size,self.config.max_seq_len,self.model_dim)\n",
        "        out = self.concat_layer(new_qkv)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f160b031",
      "metadata": {
        "id": "f160b031"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "5f0c6c28",
      "metadata": {
        "id": "5f0c6c28"
      },
      "outputs": [],
      "source": [
        "class PostionalEncoding(nn.Module):\n",
        "    def __init__(self,max_seq_len,d_model):\n",
        "        super(PostionalEncoding,self).__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.d_model = d_model\n",
        "        self.encoding = torch.zeros(self.max_seq_len,self.d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        even_index = torch.arange(0,self.d_model,2).float()\n",
        "        domenator = torch.pow(10000,even_index/self.d_model)\n",
        "        position = torch.arange(0,self.max_seq_len).unsqueeze(1)\n",
        "        PE_even = torch.sin(position/domenator)\n",
        "        PE_odd = torch.cos(position/domenator)\n",
        "        stacked = torch.stack([PE_even,PE_odd],dim=2)\n",
        "        PE_flatten = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
        "        return PE_flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfde7de",
      "metadata": {
        "id": "cdfde7de"
      },
      "source": [
        "## Normalization Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "9ed90324",
      "metadata": {
        "id": "9ed90324"
      },
      "outputs": [],
      "source": [
        "class NormalizationLayer(nn.Module):\n",
        "    def __init__(self, parameter_dim):\n",
        "        super(NormalizationLayer, self).__init__()\n",
        "        self.parameters_shape = parameter_dim\n",
        "        self.gamma = nn.Parameter(torch.ones(parameter_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameter_dim))\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def forward(self, x):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = x.mean(dims, keepdim=True)\n",
        "        std = x.std(dims, keepdim=True)\n",
        "        out = self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47adc7b1",
      "metadata": {
        "id": "47adc7b1"
      },
      "source": [
        "## Position-wise Feed-Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "31a2a97c",
      "metadata": {
        "id": "31a2a97c"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,d_model,ffn_hidden,config=Config()):\n",
        "        super(FeedForward,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.layer1 = nn.Linear(self.d_model,self.ffn_hidden)\n",
        "        self.layer2 = nn.Linear(self.ffn_hidden,self.d_model)\n",
        "        self.dropout = nn.Dropout(p=config.dropout_rate)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layer1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1bf3a1",
      "metadata": {
        "id": "fb1bf3a1"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "04876c6d",
      "metadata": {
        "id": "04876c6d"
      },
      "outputs": [],
      "source": [
        "class Encoder_Layer(nn.Module):\n",
        "    def __init__(self,num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate,config=Config()):\n",
        "        super(Encoder_Layer,self).__init__()\n",
        "        self.multihead_attention = Multihead_Self_Attention(config.input_dim,d_model,num_heads)\n",
        "        self.pos_encoding = PostionalEncoding(max_seq_len,d_model)\n",
        "        self.feedforward = FeedForward(d_model,ffn_hidden)\n",
        "        self.norm1 = NormalizationLayer([d_model])\n",
        "        self.norm2 = NormalizationLayer([d_model])\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        reseduial_x = x\n",
        "        x = self.multihead_attention(x,mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = x + reseduial_x\n",
        "        x = self.norm1(x)\n",
        "        reseduial_x = x\n",
        "        x = self.feedforward(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x + reseduial_x\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "1f758d6e",
      "metadata": {
        "id": "1f758d6e"
      },
      "outputs": [],
      "source": [
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "5ac0aa5b",
      "metadata": {
        "id": "5ac0aa5b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, dropout_rate, num_layers,max_seq_len):\n",
        "        super().__init__()\n",
        "        self.layers = SequentialEncoder(*[Encoder_Layer(num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate)\n",
        "                                     for _ in range(num_layers)])\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.layers(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c241c7b",
      "metadata": {
        "id": "3c241c7b"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "8d18f409",
      "metadata": {
        "id": "8d18f409"
      },
      "outputs": [],
      "source": [
        "class Decoder_Layer(nn.Module):\n",
        "    def __init__(self,num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate,config=Config()):\n",
        "        super(Decoder_Layer,self).__init__()\n",
        "        self.multihead_self_attention =  Multihead_Self_Attention(config.input_dim,d_model,num_heads)\n",
        "        self.multihead_cross_attention = Multihead_Cross_Attention(config.input_dim,d_model,num_heads)\n",
        "        self.feedforward = FeedForward(d_model,ffn_hidden)\n",
        "        self.norm1 = NormalizationLayer([d_model])\n",
        "        self.norm2 = NormalizationLayer([d_model])\n",
        "        self.norm3 = NormalizationLayer([d_model])\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask=None, cross_attention_mask=None):\n",
        "        reseduial_y = y\n",
        "        y = self.multihead_self_attention(y,mask = self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = y + reseduial_y\n",
        "        y = self.norm1(y)\n",
        "\n",
        "        reseduial_y = y\n",
        "        y = self.multihead_cross_attention(x,y,mask = cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = y + reseduial_y\n",
        "        y = self.norm2(y)\n",
        "\n",
        "\n",
        "        reseduial_y = y\n",
        "        y = self.feedforward(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = y + reseduial_y\n",
        "        y = self.norm3(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "dfafde66",
      "metadata": {
        "id": "dfafde66"
      },
      "outputs": [],
      "source": [
        "class Sequential_Decoder(nn.Sequential):\n",
        "    def forward(self,*input):\n",
        "        x, y, self_attention_mask, cross_attention_mask = input\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "8612abe6",
      "metadata": {
        "id": "8612abe6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, dropout_rate, num_layers,max_seq_len):\n",
        "        super().__init__()\n",
        "        self.layers = Sequential_Decoder(*[Decoder_Layer(num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate)\n",
        "                                     for _ in range(num_layers)])\n",
        "    def forward(self, x, y,self_attention_mask=None, cross_attention_mask=None):\n",
        "        y = self.layers(x, y,self_attention_mask, cross_attention_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "de915ee3",
      "metadata": {
        "id": "de915ee3"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(ar_batch, meter_batch,config=Config()):\n",
        "    num_sentences = len(ar_batch)\n",
        "    look_ahead_mask = torch.full([config.max_seq_len, config.max_seq_len] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, config.max_seq_len, config.max_seq_len] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, config.max_seq_len, config.max_seq_len] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, config.max_seq_len, config.max_seq_len] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      ar_sentence_length, meter_sentence_length = len(ar_batch[idx]), len(meter_batch[idx])\n",
        "      ar_chars_to_padding_mask = np.arange(ar_sentence_length + 1, config.max_seq_len)\n",
        "      meter_chars_to_padding_mask = np.arange(meter_sentence_length + 1, config.max_seq_len)\n",
        "      encoder_padding_mask[idx, :, ar_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, ar_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, meter_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, meter_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, ar_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, meter_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64e0d20",
      "metadata": {
        "id": "b64e0d20"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3493683d",
      "metadata": {
        "id": "3493683d"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "7ecccfa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ecccfa0",
        "outputId": "8a91fbac-bbb9-42eb-f308-2196a11258b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[220]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "tiktoken.get_encoding('gpt2').encode(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "aff4d4c1",
      "metadata": {
        "id": "aff4d4c1"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.enc_sentence_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.enc_positional_encoding = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.encoder = Encoder(config.d_model, config.ffn_hidden, config.num_heads, config.dropout_rate, config.num_layers, config.max_seq_len)\n",
        "\n",
        "        self.dec_sentence_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.dec_positional_encoding = nn.Embedding(config.max_seq_len, config.d_model)\n",
        "        self.decoder = Decoder(config.d_model, config.ffn_hidden, config.num_heads, config.dropout_rate, config.num_layers, config.max_seq_len)\n",
        "\n",
        "        self.linear = nn.Linear(config.d_model, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        target = y\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(x, y, self.config)\n",
        "        encoder_self_attention_mask = encoder_self_attention_mask.to(get_device())\n",
        "        decoder_self_attention_mask = decoder_self_attention_mask.to(get_device())\n",
        "        decoder_cross_attention_mask = decoder_cross_attention_mask.to(get_device())\n",
        "\n",
        "        B, T = x.size()\n",
        "        pos = torch.arange(0, self.config.max_seq_len, dtype=torch.long, device=x.device) # shape (T)\n",
        "        pos_emb = self.enc_positional_encoding(pos) # position embeddings of shape (T, n_embd)\n",
        "        embedding = self.enc_sentence_embedding(x)\n",
        "        # add padding to embedding by adding space at the end of it with token (220) to reach the max_seq_len\n",
        "        #embedding = torch.cat((embedding, torch.zeros(B, self.config.max_seq_len - T, self.config.d_model).to(get_device())), dim=1)\n",
        "        x = embedding + pos_emb\n",
        "        x = self.encoder(x, encoder_self_attention_mask)\n",
        "\n",
        "        B, T = y.size()\n",
        "        pos = torch.arange(0, self.config.max_seq_len, dtype=torch.long, device=y.device)\n",
        "        pos_emb = self.dec_positional_encoding(pos)\n",
        "        embedding = self.dec_sentence_embedding(y)\n",
        "        y = embedding + pos_emb\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
        "\n",
        "        out = self.linear(out)\n",
        "        # return logits and loss for training\n",
        "        loss = F.cross_entropy(out.view(-1, out.size(-1)), target.view(-1),ignore_index=220)\n",
        "        return out, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1dba32",
      "metadata": {
        "id": "ef1dba32"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "a1ddd323",
      "metadata": {
        "id": "a1ddd323"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, phrase, meter):\n",
        "        self.phrase = phrase\n",
        "        self.meter = meter\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.phrase)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.phrase[idx], self.meter[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "55a3ca50",
      "metadata": {
        "id": "55a3ca50"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    return tiktoken.get_encoding('gpt2').encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "c400b11a",
      "metadata": {
        "id": "c400b11a"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    return tiktoken.get_encoding('gpt2').encode(text)\n",
        "def decoder(text):\n",
        "    return tiktoken.get_encoding('gpt2').decode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "82de77d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "82de77d7",
        "outputId": "34c6681b-c1d5-4c53-ac10-fc338f8897bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' UYYYYY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "decoder([220,52,56,56,56,56,56])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e84391e",
      "metadata": {
        "id": "8e84391e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QWLc3fbuocdW"
      },
      "id": "QWLc3fbuocdW"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qB6cVrOTocuK"
      },
      "id": "qB6cVrOTocuK"
    },
    {
      "cell_type": "code",
      "source": [
        "meter = pd.read_csv('meter.csv',nrows=1000000)"
      ],
      "metadata": {
        "id": "qFCxLPSznYUq"
      },
      "id": "qFCxLPSznYUq",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meter.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sBtxa-qwnZGX",
        "outputId": "f29ac6f8-02af-4c7a-9818-f2d8bc43c4cb"
      },
      "id": "sBtxa-qwnZGX",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    الشطر   البحر\n",
              "0   خَليلَيَّ لا تَستَعجِلا أَن تَزَوَّدا  الطويل\n",
              "1     فَما لَبَثٌ يَوماً بِسابِقٍ مَغنَمٍ  الطويل\n",
              "2  وَإِن تُنظِراني اليَومَ أَقضِ لُبانَةً  الطويل\n",
              "3      لَعَمرُكَ ما نَفسٌ بِجِدٍ رَشيدَةٍ  الطويل\n",
              "4    وَإِن ظَهَرَت مِنهُ قَوارِصُ جَمَّةٌ  الطويل"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb7f6235-29e8-44c5-9f16-eb11c5bfdff9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>الشطر</th>\n",
              "      <th>البحر</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>خَليلَيَّ لا تَستَعجِلا أَن تَزَوَّدا</td>\n",
              "      <td>الطويل</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>فَما لَبَثٌ يَوماً بِسابِقٍ مَغنَمٍ</td>\n",
              "      <td>الطويل</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>وَإِن تُنظِراني اليَومَ أَقضِ لُبانَةً</td>\n",
              "      <td>الطويل</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>لَعَمرُكَ ما نَفسٌ بِجِدٍ رَشيدَةٍ</td>\n",
              "      <td>الطويل</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>وَإِن ظَهَرَت مِنهُ قَوارِصُ جَمَّةٌ</td>\n",
              "      <td>الطويل</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb7f6235-29e8-44c5-9f16-eb11c5bfdff9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb7f6235-29e8-44c5-9f16-eb11c5bfdff9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb7f6235-29e8-44c5-9f16-eb11c5bfdff9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f562dafe-8510-4c83-95fc-5e86780e3768\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f562dafe-8510-4c83-95fc-5e86780e3768')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f562dafe-8510-4c83-95fc-5e86780e3768 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "meter"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "61019c2f",
      "metadata": {
        "id": "61019c2f"
      },
      "outputs": [],
      "source": [
        "shadr = [tokenizer(tok) for tok in meter['الشطر']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meters = [tokenizer(tok) for tok in meter['البحر']]"
      ],
      "metadata": {
        "id": "Lt8GZd40n4S6"
      },
      "id": "Lt8GZd40n4S6",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "d30d6e16",
      "metadata": {
        "id": "d30d6e16"
      },
      "outputs": [],
      "source": [
        "class create_batch:\n",
        "    def __init__(self, shadr,meters,batch_size,max_seq_len):\n",
        "        self.shadr = shadr\n",
        "        self.meters = meters\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.pointer = 0\n",
        "    def next_batch(self):\n",
        "        if self.pointer + self.batch_size > len(self.shadr):\n",
        "            self.pointer = 0\n",
        "        start = self.pointer\n",
        "        end = self.pointer + self.batch_size\n",
        "        batch_shadr = self.shadr[start:end]\n",
        "        batch_meters = self.meters[start:end]\n",
        "        self.pointer += self.batch_size\n",
        "        # loop over the batch and pad the sentences to the max_seq_len with the token 220\n",
        "        for i in range(len(batch_shadr)):\n",
        "            if len(batch_shadr[i]) < self.max_seq_len:\n",
        "                batch_shadr[i] += [220] * (self.max_seq_len - len(batch_shadr[i]))\n",
        "            else:\n",
        "                batch_shadr[i] = batch_shadr[i][:self.max_seq_len]\n",
        "        for i in range(len(batch_meters)):\n",
        "            if len(batch_meters[i]) < self.max_seq_len:\n",
        "                batch_meters[i] += [220] * (self.max_seq_len - len(batch_meters[i]))\n",
        "            else:\n",
        "                batch_meters[i] = batch_meters[i][:self.max_seq_len]\n",
        "        return batch_shadr, batch_meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "dc061b48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc061b48",
        "outputId": "7cdb0527-f0d1-473c-c0da-0b099e5474c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration : 2813 loss: 0.5891317129135132\n",
            "iteration : 2814 loss: 1.6805458068847656\n",
            "iteration : 2815 loss: 1.8120754957199097\n",
            "iteration : 2816 loss: 1.591102957725525\n",
            "iteration : 2817 loss: 1.5710046291351318\n",
            "iteration : 2818 loss: 1.096391201019287\n",
            "iteration : 2819 loss: 1.4825377464294434\n",
            "iteration : 2820 loss: 1.582400918006897\n",
            "iteration : 2821 loss: 0.845054030418396\n",
            "iteration : 2822 loss: 0.6848943829536438\n",
            "iteration : 2823 loss: 1.1076363325119019\n",
            "iteration : 2824 loss: 1.313017725944519\n",
            "iteration : 2825 loss: 0.9258073568344116\n",
            "iteration : 2826 loss: 0.9749688506126404\n",
            "iteration : 2827 loss: 0.8350116014480591\n",
            "iteration : 2828 loss: 0.701933741569519\n",
            "iteration : 2829 loss: 0.6784744262695312\n",
            "iteration : 2830 loss: 0.5967182517051697\n",
            "iteration : 2831 loss: 0.5004682540893555\n",
            "iteration : 2832 loss: 0.3217141628265381\n",
            "iteration : 2833 loss: 0.20874951779842377\n",
            "iteration : 2834 loss: 0.8996937274932861\n",
            "iteration : 2835 loss: 0.8479467034339905\n",
            "iteration : 2836 loss: 0.9880697131156921\n",
            "iteration : 2837 loss: 0.4399450719356537\n",
            "iteration : 2838 loss: 0.3623540699481964\n",
            "iteration : 2839 loss: 0.4385477602481842\n",
            "iteration : 2840 loss: 0.2723207473754883\n",
            "iteration : 2841 loss: 0.5340360999107361\n",
            "iteration : 2842 loss: 0.6603918075561523\n",
            "iteration : 2843 loss: 0.6354153752326965\n",
            "iteration : 2844 loss: 0.701836347579956\n",
            "iteration : 2845 loss: 0.3975447416305542\n",
            "iteration : 2846 loss: 0.7093290686607361\n",
            "iteration : 2847 loss: 0.7042170166969299\n",
            "iteration : 2848 loss: 0.468781054019928\n",
            "iteration : 2849 loss: 0.5791155695915222\n",
            "iteration : 2850 loss: 0.7055502533912659\n",
            "iteration : 2851 loss: 0.40430212020874023\n",
            "iteration : 2852 loss: 0.5446446537971497\n",
            "iteration : 2853 loss: 0.6443405151367188\n",
            "iteration : 2854 loss: 0.434260755777359\n",
            "iteration : 2855 loss: 0.7115781307220459\n",
            "iteration : 2856 loss: 0.6782676577568054\n",
            "iteration : 2857 loss: 0.5450172424316406\n",
            "iteration : 2858 loss: 0.37982121109962463\n",
            "iteration : 2859 loss: 0.3119069039821625\n",
            "iteration : 2860 loss: 0.4625323414802551\n",
            "iteration : 2861 loss: 0.5106668472290039\n",
            "iteration : 2862 loss: 0.517827033996582\n",
            "iteration : 2863 loss: 0.4505637586116791\n",
            "iteration : 2864 loss: 0.3110921382904053\n",
            "iteration : 2865 loss: 0.2317408323287964\n",
            "iteration : 2866 loss: 1.33307945728302\n",
            "iteration : 2867 loss: 0.35124167799949646\n",
            "iteration : 2868 loss: 0.4855727553367615\n",
            "iteration : 2869 loss: 0.6629168391227722\n",
            "iteration : 2870 loss: 0.3811546266078949\n",
            "iteration : 2871 loss: 0.36280500888824463\n",
            "iteration : 2872 loss: 0.27273592352867126\n",
            "iteration : 2873 loss: 0.15888267755508423\n",
            "iteration : 2874 loss: 0.2559555768966675\n",
            "iteration : 2875 loss: 0.43680256605148315\n",
            "iteration : 2876 loss: 0.4967149496078491\n",
            "iteration : 2877 loss: 0.8175738453865051\n",
            "iteration : 2878 loss: 0.40934789180755615\n",
            "iteration : 2879 loss: 0.41413354873657227\n",
            "iteration : 2880 loss: 0.5060045123100281\n",
            "iteration : 2881 loss: 0.25476309657096863\n",
            "iteration : 2882 loss: 1.1431169509887695\n",
            "iteration : 2883 loss: 0.6402037739753723\n",
            "iteration : 2884 loss: 0.5623263120651245\n",
            "iteration : 2885 loss: 0.21968799829483032\n",
            "iteration : 2886 loss: 0.798571765422821\n",
            "iteration : 2887 loss: 0.3254268169403076\n",
            "iteration : 2888 loss: 0.5805408358573914\n",
            "iteration : 2889 loss: 0.581812858581543\n",
            "iteration : 2890 loss: 0.4319721460342407\n",
            "iteration : 2891 loss: 0.27703648805618286\n",
            "iteration : 2892 loss: 0.47487595677375793\n",
            "iteration : 2893 loss: 1.2205146551132202\n",
            "iteration : 2894 loss: 0.9287176132202148\n",
            "iteration : 2895 loss: 1.9700368642807007\n",
            "iteration : 2896 loss: 0.5560722947120667\n",
            "iteration : 2897 loss: 0.8916840553283691\n",
            "iteration : 2898 loss: 0.5219027996063232\n",
            "iteration : 2899 loss: 0.8558664917945862\n",
            "iteration : 2900 loss: 0.893782913684845\n",
            "iteration : 2901 loss: 0.4374891221523285\n",
            "iteration : 2902 loss: 2.7991535663604736\n",
            "iteration : 2903 loss: 2.8123624324798584\n",
            "iteration : 2904 loss: 2.3609273433685303\n",
            "iteration : 2905 loss: 2.2380664348602295\n",
            "iteration : 2906 loss: 1.5992182493209839\n",
            "iteration : 2907 loss: 1.9894030094146729\n",
            "iteration : 2908 loss: 2.268874406814575\n",
            "iteration : 2909 loss: 2.130647897720337\n",
            "iteration : 2910 loss: 2.138073205947876\n",
            "iteration : 2911 loss: 2.0552210807800293\n",
            "iteration : 2912 loss: 1.9108864068984985\n",
            "iteration : 2913 loss: 1.563867211341858\n",
            "iteration : 2914 loss: 2.141784906387329\n",
            "iteration : 2915 loss: 2.4390220642089844\n",
            "iteration : 2916 loss: 1.8621217012405396\n",
            "iteration : 2917 loss: 2.488567590713501\n",
            "iteration : 2918 loss: 1.9857885837554932\n",
            "iteration : 2919 loss: 1.832495927810669\n",
            "iteration : 2920 loss: 2.132601261138916\n",
            "iteration : 2921 loss: 1.992050290107727\n",
            "iteration : 2922 loss: 1.7796061038970947\n",
            "iteration : 2923 loss: 1.685721516609192\n",
            "iteration : 2924 loss: 1.709584355354309\n",
            "iteration : 2925 loss: 2.3168110847473145\n",
            "iteration : 2926 loss: 1.8964722156524658\n",
            "iteration : 2927 loss: 2.018824338912964\n",
            "iteration : 2928 loss: 1.9364205598831177\n",
            "iteration : 2929 loss: 1.6094952821731567\n",
            "iteration : 2930 loss: 1.3747899532318115\n",
            "iteration : 2931 loss: 1.325365424156189\n",
            "iteration : 2932 loss: 1.2567319869995117\n",
            "iteration : 2933 loss: 1.7494372129440308\n",
            "iteration : 2934 loss: 1.3041900396347046\n",
            "iteration : 2935 loss: 1.065029263496399\n",
            "iteration : 2936 loss: 1.3322031497955322\n",
            "iteration : 2937 loss: 0.8954843282699585\n",
            "iteration : 2938 loss: 1.3343616724014282\n",
            "iteration : 2939 loss: 1.0937882661819458\n",
            "iteration : 2940 loss: 1.0475889444351196\n",
            "iteration : 2941 loss: 2.132049322128296\n",
            "iteration : 2942 loss: 1.835018515586853\n",
            "iteration : 2943 loss: 1.8704357147216797\n",
            "iteration : 2944 loss: 1.4601129293441772\n",
            "iteration : 2945 loss: 1.354901909828186\n",
            "iteration : 2946 loss: 1.1599335670471191\n",
            "iteration : 2947 loss: 0.9663938879966736\n",
            "iteration : 2948 loss: 0.8365707397460938\n",
            "iteration : 2949 loss: 1.9376592636108398\n",
            "iteration : 2950 loss: 1.0481771230697632\n",
            "iteration : 2951 loss: 0.8231841921806335\n",
            "iteration : 2952 loss: 0.6848773956298828\n",
            "iteration : 2953 loss: 1.4070022106170654\n",
            "iteration : 2954 loss: 0.45922374725341797\n",
            "iteration : 2955 loss: 0.7948060035705566\n",
            "iteration : 2956 loss: 1.3751012086868286\n",
            "iteration : 2957 loss: 2.870302438735962\n",
            "iteration : 2958 loss: 1.6180949211120605\n",
            "iteration : 2959 loss: 1.9047296047210693\n",
            "iteration : 2960 loss: 2.0205554962158203\n",
            "iteration : 2961 loss: 2.425302743911743\n",
            "iteration : 2962 loss: 1.678835391998291\n",
            "iteration : 2963 loss: 1.938708782196045\n",
            "iteration : 2964 loss: 1.6867012977600098\n",
            "iteration : 2965 loss: 1.3893553018569946\n",
            "iteration : 2966 loss: 1.9363261461257935\n",
            "iteration : 2967 loss: 0.7846976518630981\n",
            "iteration : 2968 loss: 0.8998754024505615\n",
            "iteration : 2969 loss: 0.7001955509185791\n",
            "iteration : 2970 loss: 0.8066310286521912\n",
            "iteration : 2971 loss: 1.2812715768814087\n",
            "iteration : 2972 loss: 1.3846180438995361\n",
            "iteration : 2973 loss: 1.7500169277191162\n",
            "iteration : 2974 loss: 1.0393223762512207\n",
            "iteration : 2975 loss: 1.9927046298980713\n",
            "iteration : 2976 loss: 1.1860063076019287\n",
            "iteration : 2977 loss: 1.4282382726669312\n",
            "iteration : 2978 loss: 1.447105884552002\n",
            "iteration : 2979 loss: 1.3787604570388794\n",
            "iteration : 2980 loss: 0.8534241318702698\n",
            "iteration : 2981 loss: 0.7320636510848999\n",
            "iteration : 2982 loss: 0.8114919662475586\n",
            "iteration : 2983 loss: 1.361014485359192\n",
            "iteration : 2984 loss: 2.425386428833008\n",
            "iteration : 2985 loss: 1.292115330696106\n",
            "iteration : 2986 loss: 1.121948480606079\n",
            "iteration : 2987 loss: 1.0020095109939575\n",
            "iteration : 2988 loss: 1.353748083114624\n",
            "iteration : 2989 loss: 0.8858874440193176\n",
            "iteration : 2990 loss: 0.9640447497367859\n",
            "iteration : 2991 loss: 0.9166461229324341\n",
            "iteration : 2992 loss: 1.0610328912734985\n",
            "iteration : 2993 loss: 0.89755779504776\n",
            "iteration : 2994 loss: 0.8386771082878113\n",
            "iteration : 2995 loss: 1.8917372226715088\n",
            "iteration : 2996 loss: 0.9278457164764404\n",
            "iteration : 2997 loss: 0.6905272603034973\n",
            "iteration : 2998 loss: 0.7753847241401672\n",
            "iteration : 2999 loss: 1.5686701536178589\n",
            "iteration : 3000 loss: 1.3804473876953125\n",
            "iteration : 3001 loss: 0.8512296676635742\n",
            "iteration : 3002 loss: 1.751895546913147\n",
            "iteration : 3003 loss: 1.9585660696029663\n",
            "iteration : 3004 loss: 1.3439967632293701\n",
            "iteration : 3005 loss: 2.993054151535034\n",
            "iteration : 3006 loss: 2.615246534347534\n",
            "iteration : 3007 loss: 1.964215874671936\n",
            "iteration : 3008 loss: 2.9158661365509033\n",
            "iteration : 3009 loss: 1.7335915565490723\n",
            "iteration : 3010 loss: 1.2647267580032349\n",
            "iteration : 3011 loss: 1.1474529504776\n",
            "iteration : 3012 loss: 1.899040699005127\n",
            "iteration : 3013 loss: 4.136070728302002\n",
            "iteration : 3014 loss: 3.9697678089141846\n",
            "iteration : 3015 loss: 2.1898610591888428\n",
            "iteration : 3016 loss: 2.9552042484283447\n",
            "iteration : 3017 loss: 3.224680185317993\n",
            "iteration : 3018 loss: 2.5623037815093994\n",
            "iteration : 3019 loss: 2.750382423400879\n",
            "iteration : 3020 loss: 1.9618160724639893\n",
            "iteration : 3021 loss: 1.8710297346115112\n",
            "iteration : 3022 loss: 1.947420597076416\n",
            "iteration : 3023 loss: 2.9085099697113037\n",
            "iteration : 3024 loss: 2.313702344894409\n",
            "iteration : 3025 loss: 1.8294556140899658\n",
            "iteration : 3026 loss: 1.5240976810455322\n",
            "iteration : 3027 loss: 2.985243320465088\n",
            "iteration : 3028 loss: 1.4634757041931152\n",
            "iteration : 3029 loss: 2.252732992172241\n",
            "iteration : 3030 loss: 1.3555958271026611\n",
            "iteration : 3031 loss: 1.228710651397705\n",
            "iteration : 3032 loss: 2.53928279876709\n",
            "iteration : 3033 loss: 1.830479383468628\n",
            "iteration : 3034 loss: 1.3191622495651245\n",
            "iteration : 3035 loss: 1.2889705896377563\n",
            "iteration : 3036 loss: 1.8068313598632812\n",
            "iteration : 3037 loss: 1.6251946687698364\n",
            "iteration : 3038 loss: 1.0609971284866333\n",
            "iteration : 3039 loss: 2.501798391342163\n",
            "iteration : 3040 loss: 0.8493886590003967\n",
            "iteration : 3041 loss: 1.877269983291626\n",
            "iteration : 3042 loss: 1.3517931699752808\n",
            "iteration : 3043 loss: 0.8439236879348755\n",
            "iteration : 3044 loss: 0.939835250377655\n",
            "iteration : 3045 loss: 0.7310285568237305\n",
            "iteration : 3046 loss: 0.9794712662696838\n",
            "iteration : 3047 loss: 1.6390324831008911\n",
            "iteration : 3048 loss: 1.440078854560852\n",
            "iteration : 3049 loss: 0.6729918122291565\n",
            "iteration : 3050 loss: 0.6493855714797974\n",
            "iteration : 3051 loss: 1.6870319843292236\n",
            "iteration : 3052 loss: 0.44471338391304016\n",
            "iteration : 3053 loss: 1.418656349182129\n",
            "iteration : 3054 loss: 1.3616288900375366\n",
            "iteration : 3055 loss: 0.679178774356842\n",
            "iteration : 3056 loss: 0.5801368355751038\n",
            "iteration : 3057 loss: 1.9192880392074585\n",
            "iteration : 3058 loss: 0.8965610265731812\n",
            "iteration : 3059 loss: 0.3775668442249298\n",
            "iteration : 3060 loss: 0.44238367676734924\n",
            "iteration : 3061 loss: 1.516093134880066\n",
            "iteration : 3062 loss: 1.9502261877059937\n",
            "iteration : 3063 loss: 1.411985158920288\n",
            "iteration : 3064 loss: 1.529608130455017\n",
            "iteration : 3065 loss: 0.49570223689079285\n",
            "iteration : 3066 loss: 0.6651129126548767\n",
            "iteration : 3067 loss: 0.8341138958930969\n",
            "iteration : 3068 loss: 1.063312292098999\n",
            "iteration : 3069 loss: 0.424368292093277\n",
            "iteration : 3070 loss: 1.2954615354537964\n",
            "iteration : 3071 loss: 0.5777742266654968\n",
            "iteration : 3072 loss: 0.67787104845047\n",
            "iteration : 3073 loss: 1.1155155897140503\n",
            "iteration : 3074 loss: 0.5426888465881348\n",
            "iteration : 3075 loss: 0.39167821407318115\n",
            "iteration : 3076 loss: 0.8033159375190735\n",
            "iteration : 3077 loss: 0.7516651153564453\n",
            "iteration : 3078 loss: 0.5728417634963989\n",
            "iteration : 3079 loss: 0.566472589969635\n",
            "iteration : 3080 loss: 0.6172469258308411\n",
            "iteration : 3081 loss: 0.1868121474981308\n",
            "iteration : 3082 loss: 0.833172619342804\n",
            "iteration : 3083 loss: 0.9250121116638184\n",
            "iteration : 3084 loss: 0.7917056679725647\n",
            "iteration : 3085 loss: 1.1143417358398438\n",
            "iteration : 3086 loss: 0.6579265594482422\n",
            "iteration : 3087 loss: 1.018178105354309\n",
            "iteration : 3088 loss: 0.7112651467323303\n",
            "iteration : 3089 loss: 1.1575486660003662\n",
            "iteration : 3090 loss: 0.5605798959732056\n",
            "iteration : 3091 loss: 0.6026113629341125\n",
            "iteration : 3092 loss: 0.7469901442527771\n",
            "iteration : 3093 loss: 0.4318869113922119\n",
            "iteration : 3094 loss: 1.3239272832870483\n",
            "iteration : 3095 loss: 0.4910926818847656\n",
            "iteration : 3096 loss: 0.47352904081344604\n",
            "iteration : 3097 loss: 0.34239789843559265\n",
            "iteration : 3098 loss: 1.5791254043579102\n",
            "iteration : 3099 loss: 0.8476169109344482\n",
            "iteration : 3100 loss: 1.392045021057129\n",
            "iteration : 3101 loss: 0.7946444749832153\n",
            "iteration : 3102 loss: 0.7862082123756409\n",
            "iteration : 3103 loss: 0.7841716408729553\n",
            "iteration : 3104 loss: 0.8736883401870728\n",
            "iteration : 3105 loss: 1.3775757551193237\n",
            "iteration : 3106 loss: 1.3100836277008057\n",
            "iteration : 3107 loss: 0.5620136260986328\n",
            "iteration : 3108 loss: 0.9268097877502441\n",
            "iteration : 3109 loss: 0.8734340071678162\n",
            "iteration : 3110 loss: 0.7429484724998474\n",
            "iteration : 3111 loss: 0.49097132682800293\n",
            "iteration : 3112 loss: 0.7135899662971497\n",
            "iteration : 3113 loss: 0.6381855607032776\n",
            "iteration : 3114 loss: 0.2866831123828888\n",
            "iteration : 3115 loss: 0.41433200240135193\n",
            "iteration : 3116 loss: 1.7118453979492188\n",
            "iteration : 3117 loss: 1.4192233085632324\n",
            "iteration : 3118 loss: 0.4896436929702759\n",
            "iteration : 3119 loss: 0.7252592444419861\n",
            "iteration : 3120 loss: 0.42256033420562744\n",
            "iteration : 3121 loss: 0.15354938805103302\n",
            "iteration : 3122 loss: 0.7095987796783447\n",
            "iteration : 3123 loss: 0.45542073249816895\n",
            "iteration : 3124 loss: 0.9315266609191895\n",
            "iteration : 3125 loss: 1.2125921249389648\n",
            "iteration : 3126 loss: 0.4990091323852539\n",
            "iteration : 3127 loss: 0.8518602252006531\n",
            "iteration : 3128 loss: 0.39629730582237244\n",
            "iteration : 3129 loss: 0.28611651062965393\n",
            "iteration : 3130 loss: 1.04787278175354\n",
            "iteration : 3131 loss: 0.5128015279769897\n",
            "iteration : 3132 loss: 0.19923000037670135\n",
            "iteration : 3133 loss: 0.2183666229248047\n",
            "iteration : 3134 loss: 0.40318599343299866\n",
            "iteration : 3135 loss: 0.25264763832092285\n",
            "iteration : 3136 loss: 0.32244813442230225\n",
            "iteration : 3137 loss: 0.6208860874176025\n",
            "iteration : 3138 loss: 2.443026542663574\n",
            "iteration : 3139 loss: 0.456447571516037\n",
            "iteration : 3140 loss: 0.7527499198913574\n",
            "iteration : 3141 loss: 0.8452573418617249\n",
            "iteration : 3142 loss: 0.7579319477081299\n",
            "iteration : 3143 loss: 0.46676573157310486\n",
            "iteration : 3144 loss: 0.2861228287220001\n",
            "iteration : 3145 loss: 0.5781080722808838\n",
            "iteration : 3146 loss: 0.4246922731399536\n",
            "iteration : 3147 loss: 0.4155036509037018\n",
            "iteration : 3148 loss: 0.6274988055229187\n",
            "iteration : 3149 loss: 0.40787655115127563\n",
            "iteration : 3150 loss: 1.202282428741455\n",
            "iteration : 3151 loss: 0.41365471482276917\n",
            "iteration : 3152 loss: 0.35360047221183777\n",
            "iteration : 3153 loss: 0.3272777199745178\n",
            "iteration : 3154 loss: 0.2222205549478531\n",
            "iteration : 3155 loss: 0.2187386006116867\n",
            "iteration : 3156 loss: 0.6172530055046082\n",
            "iteration : 3157 loss: 1.327820897102356\n",
            "iteration : 3158 loss: 0.5572325587272644\n",
            "iteration : 3159 loss: 0.39605632424354553\n",
            "iteration : 3160 loss: 2.5182225704193115\n",
            "iteration : 3161 loss: 0.08212438225746155\n",
            "iteration : 3162 loss: 0.715002715587616\n",
            "iteration : 3163 loss: 0.7803682684898376\n",
            "iteration : 3164 loss: 0.7188448309898376\n",
            "iteration : 3165 loss: 0.7964289784431458\n",
            "iteration : 3166 loss: 0.5130195617675781\n",
            "iteration : 3167 loss: 0.17801843583583832\n",
            "iteration : 3168 loss: 0.2826337516307831\n",
            "iteration : 3169 loss: 0.939108669757843\n",
            "iteration : 3170 loss: 0.4105325937271118\n",
            "iteration : 3171 loss: 0.9144965410232544\n",
            "iteration : 3172 loss: 0.5304975509643555\n",
            "iteration : 3173 loss: 0.6336162090301514\n",
            "iteration : 3174 loss: 0.28363823890686035\n",
            "iteration : 3175 loss: 0.8186099529266357\n",
            "iteration : 3176 loss: 0.1667882651090622\n",
            "iteration : 3177 loss: 0.6207570433616638\n",
            "iteration : 3178 loss: 0.361950546503067\n",
            "iteration : 3179 loss: 0.580468475818634\n",
            "iteration : 3180 loss: 0.809089183807373\n",
            "iteration : 3181 loss: 0.3728639781475067\n",
            "iteration : 3182 loss: 0.3150993883609772\n",
            "iteration : 3183 loss: 0.8683784604072571\n",
            "iteration : 3184 loss: 0.4637726843357086\n",
            "iteration : 3185 loss: 0.3399582803249359\n",
            "iteration : 3186 loss: 0.47097086906433105\n",
            "iteration : 3187 loss: 0.21253767609596252\n",
            "iteration : 3188 loss: 0.522441029548645\n",
            "iteration : 3189 loss: 0.3534546494483948\n",
            "iteration : 3190 loss: 0.23296627402305603\n",
            "iteration : 3191 loss: 0.4158015549182892\n",
            "iteration : 3192 loss: 1.095910906791687\n",
            "iteration : 3193 loss: 0.6340075135231018\n",
            "iteration : 3194 loss: 0.9715909361839294\n",
            "iteration : 3195 loss: 0.15582092106342316\n",
            "iteration : 3196 loss: 0.44870492815971375\n",
            "iteration : 3197 loss: 0.14084891974925995\n",
            "iteration : 3198 loss: 0.3852790892124176\n",
            "iteration : 3199 loss: 0.11654867976903915\n",
            "iteration : 3200 loss: 0.21062564849853516\n",
            "iteration : 3201 loss: 0.3615633547306061\n",
            "iteration : 3202 loss: 0.7294393181800842\n",
            "iteration : 3203 loss: 0.1236245259642601\n",
            "iteration : 3204 loss: 0.43779560923576355\n",
            "iteration : 3205 loss: 0.17564338445663452\n",
            "iteration : 3206 loss: 0.3037809431552887\n",
            "iteration : 3207 loss: 0.188020721077919\n",
            "iteration : 3208 loss: 0.16108010709285736\n",
            "iteration : 3209 loss: 0.12627416849136353\n",
            "iteration : 3210 loss: 0.09802166372537613\n",
            "iteration : 3211 loss: 0.7114788293838501\n",
            "iteration : 3212 loss: 0.507014811038971\n",
            "iteration : 3213 loss: 3.1551547050476074\n",
            "iteration : 3214 loss: 0.18791262805461884\n",
            "iteration : 3215 loss: 1.1828491687774658\n",
            "iteration : 3216 loss: 0.7089373469352722\n",
            "iteration : 3217 loss: 0.2597739100456238\n",
            "iteration : 3218 loss: 0.5472179055213928\n",
            "iteration : 3219 loss: 1.258586049079895\n",
            "iteration : 3220 loss: 0.45455703139305115\n",
            "iteration : 3221 loss: 0.1656065136194229\n",
            "iteration : 3222 loss: 0.2218666672706604\n",
            "iteration : 3223 loss: 0.21673540771007538\n",
            "iteration : 3224 loss: 0.36763209104537964\n",
            "iteration : 3225 loss: 0.23731818795204163\n",
            "iteration : 3226 loss: 0.6404255628585815\n",
            "iteration : 3227 loss: 0.29870492219924927\n",
            "iteration : 3228 loss: 0.8270083665847778\n",
            "iteration : 3229 loss: 0.2499275952577591\n",
            "iteration : 3230 loss: 1.013347864151001\n",
            "iteration : 3231 loss: 0.43257108330726624\n",
            "iteration : 3232 loss: 1.5884686708450317\n",
            "iteration : 3233 loss: 1.2659152746200562\n",
            "iteration : 3234 loss: 0.5644968152046204\n",
            "iteration : 3235 loss: 0.4024617075920105\n",
            "iteration : 3236 loss: 0.6000701189041138\n",
            "iteration : 3237 loss: 0.24884486198425293\n",
            "iteration : 3238 loss: 0.264792263507843\n",
            "iteration : 3239 loss: 0.8006842136383057\n",
            "iteration : 3240 loss: 0.832672655582428\n",
            "iteration : 3241 loss: 0.1901741474866867\n",
            "iteration : 3242 loss: 0.2234606146812439\n",
            "iteration : 3243 loss: 0.6240679621696472\n",
            "iteration : 3244 loss: 0.24431118369102478\n",
            "iteration : 3245 loss: 0.38464730978012085\n",
            "iteration : 3246 loss: 1.508323311805725\n",
            "iteration : 3247 loss: 0.6438289880752563\n",
            "iteration : 3248 loss: 1.055066466331482\n",
            "iteration : 3249 loss: 0.2465909868478775\n",
            "iteration : 3250 loss: 0.12674184143543243\n",
            "iteration : 3251 loss: 0.08025797456502914\n",
            "iteration : 3252 loss: 0.25773167610168457\n",
            "iteration : 3253 loss: 2.334216594696045\n",
            "iteration : 3254 loss: 1.4897522926330566\n",
            "iteration : 3255 loss: 0.3440217077732086\n",
            "iteration : 3256 loss: 0.5013048648834229\n",
            "iteration : 3257 loss: 1.068083643913269\n",
            "iteration : 3258 loss: 0.20257532596588135\n",
            "iteration : 3259 loss: 0.48500192165374756\n",
            "iteration : 3260 loss: 0.5793287754058838\n",
            "iteration : 3261 loss: 0.10145460814237595\n",
            "iteration : 3262 loss: 0.7557535767555237\n",
            "iteration : 3263 loss: 0.8540840148925781\n",
            "iteration : 3264 loss: 0.3345416784286499\n",
            "iteration : 3265 loss: 0.18697349727153778\n",
            "iteration : 3266 loss: 0.22323420643806458\n",
            "iteration : 3267 loss: 0.3056475520133972\n",
            "iteration : 3268 loss: 0.2968156039714813\n",
            "iteration : 3269 loss: 0.168230339884758\n",
            "iteration : 3270 loss: 0.09681861102581024\n",
            "iteration : 3271 loss: 0.24057549238204956\n",
            "iteration : 3272 loss: 0.08721648156642914\n",
            "iteration : 3273 loss: 0.6470813751220703\n",
            "iteration : 3274 loss: 0.15998336672782898\n",
            "iteration : 3275 loss: 0.14111202955245972\n",
            "iteration : 3276 loss: 0.1959919035434723\n",
            "iteration : 3277 loss: 0.026952562853693962\n",
            "iteration : 3278 loss: 0.31684109568595886\n",
            "iteration : 3279 loss: 0.07323452830314636\n",
            "iteration : 3280 loss: 0.02951064147055149\n",
            "iteration : 3281 loss: 0.10036023706197739\n",
            "iteration : 3282 loss: 0.7897794842720032\n",
            "iteration : 3283 loss: 0.31616872549057007\n",
            "iteration : 3284 loss: 0.13677386939525604\n",
            "iteration : 3285 loss: 0.27976325154304504\n",
            "iteration : 3286 loss: 0.3192143440246582\n",
            "iteration : 3287 loss: 0.13878387212753296\n",
            "iteration : 3288 loss: 0.20414438843727112\n",
            "iteration : 3289 loss: 0.17738254368305206\n",
            "iteration : 3290 loss: 0.2168637216091156\n",
            "iteration : 3291 loss: 1.0524741411209106\n",
            "iteration : 3292 loss: 0.05696195736527443\n",
            "iteration : 3293 loss: 0.13308659195899963\n",
            "iteration : 3294 loss: 0.025684989988803864\n",
            "iteration : 3295 loss: 0.2635565996170044\n",
            "iteration : 3296 loss: 0.12716147303581238\n",
            "iteration : 3297 loss: 0.4708777964115143\n",
            "iteration : 3298 loss: 0.04256761819124222\n",
            "iteration : 3299 loss: 0.20460985600948334\n",
            "iteration : 3300 loss: 0.17378295958042145\n",
            "iteration : 3301 loss: 0.11272207647562027\n",
            "iteration : 3302 loss: 0.035799212753772736\n",
            "iteration : 3303 loss: 0.08357753604650497\n",
            "iteration : 3304 loss: 0.24680118262767792\n",
            "iteration : 3305 loss: 0.438871294260025\n",
            "iteration : 3306 loss: 0.4254409968852997\n",
            "iteration : 3307 loss: 0.09637396782636642\n",
            "iteration : 3308 loss: 1.0878273248672485\n",
            "iteration : 3309 loss: 0.050538595765829086\n",
            "iteration : 3310 loss: 0.38531431555747986\n",
            "iteration : 3311 loss: 0.3011249601840973\n",
            "iteration : 3312 loss: 1.9039745330810547\n",
            "iteration : 3313 loss: 0.03170780465006828\n",
            "iteration : 3314 loss: 0.3627476394176483\n",
            "iteration : 3315 loss: 0.5524176359176636\n",
            "iteration : 3316 loss: 0.04063434153795242\n",
            "iteration : 3317 loss: 0.06468933075666428\n",
            "iteration : 3318 loss: 0.03706943616271019\n",
            "iteration : 3319 loss: 0.030740275979042053\n",
            "iteration : 3320 loss: 0.025560937821865082\n",
            "iteration : 3321 loss: 0.014567043632268906\n",
            "iteration : 3322 loss: 0.7985377311706543\n",
            "iteration : 3323 loss: 0.7365525364875793\n",
            "iteration : 3324 loss: 0.29562389850616455\n",
            "iteration : 3325 loss: 0.7096287608146667\n",
            "iteration : 3326 loss: 0.49139130115509033\n",
            "iteration : 3327 loss: 2.2407920360565186\n",
            "iteration : 3328 loss: 2.664869546890259\n",
            "iteration : 3329 loss: 2.2884724140167236\n",
            "iteration : 3330 loss: 1.3149948120117188\n",
            "iteration : 3331 loss: 0.5909716486930847\n",
            "iteration : 3332 loss: 0.9439142942428589\n",
            "iteration : 3333 loss: 1.1527975797653198\n",
            "iteration : 3334 loss: 0.7660040259361267\n",
            "iteration : 3335 loss: 1.249466896057129\n",
            "iteration : 3336 loss: 0.776100754737854\n",
            "iteration : 3337 loss: 0.6155616641044617\n",
            "iteration : 3338 loss: 0.6248924136161804\n",
            "iteration : 3339 loss: 0.5331420302391052\n",
            "iteration : 3340 loss: 1.1439690589904785\n",
            "iteration : 3341 loss: 0.44400662183761597\n",
            "iteration : 3342 loss: 0.7393102049827576\n",
            "iteration : 3343 loss: 0.6791308522224426\n",
            "iteration : 3344 loss: 0.60577392578125\n",
            "iteration : 3345 loss: 0.4378761947154999\n",
            "iteration : 3346 loss: 0.3082682192325592\n",
            "iteration : 3347 loss: 0.41656461358070374\n",
            "iteration : 3348 loss: 0.6684887409210205\n",
            "iteration : 3349 loss: 0.36793386936187744\n",
            "iteration : 3350 loss: 0.20097824931144714\n",
            "iteration : 3351 loss: 0.46969109773635864\n",
            "iteration : 3352 loss: 0.829672634601593\n",
            "iteration : 3353 loss: 0.47484830021858215\n",
            "iteration : 3354 loss: 0.18982429802417755\n",
            "iteration : 3355 loss: 0.5439030528068542\n",
            "iteration : 3356 loss: 0.8252572417259216\n",
            "iteration : 3357 loss: 0.9313551187515259\n",
            "iteration : 3358 loss: 0.5123881101608276\n",
            "iteration : 3359 loss: 2.5581510066986084\n",
            "iteration : 3360 loss: 4.813538074493408\n",
            "iteration : 3361 loss: 2.8885338306427\n",
            "iteration : 3362 loss: 1.0337848663330078\n",
            "iteration : 3363 loss: 0.8139795660972595\n",
            "iteration : 3364 loss: 0.6699684858322144\n",
            "iteration : 3365 loss: 2.863710641860962\n",
            "iteration : 3366 loss: 1.786726713180542\n",
            "iteration : 3367 loss: 2.576263427734375\n",
            "iteration : 3368 loss: 2.989413022994995\n",
            "iteration : 3369 loss: 1.9626659154891968\n",
            "iteration : 3370 loss: 2.61480975151062\n",
            "iteration : 3371 loss: 2.525019407272339\n",
            "iteration : 3372 loss: 2.3949780464172363\n",
            "iteration : 3373 loss: 2.5018985271453857\n",
            "iteration : 3374 loss: 2.2958383560180664\n",
            "iteration : 3375 loss: 2.0923469066619873\n",
            "iteration : 3376 loss: 1.7079631090164185\n",
            "iteration : 3377 loss: 2.1930220127105713\n",
            "iteration : 3378 loss: 2.2013022899627686\n",
            "iteration : 3379 loss: 2.242971181869507\n",
            "iteration : 3380 loss: 2.2188913822174072\n",
            "iteration : 3381 loss: 2.1169567108154297\n",
            "iteration : 3382 loss: 2.069763660430908\n",
            "iteration : 3383 loss: 2.255415916442871\n",
            "iteration : 3384 loss: 1.6750166416168213\n",
            "iteration : 3385 loss: 1.7842236757278442\n",
            "iteration : 3386 loss: 2.2551443576812744\n",
            "iteration : 3387 loss: 2.190567970275879\n",
            "iteration : 3388 loss: 1.657034993171692\n",
            "iteration : 3389 loss: 1.7565834522247314\n",
            "iteration : 3390 loss: 1.7725857496261597\n",
            "iteration : 3391 loss: 1.6696704626083374\n",
            "iteration : 3392 loss: 1.6704533100128174\n",
            "iteration : 3393 loss: 2.1143486499786377\n",
            "iteration : 3394 loss: 2.134263277053833\n",
            "iteration : 3395 loss: 2.0460879802703857\n",
            "iteration : 3396 loss: 2.427682876586914\n",
            "iteration : 3397 loss: 2.3120596408843994\n",
            "iteration : 3398 loss: 1.7886755466461182\n",
            "iteration : 3399 loss: 1.939592719078064\n",
            "iteration : 3400 loss: 1.6120177507400513\n",
            "iteration : 3401 loss: 2.048266887664795\n",
            "iteration : 3402 loss: 3.1013689041137695\n",
            "iteration : 3403 loss: 2.3673360347747803\n",
            "iteration : 3404 loss: 2.699037551879883\n",
            "iteration : 3405 loss: 2.4995791912078857\n",
            "iteration : 3406 loss: 2.0980777740478516\n",
            "iteration : 3407 loss: 1.7460891008377075\n",
            "iteration : 3408 loss: 2.6097800731658936\n",
            "iteration : 3409 loss: 2.8455042839050293\n",
            "iteration : 3410 loss: 2.7944045066833496\n",
            "iteration : 3411 loss: 2.6249990463256836\n",
            "iteration : 3412 loss: 2.8518645763397217\n",
            "iteration : 3413 loss: 2.8453421592712402\n",
            "iteration : 3414 loss: 2.6703763008117676\n",
            "iteration : 3415 loss: 2.524149179458618\n",
            "iteration : 3416 loss: 2.9857895374298096\n",
            "iteration : 3417 loss: 2.2560720443725586\n",
            "iteration : 3418 loss: 2.781604766845703\n",
            "iteration : 3419 loss: 2.273603916168213\n",
            "iteration : 3420 loss: 1.7493305206298828\n",
            "iteration : 3421 loss: 2.3626208305358887\n",
            "iteration : 3422 loss: 1.6007649898529053\n",
            "iteration : 3423 loss: 1.897687554359436\n",
            "iteration : 3424 loss: 2.176464557647705\n",
            "iteration : 3425 loss: 1.9131711721420288\n",
            "iteration : 3426 loss: 2.102154016494751\n",
            "iteration : 3427 loss: 2.37007474899292\n",
            "iteration : 3428 loss: 2.179098606109619\n",
            "iteration : 3429 loss: 1.7443500757217407\n",
            "iteration : 3430 loss: 1.9477112293243408\n",
            "iteration : 3431 loss: 1.887222409248352\n",
            "iteration : 3432 loss: 2.0434348583221436\n",
            "iteration : 3433 loss: 1.4059844017028809\n",
            "iteration : 3434 loss: 1.8867164850234985\n",
            "iteration : 3435 loss: 1.8600994348526\n",
            "iteration : 3436 loss: 1.6678866147994995\n",
            "iteration : 3437 loss: 1.7078911066055298\n",
            "iteration : 3438 loss: 1.7834036350250244\n",
            "iteration : 3439 loss: 1.6903281211853027\n",
            "iteration : 3440 loss: 1.1702853441238403\n",
            "iteration : 3441 loss: 1.2064512968063354\n",
            "iteration : 3442 loss: 0.9795618653297424\n",
            "iteration : 3443 loss: 1.4628987312316895\n",
            "iteration : 3444 loss: 1.7691130638122559\n",
            "iteration : 3445 loss: 2.458749294281006\n",
            "iteration : 3446 loss: 2.1010634899139404\n",
            "iteration : 3447 loss: 1.6973267793655396\n",
            "iteration : 3448 loss: 1.580453634262085\n",
            "iteration : 3449 loss: 1.3428006172180176\n",
            "iteration : 3450 loss: 1.585468053817749\n",
            "iteration : 3451 loss: 1.0679516792297363\n",
            "iteration : 3452 loss: 0.9538620710372925\n",
            "iteration : 3453 loss: 1.8360810279846191\n",
            "iteration : 3454 loss: 0.8953407406806946\n",
            "iteration : 3455 loss: 1.4279767274856567\n",
            "iteration : 3456 loss: 0.9189605712890625\n",
            "iteration : 3457 loss: 0.9437181353569031\n",
            "iteration : 3458 loss: 0.9313628077507019\n",
            "iteration : 3459 loss: 1.4632632732391357\n",
            "iteration : 3460 loss: 1.353117823600769\n",
            "iteration : 3461 loss: 1.5112594366073608\n",
            "iteration : 3462 loss: 1.066144347190857\n",
            "iteration : 3463 loss: 0.9518731236457825\n",
            "iteration : 3464 loss: 1.6558035612106323\n",
            "iteration : 3465 loss: 1.1315008401870728\n",
            "iteration : 3466 loss: 1.1953498125076294\n",
            "iteration : 3467 loss: 0.7476564645767212\n",
            "iteration : 3468 loss: 0.7948644757270813\n",
            "iteration : 3469 loss: 0.4994873106479645\n",
            "iteration : 3470 loss: 0.9553177952766418\n",
            "iteration : 3471 loss: 1.0396702289581299\n",
            "iteration : 3472 loss: 0.9241662621498108\n",
            "iteration : 3473 loss: 0.5007389187812805\n",
            "iteration : 3474 loss: 0.77104651927948\n",
            "iteration : 3475 loss: 0.7269631028175354\n",
            "iteration : 3476 loss: 1.5502240657806396\n",
            "iteration : 3477 loss: 0.9043529033660889\n",
            "iteration : 3478 loss: 0.9468726515769958\n",
            "iteration : 3479 loss: 0.5477880835533142\n",
            "iteration : 3480 loss: 0.3315400779247284\n",
            "iteration : 3481 loss: 0.6802699565887451\n",
            "iteration : 3482 loss: 0.31178829073905945\n",
            "iteration : 3483 loss: 0.35922229290008545\n",
            "iteration : 3484 loss: 0.3171120584011078\n",
            "iteration : 3485 loss: 0.48609334230422974\n",
            "iteration : 3486 loss: 0.24221819639205933\n",
            "iteration : 3487 loss: 0.3990596532821655\n",
            "iteration : 3488 loss: 0.8619357943534851\n",
            "iteration : 3489 loss: 0.7663219571113586\n",
            "iteration : 3490 loss: 1.0525673627853394\n",
            "iteration : 3491 loss: 0.5822127461433411\n",
            "iteration : 3492 loss: 0.15430887043476105\n",
            "iteration : 3493 loss: 0.8311177492141724\n",
            "iteration : 3494 loss: 0.15209288895130157\n",
            "iteration : 3495 loss: 0.5716786980628967\n",
            "iteration : 3496 loss: 0.6958299875259399\n",
            "iteration : 3497 loss: 1.4270076751708984\n",
            "iteration : 3498 loss: 0.6748168468475342\n",
            "iteration : 3499 loss: 0.7681102752685547\n",
            "iteration : 3500 loss: 1.0900437831878662\n",
            "iteration : 3501 loss: 0.5213218927383423\n",
            "iteration : 3502 loss: 0.5383773446083069\n",
            "iteration : 3503 loss: 0.5573144555091858\n",
            "iteration : 3504 loss: 0.6001713871955872\n",
            "iteration : 3505 loss: 0.16716770827770233\n",
            "iteration : 3506 loss: 2.4201252460479736\n",
            "iteration : 3507 loss: 0.3848651647567749\n",
            "iteration : 3508 loss: 0.6638773679733276\n",
            "iteration : 3509 loss: 0.464387446641922\n",
            "iteration : 3510 loss: 0.6976442337036133\n",
            "iteration : 3511 loss: 0.6707754135131836\n",
            "iteration : 3512 loss: 0.6355215311050415\n",
            "iteration : 3513 loss: 0.20828783512115479\n",
            "iteration : 3514 loss: 0.5628789663314819\n",
            "iteration : 3515 loss: 1.2679040431976318\n",
            "iteration : 3516 loss: 0.48105984926223755\n",
            "iteration : 3517 loss: 0.41548922657966614\n",
            "iteration : 3518 loss: 0.7157362103462219\n",
            "iteration : 3519 loss: 2.38748836517334\n",
            "iteration : 3520 loss: 2.101963758468628\n",
            "iteration : 3521 loss: 1.754560112953186\n",
            "iteration : 3522 loss: 2.483074903488159\n",
            "iteration : 3523 loss: 2.2040205001831055\n",
            "iteration : 3524 loss: 0.985142171382904\n",
            "iteration : 3525 loss: 1.0681527853012085\n",
            "iteration : 3526 loss: 1.5286145210266113\n",
            "iteration : 3527 loss: 1.7939414978027344\n",
            "iteration : 3528 loss: 1.8601020574569702\n",
            "iteration : 3529 loss: 3.224259376525879\n",
            "iteration : 3530 loss: 2.1262004375457764\n",
            "iteration : 3531 loss: 2.6215667724609375\n",
            "iteration : 3532 loss: 2.3288002014160156\n",
            "iteration : 3533 loss: 3.0377883911132812\n",
            "iteration : 3534 loss: 1.9390023946762085\n",
            "iteration : 3535 loss: 1.596401333808899\n",
            "iteration : 3536 loss: 1.8012381792068481\n",
            "iteration : 3537 loss: 1.5002189874649048\n",
            "iteration : 3538 loss: 1.5179532766342163\n",
            "iteration : 3539 loss: 1.5238488912582397\n",
            "iteration : 3540 loss: 1.2613461017608643\n",
            "iteration : 3541 loss: 2.4360365867614746\n",
            "iteration : 3542 loss: 1.4838773012161255\n",
            "iteration : 3543 loss: 1.9072295427322388\n",
            "iteration : 3544 loss: 1.949934959411621\n",
            "iteration : 3545 loss: 1.3947194814682007\n",
            "iteration : 3546 loss: 1.3621019124984741\n",
            "iteration : 3547 loss: 1.4375945329666138\n",
            "iteration : 3548 loss: 1.2654377222061157\n",
            "iteration : 3549 loss: 1.880763053894043\n",
            "iteration : 3550 loss: 0.871221125125885\n",
            "iteration : 3551 loss: 1.7453848123550415\n",
            "iteration : 3552 loss: 3.059849977493286\n",
            "iteration : 3553 loss: 1.5514521598815918\n",
            "iteration : 3554 loss: 3.070821523666382\n",
            "iteration : 3555 loss: 1.5798239707946777\n",
            "iteration : 3556 loss: 2.4257490634918213\n",
            "iteration : 3557 loss: 2.0225868225097656\n",
            "iteration : 3558 loss: 1.891486644744873\n",
            "iteration : 3559 loss: 1.5336663722991943\n",
            "iteration : 3560 loss: 2.2375824451446533\n",
            "iteration : 3561 loss: 1.8248324394226074\n",
            "iteration : 3562 loss: 1.6985059976577759\n",
            "iteration : 3563 loss: 1.3837025165557861\n",
            "iteration : 3564 loss: 1.4845623970031738\n",
            "iteration : 3565 loss: 1.7477357387542725\n",
            "iteration : 3566 loss: 2.796445369720459\n",
            "iteration : 3567 loss: 0.8869447112083435\n",
            "iteration : 3568 loss: 1.7269172668457031\n",
            "iteration : 3569 loss: 1.5662890672683716\n",
            "iteration : 3570 loss: 2.076528310775757\n",
            "iteration : 3571 loss: 2.5567784309387207\n",
            "iteration : 3572 loss: 2.758877754211426\n",
            "iteration : 3573 loss: 2.365854024887085\n",
            "iteration : 3574 loss: 2.3619372844696045\n",
            "iteration : 3575 loss: 2.3346307277679443\n",
            "iteration : 3576 loss: 2.320342540740967\n",
            "iteration : 3577 loss: 2.419584035873413\n",
            "iteration : 3578 loss: 2.3343329429626465\n",
            "iteration : 3579 loss: 2.279155731201172\n",
            "iteration : 3580 loss: 1.9765783548355103\n",
            "iteration : 3581 loss: 2.609079599380493\n",
            "iteration : 3582 loss: 2.00876522064209\n",
            "iteration : 3583 loss: 2.4014675617218018\n",
            "iteration : 3584 loss: 2.2373669147491455\n",
            "iteration : 3585 loss: 2.1285293102264404\n",
            "iteration : 3586 loss: 1.444757103919983\n",
            "iteration : 3587 loss: 1.013882040977478\n",
            "iteration : 3588 loss: 1.7224539518356323\n",
            "iteration : 3589 loss: 2.046926498413086\n",
            "iteration : 3590 loss: 1.600271224975586\n",
            "iteration : 3591 loss: 1.6479741334915161\n",
            "iteration : 3592 loss: 1.4827804565429688\n",
            "iteration : 3593 loss: 2.0715084075927734\n",
            "iteration : 3594 loss: 1.2152749300003052\n",
            "iteration : 3595 loss: 2.020068645477295\n",
            "iteration : 3596 loss: 1.075344443321228\n",
            "iteration : 3597 loss: 0.7538085579872131\n",
            "iteration : 3598 loss: 1.178544044494629\n",
            "iteration : 3599 loss: 1.0899323225021362\n",
            "iteration : 3600 loss: 1.53163480758667\n",
            "iteration : 3601 loss: 1.7738964557647705\n",
            "iteration : 3602 loss: 0.7491443157196045\n",
            "iteration : 3603 loss: 1.069911241531372\n",
            "iteration : 3604 loss: 1.5496397018432617\n",
            "iteration : 3605 loss: 1.6288710832595825\n",
            "iteration : 3606 loss: 2.2980828285217285\n",
            "iteration : 3607 loss: 1.3013361692428589\n",
            "iteration : 3608 loss: 1.2035290002822876\n",
            "iteration : 3609 loss: 1.6470383405685425\n",
            "iteration : 3610 loss: 0.9453080296516418\n",
            "iteration : 3611 loss: 1.4358105659484863\n",
            "iteration : 3612 loss: 1.235506296157837\n",
            "iteration : 3613 loss: 1.8503998517990112\n",
            "iteration : 3614 loss: 0.9730178117752075\n",
            "iteration : 3615 loss: 0.7373583912849426\n",
            "iteration : 3616 loss: 0.5182455778121948\n",
            "iteration : 3617 loss: 0.43712517619132996\n",
            "iteration : 3618 loss: 0.7983278632164001\n",
            "iteration : 3619 loss: 1.2298598289489746\n",
            "iteration : 3620 loss: 0.3700529932975769\n",
            "iteration : 3621 loss: 0.8693718314170837\n",
            "iteration : 3622 loss: 1.1598302125930786\n",
            "iteration : 3623 loss: 1.4341492652893066\n",
            "iteration : 3624 loss: 0.990013062953949\n",
            "iteration : 3625 loss: 0.8708113431930542\n",
            "iteration : 3626 loss: 0.9220131635665894\n",
            "iteration : 3627 loss: 2.012464761734009\n",
            "iteration : 3628 loss: 1.5866409540176392\n",
            "iteration : 3629 loss: 1.8038634061813354\n",
            "iteration : 3630 loss: 1.4791532754898071\n",
            "iteration : 3631 loss: 1.0425201654434204\n",
            "iteration : 3632 loss: 1.2374621629714966\n",
            "iteration : 3633 loss: 0.8482872843742371\n",
            "iteration : 3634 loss: 0.851475179195404\n",
            "iteration : 3635 loss: 1.9177863597869873\n",
            "iteration : 3636 loss: 2.5610504150390625\n",
            "iteration : 3637 loss: 1.7420570850372314\n",
            "iteration : 3638 loss: 1.6640257835388184\n",
            "iteration : 3639 loss: 1.6601759195327759\n",
            "iteration : 3640 loss: 1.5181505680084229\n",
            "iteration : 3641 loss: 1.4394747018814087\n",
            "iteration : 3642 loss: 1.7414612770080566\n",
            "iteration : 3643 loss: 1.4934402704238892\n",
            "iteration : 3644 loss: 1.3124234676361084\n",
            "iteration : 3645 loss: 1.729981541633606\n",
            "iteration : 3646 loss: 1.8781700134277344\n",
            "iteration : 3647 loss: 1.315830945968628\n",
            "iteration : 3648 loss: 1.2992781400680542\n",
            "iteration : 3649 loss: 1.2143956422805786\n",
            "iteration : 3650 loss: 1.6402630805969238\n",
            "iteration : 3651 loss: 1.4229329824447632\n",
            "iteration : 3652 loss: 1.4050627946853638\n",
            "iteration : 3653 loss: 1.4562551975250244\n",
            "iteration : 3654 loss: 1.4496532678604126\n",
            "iteration : 3655 loss: 1.1281410455703735\n",
            "iteration : 3656 loss: 1.0377432107925415\n",
            "iteration : 3657 loss: 0.9576113224029541\n",
            "iteration : 3658 loss: 1.1834098100662231\n",
            "iteration : 3659 loss: 0.875500500202179\n",
            "iteration : 3660 loss: 1.1501054763793945\n",
            "iteration : 3661 loss: 1.0817986726760864\n",
            "iteration : 3662 loss: 1.160144329071045\n",
            "iteration : 3663 loss: 1.0308469533920288\n",
            "iteration : 3664 loss: 1.0527925491333008\n",
            "iteration : 3665 loss: 1.2282326221466064\n",
            "iteration : 3666 loss: 0.937984049320221\n",
            "iteration : 3667 loss: 1.242952823638916\n",
            "iteration : 3668 loss: 0.9360565543174744\n",
            "iteration : 3669 loss: 1.0447622537612915\n",
            "iteration : 3670 loss: 1.0071457624435425\n",
            "iteration : 3671 loss: 0.9778275489807129\n",
            "iteration : 3672 loss: 0.8098111748695374\n",
            "iteration : 3673 loss: 1.0782302618026733\n",
            "iteration : 3674 loss: 0.9279007911682129\n",
            "iteration : 3675 loss: 0.9294754862785339\n",
            "iteration : 3676 loss: 0.9101256728172302\n",
            "iteration : 3677 loss: 0.864117443561554\n",
            "iteration : 3678 loss: 1.116127610206604\n",
            "iteration : 3679 loss: 0.9368128180503845\n",
            "iteration : 3680 loss: 0.8456538915634155\n",
            "iteration : 3681 loss: 0.8222809433937073\n",
            "iteration : 3682 loss: 0.8344624042510986\n",
            "iteration : 3683 loss: 0.677791178226471\n",
            "iteration : 3684 loss: 0.718718945980072\n",
            "iteration : 3685 loss: 0.6072129607200623\n",
            "iteration : 3686 loss: 0.6966180801391602\n",
            "iteration : 3687 loss: 0.5849769115447998\n",
            "iteration : 3688 loss: 0.4931829869747162\n",
            "iteration : 3689 loss: 0.8206093311309814\n",
            "iteration : 3690 loss: 0.6957170367240906\n",
            "iteration : 3691 loss: 1.0432014465332031\n",
            "iteration : 3692 loss: 0.6575417518615723\n",
            "iteration : 3693 loss: 0.9013639092445374\n",
            "iteration : 3694 loss: 0.5453699231147766\n",
            "iteration : 3695 loss: 0.5846552848815918\n",
            "iteration : 3696 loss: 0.5675448179244995\n",
            "iteration : 3697 loss: 0.4679560959339142\n",
            "iteration : 3698 loss: 0.4790395200252533\n",
            "iteration : 3699 loss: 0.6531679034233093\n",
            "iteration : 3700 loss: 0.35296595096588135\n",
            "iteration : 3701 loss: 0.25825294852256775\n",
            "iteration : 3702 loss: 0.5255898237228394\n",
            "iteration : 3703 loss: 2.7564165592193604\n",
            "iteration : 3704 loss: 1.8264392614364624\n",
            "iteration : 3705 loss: 1.9824392795562744\n",
            "iteration : 3706 loss: 4.052061080932617\n",
            "iteration : 3707 loss: 3.8452303409576416\n",
            "iteration : 3708 loss: 2.339128255844116\n",
            "iteration : 3709 loss: 2.347205400466919\n",
            "iteration : 3710 loss: 1.6588222980499268\n",
            "iteration : 3711 loss: 1.1887365579605103\n",
            "iteration : 3712 loss: 1.1075412034988403\n",
            "iteration : 3713 loss: 1.3771352767944336\n",
            "iteration : 3714 loss: 1.7318896055221558\n",
            "iteration : 3715 loss: 1.3073557615280151\n",
            "iteration : 3716 loss: 1.5995311737060547\n",
            "iteration : 3717 loss: 1.9159115552902222\n",
            "iteration : 3718 loss: 2.4502224922180176\n",
            "iteration : 3719 loss: 2.085355758666992\n",
            "iteration : 3720 loss: 1.1198406219482422\n",
            "iteration : 3721 loss: 0.9182910323143005\n",
            "iteration : 3722 loss: 1.332658290863037\n",
            "iteration : 3723 loss: 2.3692173957824707\n",
            "iteration : 3724 loss: 2.590712547302246\n",
            "iteration : 3725 loss: 2.640564441680908\n",
            "iteration : 3726 loss: 2.616541862487793\n",
            "iteration : 3727 loss: 2.135183334350586\n",
            "iteration : 3728 loss: 2.2034850120544434\n",
            "iteration : 3729 loss: 1.3925219774246216\n",
            "iteration : 3730 loss: 1.9171254634857178\n",
            "iteration : 3731 loss: 1.9320648908615112\n",
            "iteration : 3732 loss: 1.9679534435272217\n",
            "iteration : 3733 loss: 1.7969917058944702\n",
            "iteration : 3734 loss: 1.8391515016555786\n",
            "iteration : 3735 loss: 1.5493955612182617\n",
            "iteration : 3736 loss: 1.4636303186416626\n",
            "iteration : 3737 loss: 1.3584457635879517\n",
            "iteration : 3738 loss: 1.655234456062317\n",
            "iteration : 3739 loss: 1.7995195388793945\n",
            "iteration : 3740 loss: 1.5121864080429077\n",
            "iteration : 3741 loss: 1.0190547704696655\n",
            "iteration : 3742 loss: 1.8448927402496338\n",
            "iteration : 3743 loss: 1.6575677394866943\n",
            "iteration : 3744 loss: 1.4145619869232178\n",
            "iteration : 3745 loss: 1.4484621286392212\n",
            "iteration : 3746 loss: 1.4489434957504272\n",
            "iteration : 3747 loss: 1.2883597612380981\n",
            "iteration : 3748 loss: 1.4743930101394653\n",
            "iteration : 3749 loss: 1.5024900436401367\n",
            "iteration : 3750 loss: 1.3137909173965454\n",
            "iteration : 3751 loss: 1.1047070026397705\n",
            "iteration : 3752 loss: 0.9720933437347412\n",
            "iteration : 3753 loss: 1.0328022241592407\n",
            "iteration : 3754 loss: 0.8142050504684448\n",
            "iteration : 3755 loss: 1.4287266731262207\n",
            "iteration : 3756 loss: 0.8503726720809937\n",
            "iteration : 3757 loss: 0.9182496070861816\n",
            "iteration : 3758 loss: 1.0937715768814087\n",
            "iteration : 3759 loss: 1.2660322189331055\n",
            "iteration : 3760 loss: 1.2543820142745972\n",
            "iteration : 3761 loss: 1.5868456363677979\n",
            "iteration : 3762 loss: 1.6669939756393433\n",
            "iteration : 3763 loss: 0.9875208735466003\n",
            "iteration : 3764 loss: 0.8973701596260071\n",
            "iteration : 3765 loss: 0.7809882760047913\n",
            "iteration : 3766 loss: 0.7088767886161804\n",
            "iteration : 3767 loss: 0.6230607628822327\n",
            "iteration : 3768 loss: 0.47465118765830994\n",
            "iteration : 3769 loss: 0.7083057165145874\n",
            "iteration : 3770 loss: 1.0631526708602905\n",
            "iteration : 3771 loss: 0.9756712913513184\n",
            "iteration : 3772 loss: 1.2408652305603027\n",
            "iteration : 3773 loss: 1.3092215061187744\n",
            "iteration : 3774 loss: 1.5766173601150513\n",
            "iteration : 3775 loss: 0.6312969923019409\n",
            "iteration : 3776 loss: 0.4198160767555237\n",
            "iteration : 3777 loss: 0.5637732148170471\n",
            "iteration : 3778 loss: 0.5974928140640259\n",
            "iteration : 3779 loss: 0.41979724168777466\n",
            "iteration : 3780 loss: 0.3201812505722046\n",
            "iteration : 3781 loss: 1.007968544960022\n",
            "iteration : 3782 loss: 1.2883158922195435\n",
            "iteration : 3783 loss: 1.1080260276794434\n",
            "iteration : 3784 loss: 0.9049005508422852\n",
            "iteration : 3785 loss: 0.7980977296829224\n",
            "iteration : 3786 loss: 0.9341992139816284\n",
            "iteration : 3787 loss: 0.6435858011245728\n",
            "iteration : 3788 loss: 0.8519453406333923\n",
            "iteration : 3789 loss: 0.845798671245575\n",
            "iteration : 3790 loss: 1.0410326719284058\n",
            "iteration : 3791 loss: 1.0348179340362549\n",
            "iteration : 3792 loss: 0.9062267541885376\n",
            "iteration : 3793 loss: 0.8481557369232178\n",
            "iteration : 3794 loss: 0.7024156451225281\n",
            "iteration : 3795 loss: 0.6163682341575623\n",
            "iteration : 3796 loss: 0.6444802284240723\n",
            "iteration : 3797 loss: 0.6841099858283997\n",
            "iteration : 3798 loss: 0.9042629599571228\n",
            "iteration : 3799 loss: 1.1977611780166626\n",
            "iteration : 3800 loss: 0.77437824010849\n",
            "iteration : 3801 loss: 0.8078747391700745\n",
            "iteration : 3802 loss: 1.3307547569274902\n",
            "iteration : 3803 loss: 0.477082759141922\n",
            "iteration : 3804 loss: 3.11847186088562\n",
            "iteration : 3805 loss: 2.023131847381592\n",
            "iteration : 3806 loss: 1.020240068435669\n",
            "iteration : 3807 loss: 0.9816973209381104\n",
            "iteration : 3808 loss: 0.6552056670188904\n",
            "iteration : 3809 loss: 0.5883374810218811\n",
            "iteration : 3810 loss: 0.414567768573761\n",
            "iteration : 3811 loss: 0.4795527160167694\n",
            "iteration : 3812 loss: 0.445272296667099\n",
            "iteration : 3813 loss: 0.8330450057983398\n",
            "iteration : 3814 loss: 0.6777459979057312\n",
            "iteration : 3815 loss: 0.5923153758049011\n",
            "iteration : 3816 loss: 0.5966191291809082\n",
            "iteration : 3817 loss: 0.3449747562408447\n",
            "iteration : 3818 loss: 0.498159259557724\n",
            "iteration : 3819 loss: 0.39920708537101746\n",
            "iteration : 3820 loss: 0.6341910362243652\n",
            "iteration : 3821 loss: 0.37859299778938293\n",
            "iteration : 3822 loss: 0.2763929069042206\n",
            "iteration : 3823 loss: 0.2143881767988205\n",
            "iteration : 3824 loss: 0.3398423492908478\n",
            "iteration : 3825 loss: 0.9929824471473694\n",
            "iteration : 3826 loss: 1.080394983291626\n",
            "iteration : 3827 loss: 1.0464426279067993\n",
            "iteration : 3828 loss: 0.961108922958374\n",
            "iteration : 3829 loss: 1.3343236446380615\n",
            "iteration : 3830 loss: 0.34854042530059814\n",
            "iteration : 3831 loss: 0.290996253490448\n",
            "iteration : 3832 loss: 0.41007688641548157\n",
            "iteration : 3833 loss: 0.46476292610168457\n",
            "iteration : 3834 loss: 0.14281733334064484\n",
            "iteration : 3835 loss: 0.2319771647453308\n",
            "iteration : 3836 loss: 0.3829817473888397\n",
            "iteration : 3837 loss: 0.2913210093975067\n",
            "iteration : 3838 loss: 0.42089441418647766\n",
            "iteration : 3839 loss: 0.18997310101985931\n",
            "iteration : 3840 loss: 0.2623968720436096\n",
            "iteration : 3841 loss: 0.3049684762954712\n",
            "iteration : 3842 loss: 1.2377145290374756\n",
            "iteration : 3843 loss: 0.4649059772491455\n",
            "iteration : 3844 loss: 0.2898695468902588\n",
            "iteration : 3845 loss: 0.21436792612075806\n",
            "iteration : 3846 loss: 0.22079913318157196\n",
            "iteration : 3847 loss: 0.3911316692829132\n",
            "iteration : 3848 loss: 0.23677466809749603\n",
            "iteration : 3849 loss: 0.4112119674682617\n",
            "iteration : 3850 loss: 0.6987434029579163\n",
            "iteration : 3851 loss: 0.22397847473621368\n",
            "iteration : 3852 loss: 0.17581337690353394\n",
            "iteration : 3853 loss: 0.053231269121170044\n",
            "iteration : 3854 loss: 0.6873108148574829\n",
            "iteration : 3855 loss: 0.5405911803245544\n",
            "iteration : 3856 loss: 0.2527611255645752\n",
            "iteration : 3857 loss: 0.8156426548957825\n",
            "iteration : 3858 loss: 1.0674742460250854\n",
            "iteration : 3859 loss: 0.37158238887786865\n",
            "iteration : 3860 loss: 0.47062426805496216\n",
            "iteration : 3861 loss: 0.39644503593444824\n",
            "iteration : 3862 loss: 0.4902835190296173\n",
            "iteration : 3863 loss: 2.7763729095458984\n",
            "iteration : 3864 loss: 2.4621493816375732\n",
            "iteration : 3865 loss: 1.6345232725143433\n",
            "iteration : 3866 loss: 1.8463655710220337\n",
            "iteration : 3867 loss: 1.44355309009552\n",
            "iteration : 3868 loss: 1.1733031272888184\n",
            "iteration : 3869 loss: 0.9863390326499939\n",
            "iteration : 3870 loss: 1.017647624015808\n",
            "iteration : 3871 loss: 0.859968364238739\n",
            "iteration : 3872 loss: 1.0658364295959473\n",
            "iteration : 3873 loss: 0.6900983452796936\n",
            "iteration : 3874 loss: 0.5794339776039124\n",
            "iteration : 3875 loss: 0.5187717080116272\n",
            "iteration : 3876 loss: 0.5023055672645569\n",
            "iteration : 3877 loss: 0.5041826367378235\n",
            "iteration : 3878 loss: 0.3788602650165558\n",
            "iteration : 3879 loss: 0.3624129593372345\n",
            "iteration : 3880 loss: 1.0456565618515015\n",
            "iteration : 3881 loss: 1.5481799840927124\n",
            "iteration : 3882 loss: 7.011524677276611\n",
            "iteration : 3883 loss: 5.369111061096191\n",
            "iteration : 3884 loss: 3.6551313400268555\n",
            "iteration : 3885 loss: 2.5251832008361816\n",
            "iteration : 3886 loss: 2.084073066711426\n",
            "iteration : 3887 loss: 1.5194987058639526\n",
            "iteration : 3888 loss: 1.883637547492981\n",
            "iteration : 3889 loss: 1.18366277217865\n",
            "iteration : 3890 loss: 1.5356587171554565\n",
            "iteration : 3891 loss: 1.467119574546814\n",
            "iteration : 3892 loss: 1.417704463005066\n",
            "iteration : 3893 loss: 1.1056486368179321\n",
            "iteration : 3894 loss: 1.2811051607131958\n",
            "iteration : 3895 loss: 2.568359375\n",
            "iteration : 3896 loss: 3.3860244750976562\n",
            "iteration : 3897 loss: 3.3451473712921143\n",
            "iteration : 3898 loss: 1.8071810007095337\n",
            "iteration : 3899 loss: 2.035722017288208\n",
            "iteration : 3900 loss: 1.7466939687728882\n",
            "iteration : 3901 loss: 1.462335467338562\n",
            "iteration : 3902 loss: 1.1914211511611938\n",
            "iteration : 3903 loss: 1.033021092414856\n",
            "iteration : 3904 loss: 0.8232031464576721\n",
            "iteration : 3905 loss: 0.976080596446991\n",
            "iteration : 3906 loss: 1.3336578607559204\n",
            "iteration : 3907 loss: 1.0968093872070312\n",
            "iteration : 3908 loss: 1.4048265218734741\n",
            "iteration : 3909 loss: 1.1294928789138794\n",
            "iteration : 3910 loss: 0.9883015751838684\n",
            "iteration : 3911 loss: 1.0722121000289917\n",
            "iteration : 3912 loss: 1.1114071607589722\n",
            "iteration : 3913 loss: 1.0114151239395142\n",
            "iteration : 3914 loss: 1.278612732887268\n",
            "iteration : 3915 loss: 2.848308563232422\n",
            "iteration : 3916 loss: 5.437142848968506\n",
            "iteration : 3917 loss: 4.639427185058594\n",
            "iteration : 3918 loss: 5.289517879486084\n",
            "iteration : 3919 loss: 5.006702423095703\n",
            "iteration : 3920 loss: 4.474680423736572\n",
            "iteration : 3921 loss: 4.192924499511719\n",
            "iteration : 3922 loss: 4.005476474761963\n",
            "iteration : 3923 loss: 3.5208239555358887\n",
            "iteration : 3924 loss: 2.997044563293457\n",
            "iteration : 3925 loss: 2.7748329639434814\n",
            "iteration : 3926 loss: 3.708984375\n",
            "iteration : 3927 loss: 2.6566596031188965\n",
            "iteration : 3928 loss: 2.8455190658569336\n",
            "iteration : 3929 loss: 2.472477912902832\n",
            "iteration : 3930 loss: 2.774611711502075\n",
            "iteration : 3931 loss: 2.5675370693206787\n",
            "iteration : 3932 loss: 2.7956314086914062\n",
            "iteration : 3933 loss: 2.8977599143981934\n",
            "iteration : 3934 loss: 2.4360196590423584\n",
            "iteration : 3935 loss: 2.5945963859558105\n",
            "iteration : 3936 loss: 2.9494590759277344\n",
            "iteration : 3937 loss: 2.144265651702881\n",
            "iteration : 3938 loss: 2.776827812194824\n",
            "iteration : 3939 loss: 2.289329767227173\n",
            "iteration : 3940 loss: 2.150451898574829\n",
            "iteration : 3941 loss: 2.0444576740264893\n",
            "iteration : 3942 loss: 2.0451204776763916\n",
            "iteration : 3943 loss: 1.8327312469482422\n",
            "iteration : 3944 loss: 2.5749974250793457\n",
            "iteration : 3945 loss: 2.3884758949279785\n",
            "iteration : 3946 loss: 1.984114646911621\n",
            "iteration : 3947 loss: 2.1161282062530518\n",
            "iteration : 3948 loss: 1.8779711723327637\n",
            "iteration : 3949 loss: 1.6742500066757202\n",
            "iteration : 3950 loss: 1.6791149377822876\n",
            "iteration : 3951 loss: 1.629951000213623\n",
            "iteration : 3952 loss: 1.9201825857162476\n",
            "iteration : 3953 loss: 2.074078321456909\n",
            "iteration : 3954 loss: 1.9041694402694702\n",
            "iteration : 3955 loss: 1.205870270729065\n",
            "iteration : 3956 loss: 1.951736330986023\n",
            "iteration : 3957 loss: 1.7223349809646606\n",
            "iteration : 3958 loss: 1.7494100332260132\n",
            "iteration : 3959 loss: 2.2188098430633545\n",
            "iteration : 3960 loss: 1.8911346197128296\n",
            "iteration : 3961 loss: 1.9407504796981812\n",
            "iteration : 3962 loss: 2.074500322341919\n",
            "iteration : 3963 loss: 1.995457649230957\n",
            "iteration : 3964 loss: 2.045889377593994\n",
            "iteration : 3965 loss: 1.7611430883407593\n",
            "iteration : 3966 loss: 2.278385639190674\n",
            "iteration : 3967 loss: 2.09726619720459\n",
            "iteration : 3968 loss: 1.241512656211853\n",
            "iteration : 3969 loss: 1.7756409645080566\n",
            "iteration : 3970 loss: 1.6096224784851074\n",
            "iteration : 3971 loss: 1.5008463859558105\n",
            "iteration : 3972 loss: 2.424056053161621\n",
            "iteration : 3973 loss: 2.3780105113983154\n",
            "iteration : 3974 loss: 1.8740074634552002\n",
            "iteration : 3975 loss: 2.12970232963562\n",
            "iteration : 3976 loss: 2.19728684425354\n",
            "iteration : 3977 loss: 1.86962890625\n",
            "iteration : 3978 loss: 1.4490174055099487\n",
            "iteration : 3979 loss: 1.5751174688339233\n",
            "iteration : 3980 loss: 1.9461455345153809\n",
            "iteration : 3981 loss: 1.4906803369522095\n",
            "iteration : 3982 loss: 1.7288657426834106\n",
            "iteration : 3983 loss: 1.58082914352417\n",
            "iteration : 3984 loss: 1.5370237827301025\n",
            "iteration : 3985 loss: 1.2301756143569946\n",
            "iteration : 3986 loss: 1.369553804397583\n",
            "iteration : 3987 loss: 1.5070427656173706\n",
            "iteration : 3988 loss: 1.3724308013916016\n",
            "iteration : 3989 loss: 2.102335214614868\n",
            "iteration : 3990 loss: 1.7293611764907837\n",
            "iteration : 3991 loss: 1.60684072971344\n",
            "iteration : 3992 loss: 1.7680283784866333\n",
            "iteration : 3993 loss: 2.0999858379364014\n",
            "iteration : 3994 loss: 1.670009732246399\n",
            "iteration : 3995 loss: 3.0294575691223145\n",
            "iteration : 3996 loss: 2.841787576675415\n",
            "iteration : 3997 loss: 2.1176888942718506\n",
            "iteration : 3998 loss: 2.400808095932007\n",
            "iteration : 3999 loss: 1.930199146270752\n",
            "iteration : 4000 loss: 2.0385289192199707\n",
            "iteration : 4001 loss: 1.6160379648208618\n",
            "iteration : 4002 loss: 1.3857818841934204\n",
            "iteration : 4003 loss: 2.0016305446624756\n",
            "iteration : 4004 loss: 2.1251423358917236\n",
            "iteration : 4005 loss: 2.7423973083496094\n",
            "iteration : 4006 loss: 1.620099425315857\n",
            "iteration : 4007 loss: 1.6684149503707886\n",
            "iteration : 4008 loss: 1.5923917293548584\n",
            "iteration : 4009 loss: 1.852217674255371\n",
            "iteration : 4010 loss: 2.9136173725128174\n",
            "iteration : 4011 loss: 1.719745397567749\n",
            "iteration : 4012 loss: 2.075448751449585\n",
            "iteration : 4013 loss: 2.228017807006836\n",
            "iteration : 4014 loss: 2.029407501220703\n",
            "iteration : 4015 loss: 2.1998159885406494\n",
            "iteration : 4016 loss: 2.009899139404297\n",
            "iteration : 4017 loss: 2.148174524307251\n",
            "iteration : 4018 loss: 1.9731920957565308\n",
            "iteration : 4019 loss: 1.7129158973693848\n",
            "iteration : 4020 loss: 2.1649465560913086\n",
            "iteration : 4021 loss: 1.4416484832763672\n",
            "iteration : 4022 loss: 1.4417659044265747\n",
            "iteration : 4023 loss: 1.6675916910171509\n",
            "iteration : 4024 loss: 1.4283047914505005\n",
            "iteration : 4025 loss: 2.047891139984131\n",
            "iteration : 4026 loss: 1.3706817626953125\n",
            "iteration : 4027 loss: 3.173739433288574\n",
            "iteration : 4028 loss: 1.3160512447357178\n",
            "iteration : 4029 loss: 1.4812407493591309\n",
            "iteration : 4030 loss: 1.6238144636154175\n",
            "iteration : 4031 loss: 1.6531075239181519\n",
            "iteration : 4032 loss: 2.496138334274292\n",
            "iteration : 4033 loss: 1.8832169771194458\n",
            "iteration : 4034 loss: 1.8124607801437378\n",
            "iteration : 4035 loss: 2.182224750518799\n",
            "iteration : 4036 loss: 1.5031322240829468\n",
            "iteration : 4037 loss: 1.6268466711044312\n",
            "iteration : 4038 loss: 1.3328737020492554\n",
            "iteration : 4039 loss: 1.6769758462905884\n",
            "iteration : 4040 loss: 1.9456875324249268\n",
            "iteration : 4041 loss: 1.5982741117477417\n",
            "iteration : 4042 loss: 2.385561227798462\n",
            "iteration : 4043 loss: 1.803828239440918\n",
            "iteration : 4044 loss: 1.6204376220703125\n",
            "iteration : 4045 loss: 2.16878604888916\n",
            "iteration : 4046 loss: 1.6872787475585938\n",
            "iteration : 4047 loss: 1.7059773206710815\n",
            "iteration : 4048 loss: 1.5529141426086426\n",
            "iteration : 4049 loss: 1.389877200126648\n",
            "iteration : 4050 loss: 1.3161113262176514\n",
            "iteration : 4051 loss: 1.5237818956375122\n",
            "iteration : 4052 loss: 1.875217080116272\n",
            "iteration : 4053 loss: 1.6264854669570923\n",
            "iteration : 4054 loss: 2.1969552040100098\n",
            "iteration : 4055 loss: 1.371240258216858\n",
            "iteration : 4056 loss: 1.4376707077026367\n",
            "iteration : 4057 loss: 1.2866947650909424\n",
            "iteration : 4058 loss: 1.6701595783233643\n",
            "iteration : 4059 loss: 1.5874162912368774\n",
            "iteration : 4060 loss: 1.1678484678268433\n",
            "iteration : 4061 loss: 1.2429938316345215\n",
            "iteration : 4062 loss: 1.845054268836975\n",
            "iteration : 4063 loss: 1.8419331312179565\n",
            "iteration : 4064 loss: 1.7340043783187866\n",
            "iteration : 4065 loss: 1.6005116701126099\n",
            "iteration : 4066 loss: 1.6258862018585205\n",
            "iteration : 4067 loss: 1.7487719058990479\n",
            "iteration : 4068 loss: 1.6195486783981323\n",
            "iteration : 4069 loss: 1.8845176696777344\n",
            "iteration : 4070 loss: 1.6083670854568481\n",
            "iteration : 4071 loss: 1.2699663639068604\n",
            "iteration : 4072 loss: 1.1488145589828491\n",
            "iteration : 4073 loss: 1.1469944715499878\n",
            "iteration : 4074 loss: 1.1988656520843506\n",
            "iteration : 4075 loss: 1.0366350412368774\n",
            "iteration : 4076 loss: 1.1199909448623657\n",
            "iteration : 4077 loss: 1.2816284894943237\n",
            "iteration : 4078 loss: 1.5494182109832764\n",
            "iteration : 4079 loss: 1.5446356534957886\n",
            "iteration : 4080 loss: 2.252399444580078\n",
            "iteration : 4081 loss: 2.826598882675171\n",
            "iteration : 4082 loss: 2.5700533390045166\n",
            "iteration : 4083 loss: 2.255941390991211\n",
            "iteration : 4084 loss: 2.3366880416870117\n",
            "iteration : 4085 loss: 2.0898337364196777\n",
            "iteration : 4086 loss: 2.2306902408599854\n",
            "iteration : 4087 loss: 2.5998966693878174\n",
            "iteration : 4088 loss: 2.4071977138519287\n",
            "iteration : 4089 loss: 2.0233380794525146\n",
            "iteration : 4090 loss: 2.712212085723877\n",
            "iteration : 4091 loss: 2.0516836643218994\n",
            "iteration : 4092 loss: 1.9776920080184937\n",
            "iteration : 4093 loss: 1.9855154752731323\n",
            "iteration : 4094 loss: 1.5298093557357788\n",
            "iteration : 4095 loss: 1.5464831590652466\n",
            "iteration : 4096 loss: 1.5716496706008911\n",
            "iteration : 4097 loss: 1.0724979639053345\n",
            "iteration : 4098 loss: 1.3379673957824707\n",
            "iteration : 4099 loss: 1.437853217124939\n",
            "iteration : 4100 loss: 1.9009552001953125\n",
            "iteration : 4101 loss: 2.4861087799072266\n",
            "iteration : 4102 loss: 1.6006108522415161\n",
            "iteration : 4103 loss: 1.4856594800949097\n",
            "iteration : 4104 loss: 1.5874513387680054\n",
            "iteration : 4105 loss: 2.478649139404297\n",
            "iteration : 4106 loss: 1.6532100439071655\n",
            "iteration : 4107 loss: 1.6233092546463013\n",
            "iteration : 4108 loss: 1.3088616132736206\n",
            "iteration : 4109 loss: 1.8433356285095215\n",
            "iteration : 4110 loss: 2.0438010692596436\n",
            "iteration : 4111 loss: 1.937620997428894\n",
            "iteration : 4112 loss: 1.8068723678588867\n",
            "iteration : 4113 loss: 1.9620556831359863\n",
            "iteration : 4114 loss: 1.3749408721923828\n",
            "iteration : 4115 loss: 1.3989132642745972\n",
            "iteration : 4116 loss: 1.4373530149459839\n",
            "iteration : 4117 loss: 1.3635517358779907\n",
            "iteration : 4118 loss: 1.2741491794586182\n",
            "iteration : 4119 loss: 1.2609041929244995\n",
            "iteration : 4120 loss: 1.5241693258285522\n",
            "iteration : 4121 loss: 1.7221646308898926\n",
            "iteration : 4122 loss: 1.6135677099227905\n",
            "iteration : 4123 loss: 1.1758873462677002\n",
            "iteration : 4124 loss: 1.3982218503952026\n",
            "iteration : 4125 loss: 1.2606065273284912\n",
            "iteration : 4126 loss: 1.4207638502120972\n",
            "iteration : 4127 loss: 1.0635133981704712\n",
            "iteration : 4128 loss: 1.2526675462722778\n",
            "iteration : 4129 loss: 1.0639201402664185\n",
            "iteration : 4130 loss: 1.3059401512145996\n",
            "iteration : 4131 loss: 1.534738302230835\n",
            "iteration : 4132 loss: 1.164780616760254\n",
            "iteration : 4133 loss: 0.9582659602165222\n",
            "iteration : 4134 loss: 0.9977573752403259\n",
            "iteration : 4135 loss: 0.8685266971588135\n",
            "iteration : 4136 loss: 0.8647706508636475\n",
            "iteration : 4137 loss: 0.993248462677002\n",
            "iteration : 4138 loss: 1.072459101676941\n",
            "iteration : 4139 loss: 0.9184896945953369\n",
            "iteration : 4140 loss: 0.8262603282928467\n",
            "iteration : 4141 loss: 0.7118605971336365\n",
            "iteration : 4142 loss: 1.0265707969665527\n",
            "iteration : 4143 loss: 0.7828660011291504\n",
            "iteration : 4144 loss: 1.1833475828170776\n",
            "iteration : 4145 loss: 1.0703691244125366\n",
            "iteration : 4146 loss: 0.8454195857048035\n",
            "iteration : 4147 loss: 0.7749051451683044\n",
            "iteration : 4148 loss: 1.0174829959869385\n",
            "iteration : 4149 loss: 1.086227297782898\n",
            "iteration : 4150 loss: 1.056178331375122\n",
            "iteration : 4151 loss: 1.1988157033920288\n",
            "iteration : 4152 loss: 0.9644502401351929\n",
            "iteration : 4153 loss: 0.9075266122817993\n",
            "iteration : 4154 loss: 0.80933678150177\n",
            "iteration : 4155 loss: 0.6663810014724731\n",
            "iteration : 4156 loss: 0.9086480140686035\n",
            "iteration : 4157 loss: 0.4774754047393799\n",
            "iteration : 4158 loss: 0.463435560464859\n",
            "iteration : 4159 loss: 0.6358370780944824\n",
            "iteration : 4160 loss: 0.5149679780006409\n",
            "iteration : 4161 loss: 0.5402233004570007\n",
            "iteration : 4162 loss: 1.7144454717636108\n",
            "iteration : 4163 loss: 0.9192516207695007\n",
            "iteration : 4164 loss: 1.0190213918685913\n",
            "iteration : 4165 loss: 1.0125386714935303\n",
            "iteration : 4166 loss: 1.7358258962631226\n",
            "iteration : 4167 loss: 1.4567453861236572\n",
            "iteration : 4168 loss: 0.7606065273284912\n",
            "iteration : 4169 loss: 0.8559765815734863\n",
            "iteration : 4170 loss: 0.7805110216140747\n",
            "iteration : 4171 loss: 0.4976465404033661\n",
            "iteration : 4172 loss: 0.5768529772758484\n",
            "iteration : 4173 loss: 1.0251840353012085\n",
            "iteration : 4174 loss: 0.4901508092880249\n",
            "iteration : 4175 loss: 0.6211311221122742\n",
            "iteration : 4176 loss: 0.8922322392463684\n",
            "iteration : 4177 loss: 0.6828381419181824\n",
            "iteration : 4178 loss: 0.5916851162910461\n",
            "iteration : 4179 loss: 0.48790207505226135\n",
            "iteration : 4180 loss: 1.3261644840240479\n",
            "iteration : 4181 loss: 0.6207367181777954\n",
            "iteration : 4182 loss: 0.4859970510005951\n",
            "iteration : 4183 loss: 1.027340292930603\n",
            "iteration : 4184 loss: 0.6469842791557312\n",
            "iteration : 4185 loss: 0.8759109973907471\n",
            "iteration : 4186 loss: 1.0859346389770508\n",
            "iteration : 4187 loss: 0.8172610402107239\n",
            "iteration : 4188 loss: 1.5322973728179932\n",
            "iteration : 4189 loss: 2.4613802433013916\n",
            "iteration : 4190 loss: 1.003683090209961\n",
            "iteration : 4191 loss: 0.8623216152191162\n",
            "iteration : 4192 loss: 1.0930936336517334\n",
            "iteration : 4193 loss: 1.41129469871521\n",
            "iteration : 4194 loss: 1.202089548110962\n",
            "iteration : 4195 loss: 1.2018654346466064\n",
            "iteration : 4196 loss: 1.1925020217895508\n",
            "iteration : 4197 loss: 1.073045015335083\n",
            "iteration : 4198 loss: 1.1245521306991577\n",
            "iteration : 4199 loss: 1.178792119026184\n",
            "iteration : 4200 loss: 0.7934773564338684\n",
            "iteration : 4201 loss: 0.6099151968955994\n",
            "iteration : 4202 loss: 0.5283868908882141\n",
            "iteration : 4203 loss: 0.49104464054107666\n",
            "iteration : 4204 loss: 0.49298474192619324\n",
            "iteration : 4205 loss: 0.5234946608543396\n",
            "iteration : 4206 loss: 0.4473617970943451\n",
            "iteration : 4207 loss: 0.7900299429893494\n",
            "iteration : 4208 loss: 0.46013343334198\n",
            "iteration : 4209 loss: 0.9104756712913513\n",
            "iteration : 4210 loss: 0.644359290599823\n",
            "iteration : 4211 loss: 0.4141676425933838\n",
            "iteration : 4212 loss: 0.4329385459423065\n",
            "iteration : 4213 loss: 0.6980767846107483\n",
            "iteration : 4214 loss: 0.5461335778236389\n",
            "iteration : 4215 loss: 0.47855642437934875\n",
            "iteration : 4216 loss: 0.49340298771858215\n",
            "iteration : 4217 loss: 0.45830559730529785\n",
            "iteration : 4218 loss: 0.375914067029953\n",
            "iteration : 4219 loss: 0.5797183513641357\n",
            "iteration : 4220 loss: 1.000294804573059\n",
            "iteration : 4221 loss: 0.3963529169559479\n",
            "iteration : 4222 loss: 1.2925660610198975\n",
            "iteration : 4223 loss: 0.38626912236213684\n",
            "iteration : 4224 loss: 0.96586012840271\n",
            "iteration : 4225 loss: 0.65912926197052\n",
            "iteration : 4226 loss: 0.7990944385528564\n",
            "iteration : 4227 loss: 1.5251091718673706\n",
            "iteration : 4228 loss: 1.841604471206665\n",
            "iteration : 4229 loss: 0.9394623637199402\n",
            "iteration : 4230 loss: 1.7035284042358398\n",
            "iteration : 4231 loss: 1.6066858768463135\n",
            "iteration : 4232 loss: 1.6832598447799683\n",
            "iteration : 4233 loss: 1.8932204246520996\n",
            "iteration : 4234 loss: 2.009603500366211\n",
            "iteration : 4235 loss: 1.3869404792785645\n",
            "iteration : 4236 loss: 1.5719091892242432\n",
            "iteration : 4237 loss: 1.0892013311386108\n",
            "iteration : 4238 loss: 1.4778317213058472\n",
            "iteration : 4239 loss: 1.076776146888733\n",
            "iteration : 4240 loss: 1.6442360877990723\n",
            "iteration : 4241 loss: 1.3857333660125732\n",
            "iteration : 4242 loss: 1.1717801094055176\n",
            "iteration : 4243 loss: 1.0181758403778076\n",
            "iteration : 4244 loss: 1.7642712593078613\n",
            "iteration : 4245 loss: 1.211544156074524\n",
            "iteration : 4246 loss: 0.9864534735679626\n",
            "iteration : 4247 loss: 0.9782242774963379\n",
            "iteration : 4248 loss: 0.9812373518943787\n",
            "iteration : 4249 loss: 1.241060495376587\n",
            "iteration : 4250 loss: 1.71083402633667\n",
            "iteration : 4251 loss: 0.8236615061759949\n",
            "iteration : 4252 loss: 0.7845303416252136\n",
            "iteration : 4253 loss: 1.4650850296020508\n",
            "iteration : 4254 loss: 1.2160567045211792\n",
            "iteration : 4255 loss: 1.0883666276931763\n",
            "iteration : 4256 loss: 1.1646602153778076\n",
            "iteration : 4257 loss: 1.541170358657837\n",
            "iteration : 4258 loss: 1.8463563919067383\n",
            "iteration : 4259 loss: 1.6093547344207764\n",
            "iteration : 4260 loss: 1.3038487434387207\n",
            "iteration : 4261 loss: 1.8159180879592896\n",
            "iteration : 4262 loss: 0.9723435044288635\n",
            "iteration : 4263 loss: 1.041724681854248\n",
            "iteration : 4264 loss: 1.1463619470596313\n",
            "iteration : 4265 loss: 1.4365614652633667\n",
            "iteration : 4266 loss: 0.9332137107849121\n",
            "iteration : 4267 loss: 1.0604666471481323\n",
            "iteration : 4268 loss: 1.328637719154358\n",
            "iteration : 4269 loss: 0.8179258704185486\n",
            "iteration : 4270 loss: 1.078311562538147\n",
            "iteration : 4271 loss: 0.9069803357124329\n",
            "iteration : 4272 loss: 0.7527216076850891\n",
            "iteration : 4273 loss: 0.8436839580535889\n",
            "iteration : 4274 loss: 0.6844516396522522\n",
            "iteration : 4275 loss: 0.8260421752929688\n",
            "iteration : 4276 loss: 1.4399542808532715\n",
            "iteration : 4277 loss: 0.8322097063064575\n",
            "iteration : 4278 loss: 0.6259416341781616\n",
            "iteration : 4279 loss: 0.6234514713287354\n",
            "iteration : 4280 loss: 0.5142220258712769\n",
            "iteration : 4281 loss: 0.6978300213813782\n",
            "iteration : 4282 loss: 0.6139775514602661\n",
            "iteration : 4283 loss: 0.8110185265541077\n",
            "iteration : 4284 loss: 1.1450437307357788\n",
            "iteration : 4285 loss: 0.5547100305557251\n",
            "iteration : 4286 loss: 0.6412659287452698\n",
            "iteration : 4287 loss: 0.6964716911315918\n",
            "iteration : 4288 loss: 1.2766040563583374\n",
            "iteration : 4289 loss: 0.5689062476158142\n",
            "iteration : 4290 loss: 0.41109761595726013\n",
            "iteration : 4291 loss: 0.3945273756980896\n",
            "iteration : 4292 loss: 0.6994996070861816\n",
            "iteration : 4293 loss: 0.5547024011611938\n",
            "iteration : 4294 loss: 0.6644150614738464\n",
            "iteration : 4295 loss: 0.3842066526412964\n",
            "iteration : 4296 loss: 0.5308300256729126\n",
            "iteration : 4297 loss: 0.6812306642532349\n",
            "iteration : 4298 loss: 0.7433701753616333\n",
            "iteration : 4299 loss: 0.5735225081443787\n",
            "iteration : 4300 loss: 1.15443754196167\n",
            "iteration : 4301 loss: 0.5591863393783569\n",
            "iteration : 4302 loss: 0.4830152690410614\n",
            "iteration : 4303 loss: 0.3972921371459961\n",
            "iteration : 4304 loss: 0.42846718430519104\n",
            "iteration : 4305 loss: 1.3332079648971558\n",
            "iteration : 4306 loss: 0.9212527871131897\n",
            "iteration : 4307 loss: 0.9516639113426208\n",
            "iteration : 4308 loss: 0.8654593825340271\n",
            "iteration : 4309 loss: 0.43082356452941895\n",
            "iteration : 4310 loss: 0.37312832474708557\n",
            "iteration : 4311 loss: 0.6862648129463196\n",
            "iteration : 4312 loss: 0.6900515556335449\n",
            "iteration : 4313 loss: 0.9775968194007874\n",
            "iteration : 4314 loss: 1.8154805898666382\n",
            "iteration : 4315 loss: 0.5828211307525635\n",
            "iteration : 4316 loss: 0.48386967182159424\n",
            "iteration : 4317 loss: 0.3705604076385498\n",
            "iteration : 4318 loss: 0.3729608952999115\n",
            "iteration : 4319 loss: 0.49667343497276306\n",
            "iteration : 4320 loss: 0.8089896440505981\n",
            "iteration : 4321 loss: 0.41710367798805237\n",
            "iteration : 4322 loss: 0.363943487405777\n",
            "iteration : 4323 loss: 0.35697951912879944\n",
            "iteration : 4324 loss: 0.9588122963905334\n",
            "iteration : 4325 loss: 0.388614684343338\n",
            "iteration : 4326 loss: 0.3628000020980835\n",
            "iteration : 4327 loss: 0.3605286180973053\n",
            "iteration : 4328 loss: 0.3125562071800232\n",
            "iteration : 4329 loss: 0.38949406147003174\n",
            "iteration : 4330 loss: 0.4745621979236603\n",
            "iteration : 4331 loss: 0.28561022877693176\n",
            "iteration : 4332 loss: 0.3883090317249298\n",
            "iteration : 4333 loss: 0.36964619159698486\n",
            "iteration : 4334 loss: 0.7234551906585693\n",
            "iteration : 4335 loss: 0.40526142716407776\n",
            "iteration : 4336 loss: 0.4781639575958252\n",
            "iteration : 4337 loss: 0.5183008909225464\n",
            "iteration : 4338 loss: 0.766772449016571\n",
            "iteration : 4339 loss: 0.27145615220069885\n",
            "iteration : 4340 loss: 0.15788407623767853\n",
            "iteration : 4341 loss: 0.15706752240657806\n",
            "iteration : 4342 loss: 0.545857846736908\n",
            "iteration : 4343 loss: 0.2772083580493927\n",
            "iteration : 4344 loss: 0.8858643174171448\n",
            "iteration : 4345 loss: 0.43087947368621826\n",
            "iteration : 4346 loss: 0.3379519581794739\n",
            "iteration : 4347 loss: 0.5257876515388489\n",
            "iteration : 4348 loss: 0.4041646122932434\n",
            "iteration : 4349 loss: 0.8285897970199585\n",
            "iteration : 4350 loss: 0.9755574464797974\n",
            "iteration : 4351 loss: 0.45212146639823914\n",
            "iteration : 4352 loss: 0.1946084350347519\n",
            "iteration : 4353 loss: 0.06457789242267609\n",
            "iteration : 4354 loss: 0.5445188283920288\n",
            "iteration : 4355 loss: 0.5949694514274597\n",
            "iteration : 4356 loss: 0.24957896769046783\n",
            "iteration : 4357 loss: 0.37731993198394775\n",
            "iteration : 4358 loss: 0.7637203335762024\n",
            "iteration : 4359 loss: 0.5562015771865845\n",
            "iteration : 4360 loss: 0.4408855736255646\n",
            "iteration : 4361 loss: 0.3923415243625641\n",
            "iteration : 4362 loss: 0.5327559113502502\n",
            "iteration : 4363 loss: 0.3920109272003174\n",
            "iteration : 4364 loss: 0.5083634853363037\n",
            "iteration : 4365 loss: 0.34581223130226135\n",
            "iteration : 4366 loss: 0.24231941998004913\n",
            "iteration : 4367 loss: 0.17296475172042847\n",
            "iteration : 4368 loss: 0.23834073543548584\n",
            "iteration : 4369 loss: 0.36142632365226746\n",
            "iteration : 4370 loss: 0.21562747657299042\n",
            "iteration : 4371 loss: 0.23924578726291656\n",
            "iteration : 4372 loss: 0.2052832692861557\n",
            "iteration : 4373 loss: 0.1134113222360611\n",
            "iteration : 4374 loss: 0.31618452072143555\n",
            "iteration : 4375 loss: 0.20700688660144806\n",
            "iteration : 4376 loss: 0.08607477694749832\n",
            "iteration : 4377 loss: 0.33393630385398865\n",
            "iteration : 4378 loss: 0.10743129998445511\n",
            "iteration : 4379 loss: 0.12462669610977173\n",
            "iteration : 4380 loss: 0.07266831398010254\n",
            "iteration : 4381 loss: 0.15974901616573334\n",
            "iteration : 4382 loss: 0.2842760682106018\n",
            "iteration : 4383 loss: 0.7094619274139404\n",
            "iteration : 4384 loss: 0.7557724118232727\n",
            "iteration : 4385 loss: 0.2903190851211548\n",
            "iteration : 4386 loss: 0.04669733718037605\n",
            "iteration : 4387 loss: 0.21761082112789154\n",
            "iteration : 4388 loss: 0.0825643241405487\n",
            "iteration : 4389 loss: 0.23024778068065643\n",
            "iteration : 4390 loss: 0.36236003041267395\n",
            "iteration : 4391 loss: 0.4544592499732971\n",
            "iteration : 4392 loss: 0.23180995881557465\n",
            "iteration : 4393 loss: 0.33372390270233154\n",
            "iteration : 4394 loss: 0.9302904605865479\n",
            "iteration : 4395 loss: 0.1845918744802475\n",
            "iteration : 4396 loss: 0.2227655053138733\n",
            "iteration : 4397 loss: 0.40884122252464294\n",
            "iteration : 4398 loss: 0.5096332430839539\n",
            "iteration : 4399 loss: 0.4257749915122986\n",
            "iteration : 4400 loss: 0.46102118492126465\n",
            "iteration : 4401 loss: 0.1115802600979805\n",
            "iteration : 4402 loss: 0.6981741786003113\n",
            "iteration : 4403 loss: 1.0446386337280273\n",
            "iteration : 4404 loss: 0.3804100751876831\n",
            "iteration : 4405 loss: 0.10148002952337265\n",
            "iteration : 4406 loss: 0.6968739628791809\n",
            "iteration : 4407 loss: 0.446235328912735\n",
            "iteration : 4408 loss: 0.519539475440979\n",
            "iteration : 4409 loss: 0.27053001523017883\n",
            "iteration : 4410 loss: 0.36897405982017517\n",
            "iteration : 4411 loss: 1.9411059617996216\n",
            "iteration : 4412 loss: 3.5482819080352783\n",
            "iteration : 4413 loss: 3.1816246509552\n",
            "iteration : 4414 loss: 2.316520929336548\n",
            "iteration : 4415 loss: 1.504448413848877\n",
            "iteration : 4416 loss: 1.0994703769683838\n",
            "iteration : 4417 loss: 0.8752145767211914\n",
            "iteration : 4418 loss: 0.8706474900245667\n",
            "iteration : 4419 loss: 1.0279778242111206\n",
            "iteration : 4420 loss: 0.6218087673187256\n",
            "iteration : 4421 loss: 0.768277645111084\n",
            "iteration : 4422 loss: 0.7911565899848938\n",
            "iteration : 4423 loss: 1.0899518728256226\n",
            "iteration : 4424 loss: 0.5819769501686096\n",
            "iteration : 4425 loss: 1.459822416305542\n",
            "iteration : 4426 loss: 0.4135514199733734\n",
            "iteration : 4427 loss: 0.47263285517692566\n",
            "iteration : 4428 loss: 0.5967216491699219\n",
            "iteration : 4429 loss: 0.40587183833122253\n",
            "iteration : 4430 loss: 0.6662154793739319\n",
            "iteration : 4431 loss: 1.064813494682312\n",
            "iteration : 4432 loss: 0.14664267003536224\n",
            "iteration : 4433 loss: 0.4126717448234558\n",
            "iteration : 4434 loss: 0.2917977273464203\n",
            "iteration : 4435 loss: 0.2668454051017761\n",
            "iteration : 4436 loss: 1.5969144105911255\n",
            "iteration : 4437 loss: 0.3341852128505707\n",
            "iteration : 4438 loss: 0.1956745833158493\n",
            "iteration : 4439 loss: 1.4299793243408203\n",
            "iteration : 4440 loss: 0.5476807355880737\n",
            "iteration : 4441 loss: 0.1457550972700119\n",
            "iteration : 4442 loss: 0.14419905841350555\n",
            "iteration : 4443 loss: 0.16363732516765594\n",
            "iteration : 4444 loss: 0.11898720264434814\n",
            "iteration : 4445 loss: 0.07316874712705612\n",
            "iteration : 4446 loss: 0.520967423915863\n",
            "iteration : 4447 loss: 1.224923849105835\n",
            "iteration : 4448 loss: 0.12781910598278046\n",
            "iteration : 4449 loss: 0.042807430028915405\n",
            "iteration : 4450 loss: 0.14109231531620026\n",
            "iteration : 4451 loss: 1.1944483518600464\n",
            "iteration : 4452 loss: 0.7481732964515686\n",
            "iteration : 4453 loss: 0.11224959045648575\n",
            "iteration : 4454 loss: 0.14773644506931305\n",
            "iteration : 4455 loss: 0.14959776401519775\n",
            "iteration : 4456 loss: 0.17072032392024994\n",
            "iteration : 4457 loss: 1.020292043685913\n",
            "iteration : 4458 loss: 0.8481443524360657\n",
            "iteration : 4459 loss: 0.6541197299957275\n",
            "iteration : 4460 loss: 0.5764675736427307\n",
            "iteration : 4461 loss: 0.40769484639167786\n",
            "iteration : 4462 loss: 0.8482646942138672\n",
            "iteration : 4463 loss: 1.1191898584365845\n",
            "iteration : 4464 loss: 0.6211506128311157\n",
            "iteration : 4465 loss: 0.6946123838424683\n",
            "iteration : 4466 loss: 1.0491386651992798\n",
            "iteration : 4467 loss: 0.27826765179634094\n",
            "iteration : 4468 loss: 0.8289089798927307\n",
            "iteration : 4469 loss: 0.80544114112854\n",
            "iteration : 4470 loss: 1.368139624595642\n",
            "iteration : 4471 loss: 0.2479601353406906\n",
            "iteration : 4472 loss: 0.8973503112792969\n",
            "iteration : 4473 loss: 0.5585881471633911\n",
            "iteration : 4474 loss: 0.45662203431129456\n",
            "iteration : 4475 loss: 1.2089097499847412\n",
            "iteration : 4476 loss: 0.7688233256340027\n",
            "iteration : 4477 loss: 1.1913789510726929\n",
            "iteration : 4478 loss: 0.47611740231513977\n",
            "iteration : 4479 loss: 0.5392472743988037\n",
            "iteration : 4480 loss: 1.9522792100906372\n",
            "iteration : 4481 loss: 0.23715542256832123\n",
            "iteration : 4482 loss: 0.8900814652442932\n",
            "iteration : 4483 loss: 0.6782469749450684\n",
            "iteration : 4484 loss: 0.5674930214881897\n",
            "iteration : 4485 loss: 0.5382638573646545\n",
            "iteration : 4486 loss: 0.458515465259552\n",
            "iteration : 4487 loss: 0.8456864356994629\n",
            "iteration : 4488 loss: 0.30904364585876465\n",
            "iteration : 4489 loss: 0.8753286600112915\n",
            "iteration : 4490 loss: 0.7170436382293701\n",
            "iteration : 4491 loss: 0.8570308089256287\n",
            "iteration : 4492 loss: 0.4957965016365051\n",
            "iteration : 4493 loss: 0.4858020544052124\n",
            "iteration : 4494 loss: 0.9369696974754333\n",
            "iteration : 4495 loss: 0.3698776066303253\n",
            "iteration : 4496 loss: 0.1824476569890976\n",
            "iteration : 4497 loss: 0.454351544380188\n",
            "iteration : 4498 loss: 0.3007628321647644\n",
            "iteration : 4499 loss: 0.3975904583930969\n",
            "iteration : 4500 loss: 0.7961687445640564\n",
            "iteration : 4501 loss: 0.6389608383178711\n",
            "iteration : 4502 loss: 0.62469482421875\n",
            "iteration : 4503 loss: 0.29033327102661133\n",
            "iteration : 4504 loss: 0.5243763327598572\n",
            "iteration : 4505 loss: 1.4699842929840088\n",
            "iteration : 4506 loss: 0.6547141075134277\n",
            "iteration : 4507 loss: 0.6494288444519043\n",
            "iteration : 4508 loss: 0.6254822015762329\n",
            "iteration : 4509 loss: 0.7029067873954773\n",
            "iteration : 4510 loss: 0.7309398651123047\n",
            "iteration : 4511 loss: 0.6179508566856384\n",
            "iteration : 4512 loss: 0.7568544745445251\n",
            "iteration : 4513 loss: 0.9140915870666504\n",
            "iteration : 4514 loss: 0.4903963804244995\n",
            "iteration : 4515 loss: 0.9366804361343384\n",
            "iteration : 4516 loss: 0.9963954091072083\n",
            "iteration : 4517 loss: 0.6860277056694031\n",
            "iteration : 4518 loss: 0.9101186394691467\n",
            "iteration : 4519 loss: 0.3496476411819458\n",
            "iteration : 4520 loss: 0.6886177062988281\n",
            "iteration : 4521 loss: 1.0009639263153076\n",
            "iteration : 4522 loss: 0.601414144039154\n",
            "iteration : 4523 loss: 0.39971020817756653\n",
            "iteration : 4524 loss: 0.4909098744392395\n",
            "iteration : 4525 loss: 0.49068960547447205\n",
            "iteration : 4526 loss: 0.5451384782791138\n",
            "iteration : 4527 loss: 0.864582359790802\n",
            "iteration : 4528 loss: 0.30508849024772644\n",
            "iteration : 4529 loss: 0.34611088037490845\n",
            "iteration : 4530 loss: 0.24091745913028717\n",
            "iteration : 4531 loss: 0.3514620065689087\n",
            "iteration : 4532 loss: 0.2531658113002777\n",
            "iteration : 4533 loss: 0.24990521371364594\n",
            "iteration : 4534 loss: 0.24913227558135986\n",
            "iteration : 4535 loss: 0.15565279126167297\n",
            "iteration : 4536 loss: 0.3393155038356781\n",
            "iteration : 4537 loss: 0.2089090794324875\n",
            "iteration : 4538 loss: 0.3596286177635193\n",
            "iteration : 4539 loss: 0.41024282574653625\n",
            "iteration : 4540 loss: 0.16535356640815735\n",
            "iteration : 4541 loss: 0.18048220872879028\n",
            "iteration : 4542 loss: 0.15349943935871124\n",
            "iteration : 4543 loss: 0.7851176261901855\n",
            "iteration : 4544 loss: 0.6783604025840759\n",
            "iteration : 4545 loss: 0.7417089939117432\n",
            "iteration : 4546 loss: 0.7626817226409912\n",
            "iteration : 4547 loss: 0.45472925901412964\n",
            "iteration : 4548 loss: 0.6318190693855286\n",
            "iteration : 4549 loss: 0.809865415096283\n",
            "iteration : 4550 loss: 0.7665122747421265\n",
            "iteration : 4551 loss: 0.49138399958610535\n",
            "iteration : 4552 loss: 0.14487795531749725\n",
            "iteration : 4553 loss: 0.3406310975551605\n",
            "iteration : 4554 loss: 0.8073692917823792\n",
            "iteration : 4555 loss: 0.16297945380210876\n",
            "iteration : 4556 loss: 0.6227640509605408\n",
            "iteration : 4557 loss: 0.4036850333213806\n",
            "iteration : 4558 loss: 0.19767801463603973\n",
            "iteration : 4559 loss: 0.4800764322280884\n",
            "iteration : 4560 loss: 0.28825512528419495\n",
            "iteration : 4561 loss: 1.2656714916229248\n",
            "iteration : 4562 loss: 1.4185333251953125\n",
            "iteration : 4563 loss: 0.621957540512085\n",
            "iteration : 4564 loss: 1.1929421424865723\n",
            "iteration : 4565 loss: 0.6046928763389587\n",
            "iteration : 4566 loss: 0.655166745185852\n",
            "iteration : 4567 loss: 0.7228943109512329\n",
            "iteration : 4568 loss: 0.7433772683143616\n",
            "iteration : 4569 loss: 0.6790483593940735\n",
            "iteration : 4570 loss: 0.849197268486023\n",
            "iteration : 4571 loss: 0.8086355924606323\n",
            "iteration : 4572 loss: 0.6307569146156311\n",
            "iteration : 4573 loss: 1.1870801448822021\n",
            "iteration : 4574 loss: 0.8069420456886292\n",
            "iteration : 4575 loss: 0.2840316593647003\n",
            "iteration : 4576 loss: 1.1887189149856567\n",
            "iteration : 4577 loss: 0.7445516586303711\n",
            "iteration : 4578 loss: 0.5191070437431335\n",
            "iteration : 4579 loss: 0.37379783391952515\n",
            "iteration : 4580 loss: 0.6234988570213318\n",
            "iteration : 4581 loss: 0.3917367458343506\n",
            "iteration : 4582 loss: 0.5157843828201294\n",
            "iteration : 4583 loss: 0.5824017524719238\n",
            "iteration : 4584 loss: 0.4399758279323578\n",
            "iteration : 4585 loss: 0.3860708773136139\n",
            "iteration : 4586 loss: 0.8579701781272888\n",
            "iteration : 4587 loss: 0.5877119302749634\n",
            "iteration : 4588 loss: 0.6428825259208679\n",
            "iteration : 4589 loss: 0.7218152284622192\n",
            "iteration : 4590 loss: 0.5675272941589355\n",
            "iteration : 4591 loss: 0.5188498497009277\n",
            "iteration : 4592 loss: 0.2706702649593353\n",
            "iteration : 4593 loss: 0.3304033577442169\n",
            "iteration : 4594 loss: 0.8249198198318481\n",
            "iteration : 4595 loss: 0.44877469539642334\n",
            "iteration : 4596 loss: 0.37438833713531494\n",
            "iteration : 4597 loss: 0.25438186526298523\n",
            "iteration : 4598 loss: 0.2831808924674988\n",
            "iteration : 4599 loss: 0.46605733036994934\n",
            "iteration : 4600 loss: 0.07968102395534515\n",
            "iteration : 4601 loss: 0.18326978385448456\n",
            "iteration : 4602 loss: 0.18794825673103333\n",
            "iteration : 4603 loss: 0.1024293303489685\n",
            "iteration : 4604 loss: 0.05169409140944481\n",
            "iteration : 4605 loss: 0.8242168426513672\n",
            "iteration : 4606 loss: 0.7381545901298523\n",
            "iteration : 4607 loss: 0.2892545759677887\n",
            "iteration : 4608 loss: 0.3208383023738861\n",
            "iteration : 4609 loss: 0.4841625392436981\n",
            "iteration : 4610 loss: 0.35408151149749756\n",
            "iteration : 4611 loss: 0.18302200734615326\n",
            "iteration : 4612 loss: 0.25591394305229187\n",
            "iteration : 4613 loss: 0.2525334358215332\n",
            "iteration : 4614 loss: 0.2876614034175873\n",
            "iteration : 4615 loss: 0.38452792167663574\n",
            "iteration : 4616 loss: 0.8550640344619751\n",
            "iteration : 4617 loss: 0.4103170931339264\n",
            "iteration : 4618 loss: 0.2512858808040619\n",
            "iteration : 4619 loss: 1.1358603239059448\n",
            "iteration : 4620 loss: 0.6326955556869507\n",
            "iteration : 4621 loss: 0.4688577651977539\n",
            "iteration : 4622 loss: 0.47989723086357117\n",
            "iteration : 4623 loss: 0.7014040350914001\n",
            "iteration : 4624 loss: 0.2828880250453949\n",
            "iteration : 4625 loss: 0.8837139010429382\n",
            "iteration : 4626 loss: 0.8691006898880005\n",
            "iteration : 4627 loss: 0.38111168146133423\n",
            "iteration : 4628 loss: 1.0075767040252686\n",
            "iteration : 4629 loss: 0.5829229950904846\n",
            "iteration : 4630 loss: 0.3569210469722748\n",
            "iteration : 4631 loss: 0.7384247779846191\n",
            "iteration : 4632 loss: 0.3975214958190918\n",
            "iteration : 4633 loss: 0.2156084179878235\n",
            "iteration : 4634 loss: 0.26215994358062744\n",
            "iteration : 4635 loss: 0.6745062470436096\n",
            "iteration : 4636 loss: 0.42750152945518494\n",
            "iteration : 4637 loss: 1.5019612312316895\n",
            "iteration : 4638 loss: 0.714055061340332\n",
            "iteration : 4639 loss: 0.846040666103363\n",
            "iteration : 4640 loss: 1.1862200498580933\n",
            "iteration : 4641 loss: 0.3645799458026886\n",
            "iteration : 4642 loss: 0.7153899073600769\n",
            "iteration : 4643 loss: 0.27847620844841003\n",
            "iteration : 4644 loss: 0.5365926623344421\n",
            "iteration : 4645 loss: 0.5262414813041687\n",
            "iteration : 4646 loss: 0.4912804663181305\n",
            "iteration : 4647 loss: 0.30700668692588806\n",
            "iteration : 4648 loss: 0.32252854108810425\n",
            "iteration : 4649 loss: 0.40117987990379333\n",
            "iteration : 4650 loss: 0.4525376558303833\n",
            "iteration : 4651 loss: 0.2705811858177185\n",
            "iteration : 4652 loss: 0.9713194966316223\n",
            "iteration : 4653 loss: 0.9019473791122437\n",
            "iteration : 4654 loss: 1.0820502042770386\n",
            "iteration : 4655 loss: 0.4785856306552887\n",
            "iteration : 4656 loss: 0.39669477939605713\n",
            "iteration : 4657 loss: 0.3551977872848511\n",
            "iteration : 4658 loss: 0.3494637608528137\n",
            "iteration : 4659 loss: 0.3796185851097107\n",
            "iteration : 4660 loss: 0.3134790062904358\n",
            "iteration : 4661 loss: 0.4295526444911957\n",
            "iteration : 4662 loss: 0.5673133134841919\n",
            "iteration : 4663 loss: 0.3640690743923187\n",
            "iteration : 4664 loss: 0.12526090443134308\n",
            "iteration : 4665 loss: 0.20579563081264496\n",
            "iteration : 4666 loss: 0.21887607872486115\n",
            "iteration : 4667 loss: 0.5773276686668396\n",
            "iteration : 4668 loss: 0.5956698060035706\n",
            "iteration : 4669 loss: 1.1634021997451782\n",
            "iteration : 4670 loss: 0.5481190085411072\n",
            "iteration : 4671 loss: 0.1492430567741394\n",
            "iteration : 4672 loss: 0.40625762939453125\n",
            "iteration : 4673 loss: 0.1276216357946396\n",
            "iteration : 4674 loss: 0.23072759807109833\n",
            "iteration : 4675 loss: 0.29599952697753906\n",
            "iteration : 4676 loss: 0.21689538657665253\n",
            "iteration : 4677 loss: 0.8387765288352966\n",
            "iteration : 4678 loss: 0.4981943964958191\n",
            "iteration : 4679 loss: 0.5499855875968933\n",
            "iteration : 4680 loss: 0.7312806248664856\n",
            "iteration : 4681 loss: 0.33606359362602234\n",
            "iteration : 4682 loss: 0.2851685881614685\n",
            "iteration : 4683 loss: 0.08740270882844925\n",
            "iteration : 4684 loss: 0.17059361934661865\n",
            "iteration : 4685 loss: 0.45252519845962524\n",
            "iteration : 4686 loss: 0.9891887903213501\n",
            "iteration : 4687 loss: 0.23769491910934448\n",
            "iteration : 4688 loss: 0.16151340305805206\n",
            "iteration : 4689 loss: 0.11244456470012665\n",
            "iteration : 4690 loss: 0.16497714817523956\n",
            "iteration : 4691 loss: 0.0703781321644783\n",
            "iteration : 4692 loss: 0.3278863728046417\n",
            "iteration : 4693 loss: 1.2508007287979126\n",
            "iteration : 4694 loss: 0.6350987553596497\n",
            "iteration : 4695 loss: 0.18046659231185913\n",
            "iteration : 4696 loss: 0.1419864147901535\n",
            "iteration : 4697 loss: 0.1998613029718399\n",
            "iteration : 4698 loss: 0.8817155361175537\n",
            "iteration : 4699 loss: 0.5866358876228333\n",
            "iteration : 4700 loss: 0.1845097690820694\n",
            "iteration : 4701 loss: 0.7303755879402161\n",
            "iteration : 4702 loss: 0.5417465567588806\n",
            "iteration : 4703 loss: 0.37772613763809204\n",
            "iteration : 4704 loss: 0.12206462770700455\n",
            "iteration : 4705 loss: 0.0874624028801918\n",
            "iteration : 4706 loss: 0.29065901041030884\n",
            "iteration : 4707 loss: 0.5996353626251221\n",
            "iteration : 4708 loss: 0.3561675250530243\n",
            "iteration : 4709 loss: 0.07452019304037094\n",
            "iteration : 4710 loss: 0.20952636003494263\n",
            "iteration : 4711 loss: 0.13478606939315796\n",
            "iteration : 4712 loss: 0.0662764385342598\n",
            "iteration : 4713 loss: 0.21228894591331482\n",
            "iteration : 4714 loss: 0.0539068840444088\n",
            "iteration : 4715 loss: 0.4477798640727997\n",
            "iteration : 4716 loss: 0.18090714514255524\n",
            "iteration : 4717 loss: 0.05142414569854736\n",
            "iteration : 4718 loss: 0.0869123712182045\n",
            "iteration : 4719 loss: 0.03778111934661865\n",
            "iteration : 4720 loss: 1.1280534267425537\n",
            "iteration : 4721 loss: 1.22825026512146\n",
            "iteration : 4722 loss: 0.12411041557788849\n",
            "iteration : 4723 loss: 0.6275514364242554\n",
            "iteration : 4724 loss: 0.39987418055534363\n",
            "iteration : 4725 loss: 0.7547723054885864\n",
            "iteration : 4726 loss: 0.7383247017860413\n",
            "iteration : 4727 loss: 1.0212498903274536\n",
            "iteration : 4728 loss: 0.23710569739341736\n",
            "iteration : 4729 loss: 0.40053293108940125\n",
            "iteration : 4730 loss: 0.161632239818573\n",
            "iteration : 4731 loss: 1.0909392833709717\n",
            "iteration : 4732 loss: 0.1765977293252945\n",
            "iteration : 4733 loss: 0.07415174692869186\n",
            "iteration : 4734 loss: 0.48196089267730713\n",
            "iteration : 4735 loss: 0.415535032749176\n",
            "iteration : 4736 loss: 0.12943844497203827\n",
            "iteration : 4737 loss: 0.2685980200767517\n",
            "iteration : 4738 loss: 0.26114001870155334\n",
            "iteration : 4739 loss: 0.40005481243133545\n",
            "iteration : 4740 loss: 0.19158755242824554\n",
            "iteration : 4741 loss: 0.2726665139198303\n",
            "iteration : 4742 loss: 0.21186743676662445\n",
            "iteration : 4743 loss: 1.0786464214324951\n",
            "iteration : 4744 loss: 0.12927888333797455\n",
            "iteration : 4745 loss: 0.09102996438741684\n",
            "iteration : 4746 loss: 0.7271953225135803\n",
            "iteration : 4747 loss: 0.535521924495697\n",
            "iteration : 4748 loss: 0.24452006816864014\n",
            "iteration : 4749 loss: 0.06287463754415512\n",
            "iteration : 4750 loss: 0.22567354142665863\n",
            "iteration : 4751 loss: 0.08405566960573196\n",
            "iteration : 4752 loss: 0.017785657197237015\n",
            "iteration : 4753 loss: 0.09225142747163773\n",
            "iteration : 4754 loss: 0.3663159906864166\n",
            "iteration : 4755 loss: 0.6020488739013672\n",
            "iteration : 4756 loss: 0.5827199220657349\n",
            "iteration : 4757 loss: 0.11420407146215439\n",
            "iteration : 4758 loss: 0.48963525891304016\n",
            "iteration : 4759 loss: 0.33645790815353394\n",
            "iteration : 4760 loss: 0.1864473819732666\n",
            "iteration : 4761 loss: 0.23984743654727936\n",
            "iteration : 4762 loss: 0.4355011582374573\n",
            "iteration : 4763 loss: 0.24154643714427948\n",
            "iteration : 4764 loss: 0.7212929129600525\n",
            "iteration : 4765 loss: 0.5855791568756104\n",
            "iteration : 4766 loss: 0.1299610584974289\n",
            "iteration : 4767 loss: 0.2344355583190918\n",
            "iteration : 4768 loss: 0.26845723390579224\n",
            "iteration : 4769 loss: 0.18147780001163483\n",
            "iteration : 4770 loss: 0.7586676478385925\n",
            "iteration : 4771 loss: 0.0641041100025177\n",
            "iteration : 4772 loss: 0.06370386481285095\n",
            "iteration : 4773 loss: 0.11879340559244156\n",
            "iteration : 4774 loss: 0.21310587227344513\n",
            "iteration : 4775 loss: 0.43774551153182983\n",
            "iteration : 4776 loss: 0.48376262187957764\n",
            "iteration : 4777 loss: 0.11393614858388901\n",
            "iteration : 4778 loss: 0.7499441504478455\n",
            "iteration : 4779 loss: 0.23750583827495575\n",
            "iteration : 4780 loss: 0.1680719256401062\n",
            "iteration : 4781 loss: 0.10953880101442337\n",
            "iteration : 4782 loss: 0.4503069221973419\n",
            "iteration : 4783 loss: 0.13298457860946655\n",
            "iteration : 4784 loss: 0.06872178614139557\n",
            "iteration : 4785 loss: 0.2939532399177551\n",
            "iteration : 4786 loss: 0.45127975940704346\n",
            "iteration : 4787 loss: 0.21599440276622772\n",
            "iteration : 4788 loss: 0.10143781453371048\n",
            "iteration : 4789 loss: 0.07487580925226212\n",
            "iteration : 4790 loss: 0.1227969378232956\n",
            "iteration : 4791 loss: 0.24869373440742493\n",
            "iteration : 4792 loss: 0.061897117644548416\n",
            "iteration : 4793 loss: 0.017999110743403435\n",
            "iteration : 4794 loss: 0.021781297400593758\n",
            "iteration : 4795 loss: 0.023335717618465424\n",
            "iteration : 4796 loss: 0.040434446185827255\n",
            "iteration : 4797 loss: 0.46388378739356995\n",
            "iteration : 4798 loss: 0.03641222417354584\n",
            "iteration : 4799 loss: 0.018399391323328018\n",
            "iteration : 4800 loss: 0.052459608763456345\n",
            "iteration : 4801 loss: 0.06833962351083755\n",
            "iteration : 4802 loss: 0.2658328115940094\n",
            "iteration : 4803 loss: 0.12255153059959412\n",
            "iteration : 4804 loss: 0.807786226272583\n",
            "iteration : 4805 loss: 0.20108990371227264\n",
            "iteration : 4806 loss: 0.25034579634666443\n",
            "iteration : 4807 loss: 0.06712432950735092\n",
            "iteration : 4808 loss: 0.06650510430335999\n",
            "iteration : 4809 loss: 0.25888964533805847\n",
            "iteration : 4810 loss: 0.33077290654182434\n",
            "iteration : 4811 loss: 0.02757013402879238\n",
            "iteration : 4812 loss: 0.006248515099287033\n",
            "iteration : 4813 loss: 1.2675472497940063\n",
            "iteration : 4814 loss: 0.1169721856713295\n",
            "iteration : 4815 loss: 0.14694388210773468\n",
            "iteration : 4816 loss: 0.3330021798610687\n",
            "iteration : 4817 loss: 0.20050130784511566\n",
            "iteration : 4818 loss: 0.17810769379138947\n",
            "iteration : 4819 loss: 0.2955280840396881\n",
            "iteration : 4820 loss: 0.21294817328453064\n",
            "iteration : 4821 loss: 0.007362900767475367\n",
            "iteration : 4822 loss: 0.05223805084824562\n",
            "iteration : 4823 loss: 0.04240218177437782\n",
            "iteration : 4824 loss: 0.16359943151474\n",
            "iteration : 4825 loss: 0.12152069807052612\n",
            "iteration : 4826 loss: 0.06229917332530022\n",
            "iteration : 4827 loss: 0.0324433259665966\n",
            "iteration : 4828 loss: 0.13827167451381683\n",
            "iteration : 4829 loss: 0.09275288134813309\n",
            "iteration : 4830 loss: 0.02220364473760128\n",
            "iteration : 4831 loss: 0.14450354874134064\n",
            "iteration : 4832 loss: 0.21425853669643402\n",
            "iteration : 4833 loss: 0.07586681097745895\n",
            "iteration : 4834 loss: 0.022533070296049118\n",
            "iteration : 4835 loss: 0.09194069355726242\n",
            "iteration : 4836 loss: 0.04668872803449631\n",
            "iteration : 4837 loss: 0.04395049810409546\n",
            "iteration : 4838 loss: 0.03907297924160957\n",
            "iteration : 4839 loss: 0.050836049020290375\n",
            "iteration : 4840 loss: 0.028068285435438156\n",
            "iteration : 4841 loss: 0.04498759284615517\n",
            "iteration : 4842 loss: 0.034113168716430664\n",
            "iteration : 4843 loss: 0.0760575458407402\n",
            "iteration : 4844 loss: 0.02089359052479267\n",
            "iteration : 4845 loss: 0.029161112383008003\n",
            "iteration : 4846 loss: 0.03055664710700512\n",
            "iteration : 4847 loss: 0.0073259249329566956\n",
            "iteration : 4848 loss: 0.02770378813147545\n",
            "iteration : 4849 loss: 0.028486376628279686\n",
            "iteration : 4850 loss: 0.020469320937991142\n",
            "iteration : 4851 loss: 0.0848022997379303\n",
            "iteration : 4852 loss: 0.05476038530468941\n",
            "iteration : 4853 loss: 0.042707160115242004\n",
            "iteration : 4854 loss: 0.025062866508960724\n",
            "iteration : 4855 loss: 0.00571403419598937\n",
            "iteration : 4856 loss: 0.016445282846689224\n",
            "iteration : 4857 loss: 0.013224813155829906\n",
            "iteration : 4858 loss: 0.004546247888356447\n",
            "iteration : 4859 loss: 0.008203896693885326\n",
            "iteration : 4860 loss: 0.023397574201226234\n",
            "iteration : 4861 loss: 0.7851696014404297\n",
            "iteration : 4862 loss: 0.0573088638484478\n",
            "iteration : 4863 loss: 0.032097525894641876\n",
            "iteration : 4864 loss: 0.14491404592990875\n",
            "iteration : 4865 loss: 0.17894800007343292\n",
            "iteration : 4866 loss: 0.03346233814954758\n",
            "iteration : 4867 loss: 0.08430715650320053\n",
            "iteration : 4868 loss: 0.029967645183205605\n",
            "iteration : 4869 loss: 0.32917320728302\n",
            "iteration : 4870 loss: 0.02823738381266594\n",
            "iteration : 4871 loss: 1.0389041900634766\n",
            "iteration : 4872 loss: 0.3270310163497925\n",
            "iteration : 4873 loss: 0.5180914998054504\n",
            "iteration : 4874 loss: 0.21559010446071625\n",
            "iteration : 4875 loss: 0.14551736414432526\n",
            "iteration : 4876 loss: 0.20010456442832947\n",
            "iteration : 4877 loss: 0.04234840348362923\n",
            "iteration : 4878 loss: 0.0194566547870636\n",
            "iteration : 4879 loss: 0.2074371576309204\n",
            "iteration : 4880 loss: 0.2493206262588501\n",
            "iteration : 4881 loss: 0.3038887679576874\n",
            "iteration : 4882 loss: 0.4594871997833252\n",
            "iteration : 4883 loss: 0.5339354276657104\n",
            "iteration : 4884 loss: 0.5722631216049194\n",
            "iteration : 4885 loss: 0.3271220624446869\n",
            "iteration : 4886 loss: 0.11731395125389099\n",
            "iteration : 4887 loss: 0.24644507467746735\n",
            "iteration : 4888 loss: 0.4885798394680023\n",
            "iteration : 4889 loss: 0.14049091935157776\n",
            "iteration : 4890 loss: 0.028086023405194283\n",
            "iteration : 4891 loss: 1.3001724481582642\n",
            "iteration : 4892 loss: 0.30587923526763916\n",
            "iteration : 4893 loss: 0.2711210250854492\n",
            "iteration : 4894 loss: 0.40739351511001587\n",
            "iteration : 4895 loss: 1.0618456602096558\n",
            "iteration : 4896 loss: 0.30074018239974976\n",
            "iteration : 4897 loss: 0.6522106528282166\n",
            "iteration : 4898 loss: 0.1250464767217636\n",
            "iteration : 4899 loss: 0.025948822498321533\n",
            "iteration : 4900 loss: 0.02101370133459568\n",
            "iteration : 4901 loss: 0.01336886826902628\n",
            "iteration : 4902 loss: 0.008818951435387135\n",
            "iteration : 4903 loss: 0.010073650628328323\n",
            "iteration : 4904 loss: 1.3610444068908691\n",
            "iteration : 4905 loss: 0.10849063843488693\n",
            "iteration : 4906 loss: 0.10394269973039627\n",
            "iteration : 4907 loss: 0.6135281324386597\n",
            "iteration : 4908 loss: 0.8689207434654236\n",
            "iteration : 4909 loss: 0.9343513250350952\n",
            "iteration : 4910 loss: 0.6118677854537964\n",
            "iteration : 4911 loss: 0.25147897005081177\n",
            "iteration : 4912 loss: 0.04734627529978752\n",
            "iteration : 4913 loss: 0.4753299951553345\n",
            "iteration : 4914 loss: 0.31517866253852844\n",
            "iteration : 4915 loss: 0.06854923814535141\n",
            "iteration : 4916 loss: 0.23343801498413086\n",
            "iteration : 4917 loss: 0.1409720927476883\n",
            "iteration : 4918 loss: 0.3697916567325592\n",
            "iteration : 4919 loss: 0.14718496799468994\n",
            "iteration : 4920 loss: 0.7383116483688354\n",
            "iteration : 4921 loss: 0.8739676475524902\n",
            "iteration : 4922 loss: 0.004264738876372576\n",
            "iteration : 4923 loss: 0.34936925768852234\n",
            "iteration : 4924 loss: 0.11317574977874756\n",
            "iteration : 4925 loss: 0.7377780079841614\n",
            "iteration : 4926 loss: 0.7269065380096436\n",
            "iteration : 4927 loss: 0.5255602598190308\n",
            "iteration : 4928 loss: 0.39802086353302\n",
            "iteration : 4929 loss: 0.29498621821403503\n",
            "iteration : 4930 loss: 0.5629472732543945\n",
            "iteration : 4931 loss: 0.43846702575683594\n",
            "iteration : 4932 loss: 0.22461935877799988\n",
            "iteration : 4933 loss: 0.4211922585964203\n",
            "iteration : 4934 loss: 0.26506415009498596\n",
            "iteration : 4935 loss: 0.008808142505586147\n",
            "iteration : 4936 loss: 0.4883235692977905\n",
            "iteration : 4937 loss: 0.07196719199419022\n",
            "iteration : 4938 loss: 0.3904303014278412\n",
            "iteration : 4939 loss: 0.07488080859184265\n",
            "iteration : 4940 loss: 0.4886793792247772\n",
            "iteration : 4941 loss: 0.365560919046402\n",
            "iteration : 4942 loss: 0.0767681822180748\n",
            "iteration : 4943 loss: 0.03477320075035095\n",
            "iteration : 4944 loss: 0.43043550848960876\n",
            "iteration : 4945 loss: 0.13796131312847137\n",
            "iteration : 4946 loss: 0.10502427816390991\n",
            "iteration : 4947 loss: 0.17393246293067932\n",
            "iteration : 4948 loss: 0.6303278207778931\n",
            "iteration : 4949 loss: 0.7023626565933228\n",
            "iteration : 4950 loss: 0.34127679467201233\n",
            "iteration : 4951 loss: 0.34889280796051025\n",
            "iteration : 4952 loss: 0.7512317895889282\n",
            "iteration : 4953 loss: 0.13455146551132202\n",
            "iteration : 4954 loss: 0.24031738936901093\n",
            "iteration : 4955 loss: 0.11368345469236374\n",
            "iteration : 4956 loss: 0.008747111074626446\n",
            "iteration : 4957 loss: 0.12554015219211578\n",
            "iteration : 4958 loss: 0.4390895366668701\n",
            "iteration : 4959 loss: 0.13986454904079437\n",
            "iteration : 4960 loss: 0.21887853741645813\n",
            "iteration : 4961 loss: 0.18329158425331116\n",
            "iteration : 4962 loss: 0.1541578769683838\n",
            "iteration : 4963 loss: 0.40002429485321045\n",
            "iteration : 4964 loss: 0.32885631918907166\n",
            "iteration : 4965 loss: 0.09582877904176712\n",
            "iteration : 4966 loss: 0.021753979846835136\n",
            "iteration : 4967 loss: 0.08387695997953415\n",
            "iteration : 4968 loss: 0.0309774037450552\n",
            "iteration : 4969 loss: 0.031660616397857666\n",
            "iteration : 4970 loss: 0.07753626257181168\n",
            "iteration : 4971 loss: 0.025460679084062576\n",
            "iteration : 4972 loss: 0.007378770038485527\n",
            "iteration : 4973 loss: 0.15916140377521515\n",
            "iteration : 4974 loss: 0.8415012359619141\n",
            "iteration : 4975 loss: 0.04686959832906723\n",
            "iteration : 4976 loss: 0.07361271232366562\n",
            "iteration : 4977 loss: 0.16065721213817596\n",
            "iteration : 4978 loss: 0.08229344338178635\n",
            "iteration : 4979 loss: 0.010687322355806828\n",
            "iteration : 4980 loss: 0.05605935677886009\n",
            "iteration : 4981 loss: 0.05532336235046387\n",
            "iteration : 4982 loss: 0.21945731341838837\n",
            "iteration : 4983 loss: 0.12837421894073486\n",
            "iteration : 4984 loss: 0.04980340227484703\n",
            "iteration : 4985 loss: 0.027596691623330116\n",
            "iteration : 4986 loss: 0.13587723672389984\n",
            "iteration : 4987 loss: 0.0037186546251177788\n",
            "iteration : 4988 loss: 0.02933245711028576\n",
            "iteration : 4989 loss: 0.10351360589265823\n",
            "iteration : 4990 loss: 0.0420030914247036\n",
            "iteration : 4991 loss: 0.31411048769950867\n",
            "iteration : 4992 loss: 0.5389989614486694\n",
            "iteration : 4993 loss: 0.05968299135565758\n",
            "iteration : 4994 loss: 0.06803321093320847\n",
            "iteration : 4995 loss: 0.18236494064331055\n",
            "iteration : 4996 loss: 0.10385963320732117\n",
            "iteration : 4997 loss: 0.0853143110871315\n",
            "iteration : 4998 loss: 0.005353536456823349\n",
            "iteration : 4999 loss: 0.028400689363479614\n",
            "iteration : 5000 loss: 0.009379326365888119\n",
            "iteration : 5001 loss: 0.301093727350235\n",
            "iteration : 5002 loss: 0.04253175109624863\n",
            "iteration : 5003 loss: 0.03688850253820419\n",
            "iteration : 5004 loss: 0.00915859080851078\n",
            "iteration : 5005 loss: 0.03134112060070038\n",
            "iteration : 5006 loss: 0.013375780545175076\n",
            "iteration : 5007 loss: 0.0788479819893837\n",
            "iteration : 5008 loss: 0.013589022681117058\n",
            "iteration : 5009 loss: 0.14416104555130005\n",
            "iteration : 5010 loss: 0.044894859194755554\n",
            "iteration : 5011 loss: 0.26488199830055237\n",
            "iteration : 5012 loss: 0.4741435945034027\n",
            "iteration : 5013 loss: 0.07331516593694687\n",
            "iteration : 5014 loss: 0.007484095636755228\n",
            "iteration : 5015 loss: 0.05902215838432312\n",
            "iteration : 5016 loss: 0.05429534986615181\n",
            "iteration : 5017 loss: 0.26335906982421875\n",
            "iteration : 5018 loss: 0.43061941862106323\n",
            "iteration : 5019 loss: 0.014893394894897938\n",
            "iteration : 5020 loss: 0.055448587983846664\n",
            "iteration : 5021 loss: 0.23059050738811493\n",
            "iteration : 5022 loss: 0.13942298293113708\n",
            "iteration : 5023 loss: 0.05664779618382454\n",
            "iteration : 5024 loss: 0.1349187046289444\n",
            "iteration : 5025 loss: 0.21345318853855133\n",
            "iteration : 5026 loss: 0.08791926503181458\n",
            "iteration : 5027 loss: 0.04055138677358627\n",
            "iteration : 5028 loss: 0.04988647997379303\n",
            "iteration : 5029 loss: 0.021785032004117966\n",
            "iteration : 5030 loss: 0.020403197035193443\n",
            "iteration : 5031 loss: 0.003983696922659874\n",
            "iteration : 5032 loss: 0.01255329791456461\n",
            "iteration : 5033 loss: 0.017622599378228188\n",
            "iteration : 5034 loss: 0.03913147374987602\n",
            "iteration : 5035 loss: 0.020454516634345055\n",
            "iteration : 5036 loss: 0.12579061090946198\n",
            "iteration : 5037 loss: 0.07199215143918991\n",
            "iteration : 5038 loss: 0.024446837604045868\n",
            "iteration : 5039 loss: 0.007195251062512398\n",
            "iteration : 5040 loss: 0.016492677852511406\n",
            "iteration : 5041 loss: 0.0091354139149189\n",
            "iteration : 5042 loss: 0.01660722680389881\n",
            "iteration : 5043 loss: 0.0234471894800663\n",
            "iteration : 5044 loss: 0.1899154633283615\n",
            "iteration : 5045 loss: 0.08026211708784103\n",
            "iteration : 5046 loss: 0.029661498963832855\n",
            "iteration : 5047 loss: 0.015017026104032993\n",
            "iteration : 5048 loss: 0.007196368649601936\n",
            "iteration : 5049 loss: 0.017325259745121002\n",
            "iteration : 5050 loss: 0.1260521113872528\n",
            "iteration : 5051 loss: 0.060822077095508575\n",
            "iteration : 5052 loss: 0.09879759699106216\n",
            "iteration : 5053 loss: 0.2915010154247284\n",
            "iteration : 5054 loss: 0.037349309772253036\n",
            "iteration : 5055 loss: 0.17773354053497314\n",
            "iteration : 5056 loss: 0.09979477524757385\n",
            "iteration : 5057 loss: 0.09747035801410675\n",
            "iteration : 5058 loss: 0.03228554502129555\n",
            "iteration : 5059 loss: 0.11249186098575592\n",
            "iteration : 5060 loss: 0.03892539069056511\n",
            "iteration : 5061 loss: 0.010495883412659168\n",
            "iteration : 5062 loss: 0.015819381922483444\n",
            "iteration : 5063 loss: 0.017508169636130333\n",
            "iteration : 5064 loss: 0.04332795366644859\n",
            "iteration : 5065 loss: 0.3500480651855469\n",
            "iteration : 5066 loss: 0.03617590665817261\n",
            "iteration : 5067 loss: 0.014275950379669666\n",
            "iteration : 5068 loss: 0.09921782463788986\n",
            "iteration : 5069 loss: 0.016360756009817123\n",
            "iteration : 5070 loss: 0.04475588724017143\n",
            "iteration : 5071 loss: 0.015865327790379524\n",
            "iteration : 5072 loss: 0.023826448246836662\n",
            "iteration : 5073 loss: 0.006700216792523861\n",
            "iteration : 5074 loss: 0.02853335067629814\n",
            "iteration : 5075 loss: 0.008946073241531849\n",
            "iteration : 5076 loss: 0.029066234827041626\n",
            "iteration : 5077 loss: 0.021834729239344597\n",
            "iteration : 5078 loss: 0.02072613686323166\n",
            "iteration : 5079 loss: 0.014324312098324299\n",
            "iteration : 5080 loss: 0.009392435662448406\n",
            "iteration : 5081 loss: 0.005545210558921099\n",
            "iteration : 5082 loss: 0.005568806082010269\n",
            "iteration : 5083 loss: 0.03863972797989845\n",
            "iteration : 5084 loss: 0.01870194636285305\n",
            "iteration : 5085 loss: 0.47500383853912354\n",
            "iteration : 5086 loss: 0.010586682707071304\n",
            "iteration : 5087 loss: 0.024891212582588196\n",
            "iteration : 5088 loss: 0.03224528953433037\n",
            "iteration : 5089 loss: 0.009863673709332943\n",
            "iteration : 5090 loss: 0.007776506710797548\n",
            "iteration : 5091 loss: 0.22933952510356903\n",
            "iteration : 5092 loss: 0.012181742116808891\n",
            "iteration : 5093 loss: 0.017094114795327187\n",
            "iteration : 5094 loss: 0.01047971099615097\n",
            "iteration : 5095 loss: 0.013012365438044071\n",
            "iteration : 5096 loss: 0.011772995814681053\n",
            "iteration : 5097 loss: 0.005102195311337709\n",
            "iteration : 5098 loss: 0.019131116569042206\n",
            "iteration : 5099 loss: 0.008157499134540558\n",
            "iteration : 5100 loss: 0.004496163222938776\n",
            "iteration : 5101 loss: 0.019993390887975693\n",
            "iteration : 5102 loss: 0.04413837939500809\n",
            "iteration : 5103 loss: 0.003949958365410566\n",
            "iteration : 5104 loss: 0.008191977627575397\n",
            "iteration : 5105 loss: 0.3148127496242523\n",
            "iteration : 5106 loss: 0.20289932191371918\n",
            "iteration : 5107 loss: 0.009094173088669777\n",
            "iteration : 5108 loss: 0.007536865770816803\n",
            "iteration : 5109 loss: 0.003326921258121729\n",
            "iteration : 5110 loss: 0.00884457491338253\n",
            "iteration : 5111 loss: 0.08785276859998703\n",
            "iteration : 5112 loss: 0.0038407661486417055\n",
            "iteration : 5113 loss: 0.23670871555805206\n",
            "iteration : 5114 loss: 0.1571076512336731\n",
            "iteration : 5115 loss: 0.09145330637693405\n",
            "iteration : 5116 loss: 0.06477949768304825\n",
            "iteration : 5117 loss: 0.19207222759723663\n",
            "iteration : 5118 loss: 0.0481293685734272\n",
            "iteration : 5119 loss: 0.01212293654680252\n",
            "iteration : 5120 loss: 0.006934117525815964\n",
            "iteration : 5121 loss: 0.07128070294857025\n",
            "iteration : 5122 loss: 0.03755263239145279\n",
            "iteration : 5123 loss: 0.14102908968925476\n",
            "iteration : 5124 loss: 0.10745888948440552\n",
            "iteration : 5125 loss: 0.002546243602409959\n",
            "iteration : 5126 loss: 0.005338655784726143\n",
            "iteration : 5127 loss: 0.155878946185112\n",
            "iteration : 5128 loss: 0.002755816327407956\n",
            "iteration : 5129 loss: 0.0294621754437685\n",
            "iteration : 5130 loss: 0.005610113497823477\n",
            "iteration : 5131 loss: 0.002210287842899561\n",
            "iteration : 5132 loss: 0.0069317505694925785\n",
            "iteration : 5133 loss: 0.23243892192840576\n",
            "iteration : 5134 loss: 0.9968698024749756\n",
            "iteration : 5135 loss: 0.026785505935549736\n",
            "iteration : 5136 loss: 0.3922136425971985\n",
            "iteration : 5137 loss: 0.1518564373254776\n",
            "iteration : 5138 loss: 0.34105974435806274\n",
            "iteration : 5139 loss: 0.02604498527944088\n",
            "iteration : 5140 loss: 0.626814067363739\n",
            "iteration : 5141 loss: 0.05323014035820961\n",
            "iteration : 5142 loss: 0.22001740336418152\n",
            "iteration : 5143 loss: 0.4808027148246765\n",
            "iteration : 5144 loss: 0.14886175096035004\n",
            "iteration : 5145 loss: 0.15012848377227783\n",
            "iteration : 5146 loss: 0.13420309126377106\n",
            "iteration : 5147 loss: 0.03257668763399124\n",
            "iteration : 5148 loss: 0.04407699778676033\n",
            "iteration : 5149 loss: 0.10301736742258072\n",
            "iteration : 5150 loss: 0.077364481985569\n",
            "iteration : 5151 loss: 0.29761481285095215\n",
            "iteration : 5152 loss: 0.7150803208351135\n",
            "iteration : 5153 loss: 0.473470002412796\n",
            "iteration : 5154 loss: 0.14857307076454163\n",
            "iteration : 5155 loss: 0.2982833683490753\n",
            "iteration : 5156 loss: 0.430923193693161\n",
            "iteration : 5157 loss: 0.11054982990026474\n",
            "iteration : 5158 loss: 0.38042497634887695\n",
            "iteration : 5159 loss: 0.28847962617874146\n",
            "iteration : 5160 loss: 0.3177335858345032\n",
            "iteration : 5161 loss: 0.287953644990921\n",
            "iteration : 5162 loss: 0.19270378351211548\n",
            "iteration : 5163 loss: 0.1412210315465927\n",
            "iteration : 5164 loss: 0.055521197617053986\n",
            "iteration : 5165 loss: 0.055342573672533035\n",
            "iteration : 5166 loss: 0.43177419900894165\n",
            "iteration : 5167 loss: 0.16599588096141815\n",
            "iteration : 5168 loss: 0.27595511078834534\n",
            "iteration : 5169 loss: 0.31767556071281433\n",
            "iteration : 5170 loss: 0.08205266296863556\n",
            "iteration : 5171 loss: 0.10862631350755692\n",
            "iteration : 5172 loss: 0.18359701335430145\n",
            "iteration : 5173 loss: 0.07918699830770493\n",
            "iteration : 5174 loss: 0.18581019341945648\n",
            "iteration : 5175 loss: 0.08798518031835556\n",
            "iteration : 5176 loss: 0.08518582582473755\n",
            "iteration : 5177 loss: 0.04638838768005371\n",
            "iteration : 5178 loss: 0.07657615095376968\n",
            "iteration : 5179 loss: 0.13340409100055695\n",
            "iteration : 5180 loss: 0.5019683241844177\n",
            "iteration : 5181 loss: 0.575374960899353\n",
            "iteration : 5182 loss: 0.09359682351350784\n",
            "iteration : 5183 loss: 0.2352505475282669\n",
            "iteration : 5184 loss: 0.7024741172790527\n",
            "iteration : 5185 loss: 0.149889275431633\n",
            "iteration : 5186 loss: 0.28642508387565613\n",
            "iteration : 5187 loss: 0.40109553933143616\n",
            "iteration : 5188 loss: 0.24576926231384277\n",
            "iteration : 5189 loss: 0.29330378770828247\n",
            "iteration : 5190 loss: 0.06963156163692474\n",
            "iteration : 5191 loss: 0.31305423378944397\n",
            "iteration : 5192 loss: 0.32415661215782166\n",
            "iteration : 5193 loss: 0.09827083349227905\n",
            "iteration : 5194 loss: 0.3972024619579315\n",
            "iteration : 5195 loss: 0.3833766281604767\n",
            "iteration : 5196 loss: 0.4869531989097595\n",
            "iteration : 5197 loss: 0.19114473462104797\n",
            "iteration : 5198 loss: 0.9961683750152588\n",
            "iteration : 5199 loss: 0.27343225479125977\n",
            "iteration : 5200 loss: 1.1706626415252686\n",
            "iteration : 5201 loss: 0.1621110439300537\n",
            "iteration : 5202 loss: 0.29364073276519775\n",
            "iteration : 5203 loss: 0.22958406805992126\n",
            "iteration : 5204 loss: 0.41085392236709595\n",
            "iteration : 5205 loss: 0.1621912270784378\n",
            "iteration : 5206 loss: 0.17160716652870178\n",
            "iteration : 5207 loss: 0.17819102108478546\n",
            "iteration : 5208 loss: 0.21882154047489166\n",
            "iteration : 5209 loss: 0.1650526225566864\n",
            "iteration : 5210 loss: 0.051005180925130844\n",
            "iteration : 5211 loss: 0.033705636858940125\n",
            "iteration : 5212 loss: 0.1649956852197647\n",
            "iteration : 5213 loss: 0.1549544334411621\n",
            "iteration : 5214 loss: 0.09776657074689865\n",
            "iteration : 5215 loss: 0.12936268746852875\n",
            "iteration : 5216 loss: 0.3601821959018707\n",
            "iteration : 5217 loss: 0.12431009113788605\n",
            "iteration : 5218 loss: 0.1441868096590042\n",
            "iteration : 5219 loss: 0.07319466769695282\n",
            "iteration : 5220 loss: 0.38023701310157776\n",
            "iteration : 5221 loss: 0.06362020969390869\n",
            "iteration : 5222 loss: 0.15284642577171326\n",
            "iteration : 5223 loss: 0.1494159996509552\n",
            "iteration : 5224 loss: 0.1752660572528839\n",
            "iteration : 5225 loss: 0.22927702963352203\n",
            "iteration : 5226 loss: 0.2418462038040161\n",
            "iteration : 5227 loss: 0.0902465283870697\n",
            "iteration : 5228 loss: 0.12167128920555115\n",
            "iteration : 5229 loss: 0.020102012902498245\n",
            "iteration : 5230 loss: 0.09151767194271088\n",
            "iteration : 5231 loss: 0.01568485051393509\n",
            "iteration : 5232 loss: 0.012754806317389011\n",
            "iteration : 5233 loss: 0.007243956904858351\n",
            "iteration : 5234 loss: 0.012015636079013348\n",
            "iteration : 5235 loss: 0.01895941235125065\n",
            "iteration : 5236 loss: 0.008682115003466606\n",
            "iteration : 5237 loss: 0.019407054409384727\n",
            "iteration : 5238 loss: 0.01135504525154829\n",
            "iteration : 5239 loss: 0.0069082025438547134\n",
            "iteration : 5240 loss: 0.019195517525076866\n",
            "iteration : 5241 loss: 0.006809282582253218\n",
            "iteration : 5242 loss: 0.008260305039584637\n",
            "iteration : 5243 loss: 0.006179001182317734\n",
            "iteration : 5244 loss: 0.021214907988905907\n",
            "iteration : 5245 loss: 0.020657410845160484\n",
            "iteration : 5246 loss: 0.12339405715465546\n",
            "iteration : 5247 loss: 0.07829798012971878\n",
            "iteration : 5248 loss: 0.0050471872091293335\n",
            "iteration : 5249 loss: 0.011424234136939049\n",
            "iteration : 5250 loss: 0.008701068349182606\n",
            "iteration : 5251 loss: 0.0038825974334031343\n",
            "iteration : 5252 loss: 0.04088026285171509\n",
            "iteration : 5253 loss: 0.14214162528514862\n",
            "iteration : 5254 loss: 0.03858967125415802\n",
            "iteration : 5255 loss: 0.01407904364168644\n",
            "iteration : 5256 loss: 0.014116816222667694\n",
            "iteration : 5257 loss: 3.222968816757202\n",
            "iteration : 5258 loss: 0.01310753170400858\n",
            "iteration : 5259 loss: 0.004946981556713581\n",
            "iteration : 5260 loss: 0.8434461951255798\n",
            "iteration : 5261 loss: 0.008019477128982544\n",
            "iteration : 5262 loss: 0.02389805018901825\n",
            "iteration : 5263 loss: 0.2415865957736969\n",
            "iteration : 5264 loss: 0.9474077224731445\n",
            "iteration : 5265 loss: 0.38953492045402527\n",
            "iteration : 5266 loss: 0.24995675683021545\n",
            "iteration : 5267 loss: 1.8957287073135376\n",
            "iteration : 5268 loss: 1.503920316696167\n",
            "iteration : 5269 loss: 1.4423285722732544\n",
            "iteration : 5270 loss: 0.25629541277885437\n",
            "iteration : 5271 loss: 0.5618010759353638\n",
            "iteration : 5272 loss: 2.919468879699707\n",
            "iteration : 5273 loss: 1.3555388450622559\n",
            "iteration : 5274 loss: 1.2761104106903076\n",
            "iteration : 5275 loss: 0.48517993092536926\n",
            "iteration : 5276 loss: 1.7258400917053223\n",
            "iteration : 5277 loss: 0.8800697326660156\n",
            "iteration : 5278 loss: 1.0965193510055542\n",
            "iteration : 5279 loss: 0.8860541582107544\n",
            "iteration : 5280 loss: 1.0361806154251099\n",
            "iteration : 5281 loss: 1.3332768678665161\n",
            "iteration : 5282 loss: 1.0315440893173218\n",
            "iteration : 5283 loss: 1.2433234453201294\n",
            "iteration : 5284 loss: 0.5949656367301941\n",
            "iteration : 5285 loss: 0.7021749019622803\n",
            "iteration : 5286 loss: 0.780963659286499\n",
            "iteration : 5287 loss: 0.6593719124794006\n",
            "iteration : 5288 loss: 0.9553026556968689\n",
            "iteration : 5289 loss: 1.6473512649536133\n",
            "iteration : 5290 loss: 0.8388633131980896\n",
            "iteration : 5291 loss: 0.7055310606956482\n",
            "iteration : 5292 loss: 0.9387426376342773\n",
            "iteration : 5293 loss: 0.5845937132835388\n",
            "iteration : 5294 loss: 0.6201680898666382\n",
            "iteration : 5295 loss: 0.6645263433456421\n",
            "iteration : 5296 loss: 0.6012594103813171\n",
            "iteration : 5297 loss: 0.7657756805419922\n",
            "iteration : 5298 loss: 0.2987636625766754\n",
            "iteration : 5299 loss: 1.0115305185317993\n",
            "iteration : 5300 loss: 0.47730374336242676\n",
            "iteration : 5301 loss: 0.4609893560409546\n",
            "iteration : 5302 loss: 0.3911277651786804\n",
            "iteration : 5303 loss: 0.7465593814849854\n",
            "iteration : 5304 loss: 1.1528371572494507\n",
            "iteration : 5305 loss: 0.5077211856842041\n",
            "iteration : 5306 loss: 0.38690850138664246\n",
            "iteration : 5307 loss: 0.6832822561264038\n",
            "iteration : 5308 loss: 0.13569271564483643\n",
            "iteration : 5309 loss: 0.20350521802902222\n",
            "iteration : 5310 loss: 0.23042826354503632\n",
            "iteration : 5311 loss: 0.7963453531265259\n",
            "iteration : 5312 loss: 0.5152230858802795\n",
            "iteration : 5313 loss: 0.3953171372413635\n",
            "iteration : 5314 loss: 0.6112484931945801\n",
            "iteration : 5315 loss: 0.41045695543289185\n",
            "iteration : 5316 loss: 0.5394811034202576\n",
            "iteration : 5317 loss: 0.3975927531719208\n",
            "iteration : 5318 loss: 0.3595035970211029\n",
            "iteration : 5319 loss: 0.5280838012695312\n",
            "iteration : 5320 loss: 0.6116452813148499\n",
            "iteration : 5321 loss: 0.5136967301368713\n",
            "iteration : 5322 loss: 0.4212243854999542\n",
            "iteration : 5323 loss: 0.23574088513851166\n",
            "iteration : 5324 loss: 0.46263036131858826\n",
            "iteration : 5325 loss: 0.25581926107406616\n",
            "iteration : 5326 loss: 0.17892424762248993\n",
            "iteration : 5327 loss: 0.2954899072647095\n",
            "iteration : 5328 loss: 0.5564568042755127\n",
            "iteration : 5329 loss: 0.6582198143005371\n",
            "iteration : 5330 loss: 0.19230730831623077\n",
            "iteration : 5331 loss: 0.2825821340084076\n",
            "iteration : 5332 loss: 0.37481623888015747\n",
            "iteration : 5333 loss: 0.9655212759971619\n",
            "iteration : 5334 loss: 0.21354426443576813\n",
            "iteration : 5335 loss: 0.2787461578845978\n",
            "iteration : 5336 loss: 0.33315756916999817\n",
            "iteration : 5337 loss: 0.10921584814786911\n",
            "iteration : 5338 loss: 0.4728514850139618\n",
            "iteration : 5339 loss: 1.3456261157989502\n",
            "iteration : 5340 loss: 0.13773871958255768\n",
            "iteration : 5341 loss: 0.4533315896987915\n",
            "iteration : 5342 loss: 0.2939356565475464\n",
            "iteration : 5343 loss: 0.32647112011909485\n",
            "iteration : 5344 loss: 0.4302464425563812\n",
            "iteration : 5345 loss: 0.3619747757911682\n",
            "iteration : 5346 loss: 0.11343254148960114\n",
            "iteration : 5347 loss: 0.5455310344696045\n",
            "iteration : 5348 loss: 0.21706123650074005\n",
            "iteration : 5349 loss: 0.27429378032684326\n",
            "iteration : 5350 loss: 0.45207318663597107\n",
            "iteration : 5351 loss: 0.22861890494823456\n",
            "iteration : 5352 loss: 1.376025676727295\n",
            "iteration : 5353 loss: 0.17657122015953064\n",
            "iteration : 5354 loss: 0.33271220326423645\n",
            "iteration : 5355 loss: 0.8194636106491089\n",
            "iteration : 5356 loss: 0.23037755489349365\n",
            "iteration : 5357 loss: 0.241548553109169\n",
            "iteration : 5358 loss: 0.22407005727291107\n",
            "iteration : 5359 loss: 0.23308633267879486\n",
            "iteration : 5360 loss: 0.510982871055603\n",
            "iteration : 5361 loss: 2.74259352684021\n",
            "iteration : 5362 loss: 1.450792670249939\n",
            "iteration : 5363 loss: 0.10285904258489609\n",
            "iteration : 5364 loss: 0.19134269654750824\n",
            "iteration : 5365 loss: 0.01595515012741089\n",
            "iteration : 5366 loss: 0.05442381277680397\n",
            "iteration : 5367 loss: 0.28833940625190735\n",
            "iteration : 5368 loss: 0.05701911821961403\n",
            "iteration : 5369 loss: 0.11682510375976562\n",
            "iteration : 5370 loss: 0.6115130186080933\n",
            "iteration : 5371 loss: 1.1973938941955566\n",
            "iteration : 5372 loss: 0.07327122986316681\n",
            "iteration : 5373 loss: 0.27218785881996155\n",
            "iteration : 5374 loss: 0.5788214802742004\n",
            "iteration : 5375 loss: 0.508442759513855\n",
            "iteration : 5376 loss: 0.2778647243976593\n",
            "iteration : 5377 loss: 0.3313775658607483\n",
            "iteration : 5378 loss: 0.3820321261882782\n",
            "iteration : 5379 loss: 0.22688427567481995\n",
            "iteration : 5380 loss: 0.44835996627807617\n",
            "iteration : 5381 loss: 0.7359700798988342\n",
            "iteration : 5382 loss: 0.30418890714645386\n",
            "iteration : 5383 loss: 0.33810579776763916\n",
            "iteration : 5384 loss: 0.5444874167442322\n",
            "iteration : 5385 loss: 0.3076472580432892\n",
            "iteration : 5386 loss: 0.29527807235717773\n",
            "iteration : 5387 loss: 0.21108810603618622\n",
            "iteration : 5388 loss: 0.21293769776821136\n",
            "iteration : 5389 loss: 0.16114354133605957\n",
            "iteration : 5390 loss: 0.17973637580871582\n",
            "iteration : 5391 loss: 0.05306684598326683\n",
            "iteration : 5392 loss: 0.0388781912624836\n",
            "iteration : 5393 loss: 0.037215862423181534\n",
            "iteration : 5394 loss: 0.5133395791053772\n",
            "iteration : 5395 loss: 0.31890180706977844\n",
            "iteration : 5396 loss: 0.06842273473739624\n",
            "iteration : 5397 loss: 0.076571024954319\n",
            "iteration : 5398 loss: 0.10355820506811142\n",
            "iteration : 5399 loss: 0.05451805889606476\n",
            "iteration : 5400 loss: 0.1734458953142166\n",
            "iteration : 5401 loss: 0.2656378149986267\n",
            "iteration : 5402 loss: 0.1831224411725998\n",
            "iteration : 5403 loss: 0.714500367641449\n",
            "iteration : 5404 loss: 0.5169057250022888\n",
            "iteration : 5405 loss: 0.18553031980991364\n",
            "iteration : 5406 loss: 0.17063653469085693\n",
            "iteration : 5407 loss: 0.12792335450649261\n",
            "iteration : 5408 loss: 0.04246680438518524\n",
            "iteration : 5409 loss: 0.06264122575521469\n",
            "iteration : 5410 loss: 0.07415562868118286\n",
            "iteration : 5411 loss: 0.17181305587291718\n",
            "iteration : 5412 loss: 0.14318619668483734\n",
            "iteration : 5413 loss: 0.02981448918581009\n",
            "iteration : 5414 loss: 0.022110842168331146\n",
            "iteration : 5415 loss: 0.036736730486154556\n",
            "iteration : 5416 loss: 0.05147464945912361\n",
            "iteration : 5417 loss: 0.0430280826985836\n",
            "iteration : 5418 loss: 0.050560351461172104\n",
            "iteration : 5419 loss: 0.32277360558509827\n",
            "iteration : 5420 loss: 0.061476293951272964\n",
            "iteration : 5421 loss: 1.3870753049850464\n",
            "iteration : 5422 loss: 0.03616642579436302\n",
            "iteration : 5423 loss: 0.13752993941307068\n",
            "iteration : 5424 loss: 1.5833535194396973\n",
            "iteration : 5425 loss: 0.8567475080490112\n",
            "iteration : 5426 loss: 2.153613805770874\n",
            "iteration : 5427 loss: 0.820700466632843\n",
            "iteration : 5428 loss: 0.5853825211524963\n",
            "iteration : 5429 loss: 1.9244496822357178\n",
            "iteration : 5430 loss: 1.1770130395889282\n",
            "iteration : 5431 loss: 1.374970555305481\n",
            "iteration : 5432 loss: 0.8004408478736877\n",
            "iteration : 5433 loss: 0.8277419209480286\n",
            "iteration : 5434 loss: 1.8727123737335205\n",
            "iteration : 5435 loss: 0.3561398983001709\n",
            "iteration : 5436 loss: 0.30905503034591675\n",
            "iteration : 5437 loss: 0.38338473439216614\n",
            "iteration : 5438 loss: 0.25060734152793884\n",
            "iteration : 5439 loss: 1.9057753086090088\n",
            "iteration : 5440 loss: 0.6683741211891174\n",
            "iteration : 5441 loss: 0.7007734775543213\n",
            "iteration : 5442 loss: 1.1267189979553223\n",
            "iteration : 5443 loss: 1.6433348655700684\n",
            "iteration : 5444 loss: 1.0056827068328857\n",
            "iteration : 5445 loss: 0.781008780002594\n",
            "iteration : 5446 loss: 0.7440316081047058\n",
            "iteration : 5447 loss: 0.32120481133461\n",
            "iteration : 5448 loss: 0.8046554923057556\n",
            "iteration : 5449 loss: 1.0902880430221558\n",
            "iteration : 5450 loss: 0.9010969996452332\n",
            "iteration : 5451 loss: 0.46214166283607483\n",
            "iteration : 5452 loss: 0.3868612051010132\n",
            "iteration : 5453 loss: 1.899072289466858\n",
            "iteration : 5454 loss: 0.3850780427455902\n",
            "iteration : 5455 loss: 0.41747376322746277\n",
            "iteration : 5456 loss: 0.7100940346717834\n",
            "iteration : 5457 loss: 0.2386346161365509\n",
            "iteration : 5458 loss: 0.04590770974755287\n",
            "iteration : 5459 loss: 0.7676480412483215\n",
            "iteration : 5460 loss: 0.8666275143623352\n",
            "iteration : 5461 loss: 0.2638384699821472\n",
            "iteration : 5462 loss: 0.7546373605728149\n",
            "iteration : 5463 loss: 0.49248120188713074\n",
            "iteration : 5464 loss: 0.3603893518447876\n",
            "iteration : 5465 loss: 0.23897184431552887\n",
            "iteration : 5466 loss: 0.6048900485038757\n",
            "iteration : 5467 loss: 0.16225439310073853\n",
            "iteration : 5468 loss: 0.34565433859825134\n",
            "iteration : 5469 loss: 0.308474063873291\n",
            "iteration : 5470 loss: 0.4735828638076782\n",
            "iteration : 5471 loss: 0.3461829721927643\n",
            "iteration : 5472 loss: 0.20926715433597565\n",
            "iteration : 5473 loss: 1.0220762491226196\n",
            "iteration : 5474 loss: 0.7212890982627869\n",
            "iteration : 5475 loss: 0.2990579605102539\n",
            "iteration : 5476 loss: 0.7510439157485962\n",
            "iteration : 5477 loss: 0.2803708612918854\n",
            "iteration : 5478 loss: 0.2940541207790375\n",
            "iteration : 5479 loss: 0.09110193699598312\n",
            "iteration : 5480 loss: 0.2522374987602234\n",
            "iteration : 5481 loss: 0.461183100938797\n",
            "iteration : 5482 loss: 0.2920997738838196\n",
            "iteration : 5483 loss: 0.2979188561439514\n",
            "iteration : 5484 loss: 0.42222532629966736\n",
            "iteration : 5485 loss: 0.29031887650489807\n",
            "iteration : 5486 loss: 0.1884528249502182\n",
            "iteration : 5487 loss: 0.25502488017082214\n",
            "iteration : 5488 loss: 1.0618565082550049\n",
            "iteration : 5489 loss: 0.23984670639038086\n",
            "iteration : 5490 loss: 1.0439841747283936\n",
            "iteration : 5491 loss: 0.7041986584663391\n",
            "iteration : 5492 loss: 0.6027016043663025\n",
            "iteration : 5493 loss: 0.42649638652801514\n",
            "iteration : 5494 loss: 0.09014783799648285\n",
            "iteration : 5495 loss: 0.45027804374694824\n",
            "iteration : 5496 loss: 0.2104969620704651\n",
            "iteration : 5497 loss: 0.12999629974365234\n",
            "iteration : 5498 loss: 0.3849382996559143\n",
            "iteration : 5499 loss: 0.32903721928596497\n",
            "iteration : 5500 loss: 0.10756225138902664\n",
            "iteration : 5501 loss: 0.1337633728981018\n",
            "iteration : 5502 loss: 0.13991056382656097\n",
            "iteration : 5503 loss: 0.1854826956987381\n",
            "iteration : 5504 loss: 0.15464171767234802\n",
            "iteration : 5505 loss: 0.012189511209726334\n",
            "iteration : 5506 loss: 0.12215977162122726\n",
            "iteration : 5507 loss: 0.08648793399333954\n",
            "iteration : 5508 loss: 0.20493803918361664\n",
            "iteration : 5509 loss: 0.08098172396421432\n",
            "iteration : 5510 loss: 0.04967048019170761\n",
            "iteration : 5511 loss: 0.08483074605464935\n",
            "iteration : 5512 loss: 0.08954191207885742\n",
            "iteration : 5513 loss: 0.16378580033779144\n",
            "iteration : 5514 loss: 0.3649779260158539\n",
            "iteration : 5515 loss: 0.2666412591934204\n",
            "iteration : 5516 loss: 0.11359076201915741\n",
            "iteration : 5517 loss: 0.3516295850276947\n",
            "iteration : 5518 loss: 0.18156015872955322\n",
            "iteration : 5519 loss: 0.07179056853055954\n",
            "iteration : 5520 loss: 0.11192500591278076\n",
            "iteration : 5521 loss: 0.07216232270002365\n",
            "iteration : 5522 loss: 0.14409145712852478\n",
            "iteration : 5523 loss: 0.08446131646633148\n",
            "iteration : 5524 loss: 0.21840031445026398\n",
            "iteration : 5525 loss: 0.13500016927719116\n",
            "iteration : 5526 loss: 0.03223539888858795\n",
            "iteration : 5527 loss: 0.5687688589096069\n",
            "iteration : 5528 loss: 0.24038994312286377\n",
            "iteration : 5529 loss: 0.04370817914605141\n",
            "iteration : 5530 loss: 0.18530382215976715\n",
            "iteration : 5531 loss: 0.10995478183031082\n",
            "iteration : 5532 loss: 0.07039006799459457\n",
            "iteration : 5533 loss: 0.6382553577423096\n",
            "iteration : 5534 loss: 0.06591803580522537\n",
            "iteration : 5535 loss: 0.09627620875835419\n",
            "iteration : 5536 loss: 0.20140112936496735\n",
            "iteration : 5537 loss: 0.08242837339639664\n",
            "iteration : 5538 loss: 0.01663285307586193\n",
            "iteration : 5539 loss: 0.12205719202756882\n",
            "iteration : 5540 loss: 0.08594817668199539\n",
            "iteration : 5541 loss: 0.44223353266716003\n",
            "iteration : 5542 loss: 0.3946683406829834\n",
            "iteration : 5543 loss: 0.023645278066396713\n",
            "iteration : 5544 loss: 0.05961304530501366\n",
            "iteration : 5545 loss: 0.07677042484283447\n",
            "iteration : 5546 loss: 0.28798097372055054\n",
            "iteration : 5547 loss: 0.3035911023616791\n",
            "iteration : 5548 loss: 0.021426469087600708\n",
            "iteration : 5549 loss: 0.024909580126404762\n",
            "iteration : 5550 loss: 0.1001710519194603\n",
            "iteration : 5551 loss: 0.32232585549354553\n",
            "iteration : 5552 loss: 0.14968177676200867\n",
            "iteration : 5553 loss: 0.1732293963432312\n",
            "iteration : 5554 loss: 0.022508515045046806\n",
            "iteration : 5555 loss: 0.0351351723074913\n",
            "iteration : 5556 loss: 0.0995049774646759\n",
            "iteration : 5557 loss: 0.1324748396873474\n",
            "iteration : 5558 loss: 0.12475646287202835\n",
            "iteration : 5559 loss: 0.04872630909085274\n",
            "iteration : 5560 loss: 0.047487206757068634\n",
            "iteration : 5561 loss: 0.025231502950191498\n",
            "iteration : 5562 loss: 0.03546128794550896\n",
            "iteration : 5563 loss: 0.04063401743769646\n",
            "iteration : 5564 loss: 0.033253058791160583\n",
            "iteration : 5565 loss: 0.14130152761936188\n",
            "iteration : 5566 loss: 0.21120713651180267\n",
            "iteration : 5567 loss: 0.15381227433681488\n",
            "iteration : 5568 loss: 0.23430763185024261\n",
            "iteration : 5569 loss: 0.07355378568172455\n",
            "iteration : 5570 loss: 0.10514836758375168\n",
            "iteration : 5571 loss: 0.04388546198606491\n",
            "iteration : 5572 loss: 0.07194206863641739\n",
            "iteration : 5573 loss: 0.10257630795240402\n",
            "iteration : 5574 loss: 0.03953195735812187\n",
            "iteration : 5575 loss: 0.011292845010757446\n",
            "iteration : 5576 loss: 0.11870697885751724\n",
            "iteration : 5577 loss: 0.02997937612235546\n",
            "iteration : 5578 loss: 0.012165844440460205\n",
            "iteration : 5579 loss: 0.020825952291488647\n",
            "iteration : 5580 loss: 0.060220327228307724\n",
            "iteration : 5581 loss: 0.10377370566129684\n",
            "iteration : 5582 loss: 0.13680608570575714\n",
            "iteration : 5583 loss: 0.03604534640908241\n",
            "iteration : 5584 loss: 0.01763683557510376\n",
            "iteration : 5585 loss: 0.050105754286050797\n",
            "iteration : 5586 loss: 0.04827585071325302\n",
            "iteration : 5587 loss: 0.0361253023147583\n",
            "iteration : 5588 loss: 0.03686235472559929\n",
            "iteration : 5589 loss: 0.022140109911561012\n",
            "iteration : 5590 loss: 0.3637041449546814\n",
            "iteration : 5591 loss: 0.1875886619091034\n",
            "iteration : 5592 loss: 0.8825769424438477\n",
            "iteration : 5593 loss: 0.061217743903398514\n",
            "iteration : 5594 loss: 0.3148919343948364\n",
            "iteration : 5595 loss: 0.15241530537605286\n",
            "iteration : 5596 loss: 0.04816975072026253\n",
            "iteration : 5597 loss: 0.06025446206331253\n",
            "iteration : 5598 loss: 0.04096114635467529\n",
            "iteration : 5599 loss: 0.03261358663439751\n",
            "iteration : 5600 loss: 0.14563246071338654\n",
            "iteration : 5601 loss: 0.14495213329792023\n",
            "iteration : 5602 loss: 0.09738780558109283\n",
            "iteration : 5603 loss: 0.012068287469446659\n",
            "iteration : 5604 loss: 0.02511819265782833\n",
            "iteration : 5605 loss: 0.1808198094367981\n",
            "iteration : 5606 loss: 0.019390059635043144\n",
            "iteration : 5607 loss: 0.43757888674736023\n",
            "iteration : 5608 loss: 0.5837801098823547\n",
            "iteration : 5609 loss: 0.35236847400665283\n",
            "iteration : 5610 loss: 0.42721042037010193\n",
            "iteration : 5611 loss: 0.3337424695491791\n",
            "iteration : 5612 loss: 0.7538236379623413\n",
            "iteration : 5613 loss: 1.7837839126586914\n",
            "iteration : 5614 loss: 0.29971086978912354\n",
            "iteration : 5615 loss: 0.8219377994537354\n",
            "iteration : 5616 loss: 0.4036010503768921\n",
            "iteration : 5617 loss: 0.18247655034065247\n",
            "iteration : 5618 loss: 1.1793099641799927\n",
            "iteration : 5619 loss: 0.993284285068512\n",
            "iteration : 5620 loss: 0.7019040584564209\n",
            "iteration : 5621 loss: 0.5752034187316895\n",
            "iteration : 5622 loss: 0.9085319638252258\n",
            "iteration : 5623 loss: 0.7410978674888611\n",
            "iteration : 5624 loss: 0.5625349879264832\n",
            "iteration : 5625 loss: 0.5562429428100586\n",
            "iteration : 5626 loss: 0.7052385807037354\n",
            "iteration : 5627 loss: 0.5338671207427979\n",
            "iteration : 5628 loss: 0.47072502970695496\n",
            "iteration : 5629 loss: 0.6037226915359497\n",
            "iteration : 5630 loss: 1.3642363548278809\n",
            "iteration : 5631 loss: 1.5041207075119019\n",
            "iteration : 5632 loss: 0.8572179675102234\n",
            "iteration : 5633 loss: 1.2539867162704468\n",
            "iteration : 5634 loss: 0.9479799866676331\n",
            "iteration : 5635 loss: 0.9987155795097351\n",
            "iteration : 5636 loss: 1.4239537715911865\n",
            "iteration : 5637 loss: 1.6289609670639038\n",
            "iteration : 5638 loss: 1.494821548461914\n",
            "iteration : 5639 loss: 1.2671488523483276\n",
            "iteration : 5640 loss: 0.9923975467681885\n",
            "iteration : 5641 loss: 0.739913284778595\n",
            "iteration : 5642 loss: 1.5257188081741333\n",
            "iteration : 5643 loss: 1.6472665071487427\n",
            "iteration : 5644 loss: 0.739601731300354\n",
            "iteration : 5645 loss: 0.6999539732933044\n",
            "iteration : 5646 loss: 4.9514079093933105\n",
            "iteration : 5647 loss: 2.1530821323394775\n",
            "iteration : 5648 loss: 2.9959304332733154\n",
            "iteration : 5649 loss: 2.8772084712982178\n",
            "iteration : 5650 loss: 2.3914830684661865\n",
            "iteration : 5651 loss: 1.201115369796753\n",
            "iteration : 5652 loss: 2.332982063293457\n",
            "iteration : 5653 loss: 4.835759162902832\n",
            "iteration : 5654 loss: 3.372734546661377\n",
            "iteration : 5655 loss: 2.3197457790374756\n",
            "iteration : 5656 loss: 2.239741325378418\n",
            "iteration : 5657 loss: 2.2678191661834717\n",
            "iteration : 5658 loss: 2.3129770755767822\n",
            "iteration : 5659 loss: 2.685364007949829\n",
            "iteration : 5660 loss: 2.921637535095215\n",
            "iteration : 5661 loss: 2.2008931636810303\n",
            "iteration : 5662 loss: 3.650228261947632\n",
            "iteration : 5663 loss: 3.5333752632141113\n",
            "iteration : 5664 loss: 4.149398326873779\n",
            "iteration : 5665 loss: 3.1710329055786133\n",
            "iteration : 5666 loss: 3.2363855838775635\n",
            "iteration : 5667 loss: 2.930671453475952\n",
            "iteration : 5668 loss: 2.9123995304107666\n",
            "iteration : 5669 loss: 3.14701247215271\n",
            "iteration : 5670 loss: 3.392122507095337\n",
            "iteration : 5671 loss: 3.0471668243408203\n",
            "iteration : 5672 loss: 3.012463092803955\n",
            "iteration : 5673 loss: 2.9409303665161133\n",
            "iteration : 5674 loss: 2.806016445159912\n",
            "iteration : 5675 loss: 2.9792232513427734\n",
            "iteration : 5676 loss: 3.030906915664673\n",
            "iteration : 5677 loss: 2.888328790664673\n",
            "iteration : 5678 loss: 2.8073108196258545\n",
            "iteration : 5679 loss: 2.7687201499938965\n",
            "iteration : 5680 loss: 2.722978353500366\n",
            "iteration : 5681 loss: 2.661027669906616\n",
            "iteration : 5682 loss: 3.0375802516937256\n",
            "iteration : 5683 loss: 2.8519442081451416\n",
            "iteration : 5684 loss: 2.896669864654541\n",
            "iteration : 5685 loss: 2.8969790935516357\n",
            "iteration : 5686 loss: 2.5997397899627686\n",
            "iteration : 5687 loss: 2.69525408744812\n",
            "iteration : 5688 loss: 2.986656665802002\n",
            "iteration : 5689 loss: 2.6814727783203125\n",
            "iteration : 5690 loss: 2.9507904052734375\n",
            "iteration : 5691 loss: 2.7661988735198975\n",
            "iteration : 5692 loss: 2.8185102939605713\n",
            "iteration : 5693 loss: 2.714560031890869\n",
            "iteration : 5694 loss: 2.7285208702087402\n",
            "iteration : 5695 loss: 2.8390910625457764\n",
            "iteration : 5696 loss: 2.70920729637146\n",
            "iteration : 5697 loss: 2.702380418777466\n",
            "iteration : 5698 loss: 2.6053452491760254\n",
            "iteration : 5699 loss: 2.5766000747680664\n",
            "iteration : 5700 loss: 2.8614375591278076\n",
            "iteration : 5701 loss: 2.5511398315429688\n",
            "iteration : 5702 loss: 2.777979850769043\n",
            "iteration : 5703 loss: 2.411120891571045\n",
            "iteration : 5704 loss: 3.055917739868164\n",
            "iteration : 5705 loss: 3.022189140319824\n",
            "iteration : 5706 loss: 2.7460365295410156\n",
            "iteration : 5707 loss: 2.3743491172790527\n",
            "iteration : 5708 loss: 2.5983195304870605\n",
            "iteration : 5709 loss: 3.2981677055358887\n",
            "iteration : 5710 loss: 2.7300918102264404\n",
            "iteration : 5711 loss: 3.008021116256714\n",
            "iteration : 5712 loss: 3.3585422039031982\n",
            "iteration : 5713 loss: 2.6595685482025146\n",
            "iteration : 5714 loss: 2.847071886062622\n",
            "iteration : 5715 loss: 2.698524236679077\n",
            "iteration : 5716 loss: 2.9944324493408203\n",
            "iteration : 5717 loss: 2.8563790321350098\n",
            "iteration : 5718 loss: 3.1430246829986572\n",
            "iteration : 5719 loss: 3.179140329360962\n",
            "iteration : 5720 loss: 2.6336233615875244\n",
            "iteration : 5721 loss: 2.6532833576202393\n",
            "iteration : 5722 loss: 2.6995022296905518\n",
            "iteration : 5723 loss: 2.698237419128418\n",
            "iteration : 5724 loss: 2.660195827484131\n",
            "iteration : 5725 loss: 2.5974185466766357\n",
            "iteration : 5726 loss: 2.608994245529175\n",
            "iteration : 5727 loss: 2.6781749725341797\n",
            "iteration : 5728 loss: 2.5583512783050537\n",
            "iteration : 5729 loss: 2.658719062805176\n",
            "iteration : 5730 loss: 2.7862045764923096\n",
            "iteration : 5731 loss: 2.301115036010742\n",
            "iteration : 5732 loss: 2.624732255935669\n",
            "iteration : 5733 loss: 2.4217424392700195\n",
            "iteration : 5734 loss: 2.2078676223754883\n",
            "iteration : 5735 loss: 3.2971415519714355\n",
            "iteration : 5736 loss: 2.354069948196411\n",
            "iteration : 5737 loss: 2.573470115661621\n",
            "iteration : 5738 loss: 2.6994783878326416\n",
            "iteration : 5739 loss: 2.241870880126953\n",
            "iteration : 5740 loss: 2.883223533630371\n",
            "iteration : 5741 loss: 3.06156063079834\n",
            "iteration : 5742 loss: 3.042848825454712\n",
            "iteration : 5743 loss: 3.1098577976226807\n",
            "iteration : 5744 loss: 3.2371416091918945\n",
            "iteration : 5745 loss: 2.9322855472564697\n",
            "iteration : 5746 loss: 2.5828192234039307\n",
            "iteration : 5747 loss: 2.7041728496551514\n",
            "iteration : 5748 loss: 2.5050320625305176\n",
            "iteration : 5749 loss: 2.6280174255371094\n",
            "iteration : 5750 loss: 3.069640636444092\n",
            "iteration : 5751 loss: 3.0374932289123535\n",
            "iteration : 5752 loss: 2.3005239963531494\n",
            "iteration : 5753 loss: 3.0276529788970947\n",
            "iteration : 5754 loss: 2.796724796295166\n",
            "iteration : 5755 loss: 3.231689691543579\n",
            "iteration : 5756 loss: 2.523627281188965\n",
            "iteration : 5757 loss: 2.246961832046509\n",
            "iteration : 5758 loss: 2.772460699081421\n",
            "iteration : 5759 loss: 3.338939666748047\n",
            "iteration : 5760 loss: 2.6489059925079346\n",
            "iteration : 5761 loss: 2.5324721336364746\n",
            "iteration : 5762 loss: 2.9327797889709473\n",
            "iteration : 5763 loss: 2.7443535327911377\n",
            "iteration : 5764 loss: 2.9636576175689697\n",
            "iteration : 5765 loss: 2.3239152431488037\n",
            "iteration : 5766 loss: 2.592303991317749\n",
            "iteration : 5767 loss: 2.612985849380493\n",
            "iteration : 5768 loss: 2.5907702445983887\n",
            "iteration : 5769 loss: 2.605679750442505\n",
            "iteration : 5770 loss: 2.5735864639282227\n",
            "iteration : 5771 loss: 2.8188905715942383\n",
            "iteration : 5772 loss: 3.0325305461883545\n",
            "iteration : 5773 loss: 2.634927988052368\n",
            "iteration : 5774 loss: 3.2277917861938477\n",
            "iteration : 5775 loss: 3.018265724182129\n",
            "iteration : 5776 loss: 2.7518317699432373\n",
            "iteration : 5777 loss: 2.753308057785034\n",
            "iteration : 5778 loss: 2.7123148441314697\n",
            "iteration : 5779 loss: 2.7740232944488525\n",
            "iteration : 5780 loss: 2.9833033084869385\n",
            "iteration : 5781 loss: 2.651047468185425\n",
            "iteration : 5782 loss: 3.0167012214660645\n",
            "iteration : 5783 loss: 3.217926025390625\n",
            "iteration : 5784 loss: 2.731428384780884\n",
            "iteration : 5785 loss: 2.682387351989746\n",
            "iteration : 5786 loss: 2.814040184020996\n",
            "iteration : 5787 loss: 2.6331799030303955\n",
            "iteration : 5788 loss: 2.620291233062744\n",
            "iteration : 5789 loss: 2.7542314529418945\n",
            "iteration : 5790 loss: 2.7860794067382812\n",
            "iteration : 5791 loss: 2.596933603286743\n",
            "iteration : 5792 loss: 2.7804317474365234\n",
            "iteration : 5793 loss: 2.518192768096924\n",
            "iteration : 5794 loss: 2.503438711166382\n",
            "iteration : 5795 loss: 2.8072304725646973\n",
            "iteration : 5796 loss: 2.852766990661621\n",
            "iteration : 5797 loss: 2.5448007583618164\n",
            "iteration : 5798 loss: 2.5981650352478027\n",
            "iteration : 5799 loss: 2.2847208976745605\n",
            "iteration : 5800 loss: 2.505167007446289\n",
            "iteration : 5801 loss: 2.3902041912078857\n",
            "iteration : 5802 loss: 2.408935546875\n",
            "iteration : 5803 loss: 2.313944101333618\n",
            "iteration : 5804 loss: 2.93359637260437\n",
            "iteration : 5805 loss: 2.320094108581543\n",
            "iteration : 5806 loss: 2.2236201763153076\n",
            "iteration : 5807 loss: 2.4353225231170654\n",
            "iteration : 5808 loss: 2.5934317111968994\n",
            "iteration : 5809 loss: 3.8509409427642822\n",
            "iteration : 5810 loss: 2.9132041931152344\n",
            "iteration : 5811 loss: 3.0664145946502686\n",
            "iteration : 5812 loss: 2.905738592147827\n",
            "iteration : 5813 loss: 2.9605157375335693\n",
            "iteration : 5814 loss: 3.169919729232788\n",
            "iteration : 5815 loss: 2.9128916263580322\n",
            "iteration : 5816 loss: 3.010920524597168\n",
            "iteration : 5817 loss: 2.9856598377227783\n",
            "iteration : 5818 loss: 2.8373732566833496\n",
            "iteration : 5819 loss: 2.6529622077941895\n",
            "iteration : 5820 loss: 2.7309582233428955\n",
            "iteration : 5821 loss: 2.8277547359466553\n",
            "iteration : 5822 loss: 2.490995407104492\n",
            "iteration : 5823 loss: 2.9969217777252197\n",
            "iteration : 5824 loss: 2.9125990867614746\n",
            "iteration : 5825 loss: 2.8117432594299316\n",
            "iteration : 5826 loss: 3.1136744022369385\n",
            "iteration : 5827 loss: 2.8453218936920166\n",
            "iteration : 5828 loss: 2.6856749057769775\n",
            "iteration : 5829 loss: 2.9398274421691895\n",
            "iteration : 5830 loss: 2.7937490940093994\n",
            "iteration : 5831 loss: 3.27005934715271\n",
            "iteration : 5832 loss: 3.0510001182556152\n",
            "iteration : 5833 loss: 2.6280782222747803\n",
            "iteration : 5834 loss: 3.270592451095581\n",
            "iteration : 5835 loss: 2.8836941719055176\n",
            "iteration : 5836 loss: 2.633759021759033\n",
            "iteration : 5837 loss: 2.7542247772216797\n",
            "iteration : 5838 loss: 2.7389614582061768\n",
            "iteration : 5839 loss: 2.8321139812469482\n",
            "iteration : 5840 loss: 2.9400582313537598\n",
            "iteration : 5841 loss: 2.9284238815307617\n",
            "iteration : 5842 loss: 3.005044937133789\n",
            "iteration : 5843 loss: 2.6468796730041504\n",
            "iteration : 5844 loss: 2.826519012451172\n",
            "iteration : 5845 loss: 2.7437448501586914\n",
            "iteration : 5846 loss: 2.717331647872925\n",
            "iteration : 5847 loss: 2.7928736209869385\n",
            "iteration : 5848 loss: 2.742993116378784\n",
            "iteration : 5849 loss: 2.5267584323883057\n",
            "iteration : 5850 loss: 2.451328992843628\n",
            "iteration : 5851 loss: 2.488746404647827\n",
            "iteration : 5852 loss: 2.3817837238311768\n",
            "iteration : 5853 loss: 2.5769147872924805\n",
            "iteration : 5854 loss: 2.331432342529297\n",
            "iteration : 5855 loss: 2.568789482116699\n",
            "iteration : 5856 loss: 2.4203274250030518\n",
            "iteration : 5857 loss: 2.8636882305145264\n",
            "iteration : 5858 loss: 2.9384896755218506\n",
            "iteration : 5859 loss: 2.8714141845703125\n",
            "iteration : 5860 loss: 2.7545619010925293\n",
            "iteration : 5861 loss: 2.742682456970215\n",
            "iteration : 5862 loss: 2.777071237564087\n",
            "iteration : 5863 loss: 2.805690288543701\n",
            "iteration : 5864 loss: 2.897711753845215\n",
            "iteration : 5865 loss: 2.5593910217285156\n",
            "iteration : 5866 loss: 2.6794447898864746\n",
            "iteration : 5867 loss: 2.5682930946350098\n",
            "iteration : 5868 loss: 2.567310333251953\n",
            "iteration : 5869 loss: 2.356806755065918\n",
            "iteration : 5870 loss: 2.4371235370635986\n",
            "iteration : 5871 loss: 2.471721649169922\n",
            "iteration : 5872 loss: 2.7016255855560303\n",
            "iteration : 5873 loss: 2.709738254547119\n",
            "iteration : 5874 loss: 2.6355555057525635\n",
            "iteration : 5875 loss: 2.4609458446502686\n",
            "iteration : 5876 loss: 2.586897850036621\n",
            "iteration : 5877 loss: 2.748296022415161\n",
            "iteration : 5878 loss: 2.708726167678833\n",
            "iteration : 5879 loss: 2.6028213500976562\n",
            "iteration : 5880 loss: 2.5372307300567627\n",
            "iteration : 5881 loss: 2.322878837585449\n",
            "iteration : 5882 loss: 2.300365686416626\n",
            "iteration : 5883 loss: 2.39072847366333\n",
            "iteration : 5884 loss: 2.4733352661132812\n",
            "iteration : 5885 loss: 2.7904698848724365\n",
            "iteration : 5886 loss: 2.620391845703125\n",
            "iteration : 5887 loss: 2.3262131214141846\n",
            "iteration : 5888 loss: 3.050389528274536\n",
            "iteration : 5889 loss: 2.4203755855560303\n",
            "iteration : 5890 loss: 2.5587258338928223\n",
            "iteration : 5891 loss: 2.6866743564605713\n",
            "iteration : 5892 loss: 2.3144683837890625\n",
            "iteration : 5893 loss: 2.8513643741607666\n",
            "iteration : 5894 loss: 2.9166338443756104\n",
            "iteration : 5895 loss: 2.616410255432129\n",
            "iteration : 5896 loss: 2.203753709793091\n",
            "iteration : 5897 loss: 2.2939579486846924\n",
            "iteration : 5898 loss: 2.7406458854675293\n",
            "iteration : 5899 loss: 2.7733829021453857\n",
            "iteration : 5900 loss: 2.683837652206421\n",
            "iteration : 5901 loss: 2.7005674839019775\n",
            "iteration : 5902 loss: 2.8356454372406006\n",
            "iteration : 5903 loss: 2.749032974243164\n",
            "iteration : 5904 loss: 2.3932058811187744\n",
            "iteration : 5905 loss: 2.6427011489868164\n",
            "iteration : 5906 loss: 2.7147271633148193\n",
            "iteration : 5907 loss: 2.697544574737549\n",
            "iteration : 5908 loss: 2.392481565475464\n",
            "iteration : 5909 loss: 2.7438771724700928\n",
            "iteration : 5910 loss: 2.5250797271728516\n",
            "iteration : 5911 loss: 2.50282883644104\n",
            "iteration : 5912 loss: 2.560216188430786\n",
            "iteration : 5913 loss: 2.8889060020446777\n",
            "iteration : 5914 loss: 2.4998345375061035\n",
            "iteration : 5915 loss: 2.3722474575042725\n",
            "iteration : 5916 loss: 2.2683017253875732\n",
            "iteration : 5917 loss: 2.5712087154388428\n",
            "iteration : 5918 loss: 2.7413032054901123\n",
            "iteration : 5919 loss: 2.7404561042785645\n",
            "iteration : 5920 loss: 3.331051826477051\n",
            "iteration : 5921 loss: 3.249389171600342\n",
            "iteration : 5922 loss: 2.7149746417999268\n",
            "iteration : 5923 loss: 2.8209824562072754\n",
            "iteration : 5924 loss: 3.3792545795440674\n",
            "iteration : 5925 loss: 3.105581521987915\n",
            "iteration : 5926 loss: 2.9992616176605225\n",
            "iteration : 5927 loss: 3.1880195140838623\n",
            "iteration : 5928 loss: 2.919774293899536\n",
            "iteration : 5929 loss: 2.9670522212982178\n",
            "iteration : 5930 loss: 3.072157382965088\n",
            "iteration : 5931 loss: 2.7935307025909424\n",
            "iteration : 5932 loss: 3.3221595287323\n",
            "iteration : 5933 loss: 2.7178757190704346\n",
            "iteration : 5934 loss: 3.2306301593780518\n",
            "iteration : 5935 loss: 2.89109206199646\n",
            "iteration : 5936 loss: 2.7380197048187256\n",
            "iteration : 5937 loss: 2.8817126750946045\n",
            "iteration : 5938 loss: 2.7757294178009033\n",
            "iteration : 5939 loss: 2.7858684062957764\n",
            "iteration : 5940 loss: 2.759937047958374\n",
            "iteration : 5941 loss: 2.6800575256347656\n",
            "iteration : 5942 loss: 2.6246986389160156\n",
            "iteration : 5943 loss: 2.841125726699829\n",
            "iteration : 5944 loss: 3.070636034011841\n",
            "iteration : 5945 loss: 3.026219367980957\n",
            "iteration : 5946 loss: 3.306788206100464\n",
            "iteration : 5947 loss: 2.9948906898498535\n",
            "iteration : 5948 loss: 3.009150743484497\n",
            "iteration : 5949 loss: 2.961169719696045\n",
            "iteration : 5950 loss: 2.982248067855835\n",
            "iteration : 5951 loss: 2.8801770210266113\n",
            "iteration : 5952 loss: 3.0622806549072266\n",
            "iteration : 5953 loss: 2.6676316261291504\n",
            "iteration : 5954 loss: 2.705554723739624\n",
            "iteration : 5955 loss: 2.8954858779907227\n",
            "iteration : 5956 loss: 2.9975781440734863\n",
            "iteration : 5957 loss: 2.809898614883423\n",
            "iteration : 5958 loss: 2.8757617473602295\n",
            "iteration : 5959 loss: 2.8497579097747803\n",
            "iteration : 5960 loss: 2.7795121669769287\n",
            "iteration : 5961 loss: 2.900994300842285\n",
            "iteration : 5962 loss: 2.9396538734436035\n",
            "iteration : 5963 loss: 2.8482773303985596\n",
            "iteration : 5964 loss: 2.874866008758545\n",
            "iteration : 5965 loss: 2.970200300216675\n",
            "iteration : 5966 loss: 2.6111788749694824\n",
            "iteration : 5967 loss: 2.8582825660705566\n",
            "iteration : 5968 loss: 2.8446836471557617\n",
            "iteration : 5969 loss: 2.8479907512664795\n",
            "iteration : 5970 loss: 3.0122718811035156\n",
            "iteration : 5971 loss: 2.755155086517334\n",
            "iteration : 5972 loss: 2.8893630504608154\n",
            "iteration : 5973 loss: 2.952953338623047\n",
            "iteration : 5974 loss: 2.9151926040649414\n",
            "iteration : 5975 loss: 2.758809804916382\n",
            "iteration : 5976 loss: 2.769197702407837\n",
            "iteration : 5977 loss: 2.700005292892456\n",
            "iteration : 5978 loss: 2.626546621322632\n",
            "iteration : 5979 loss: 2.5311219692230225\n",
            "iteration : 5980 loss: 2.45694899559021\n",
            "iteration : 5981 loss: 2.367598295211792\n",
            "iteration : 5982 loss: 2.280478000640869\n",
            "iteration : 5983 loss: 2.2090420722961426\n",
            "iteration : 5984 loss: 2.1349222660064697\n",
            "iteration : 5985 loss: 2.072042226791382\n",
            "iteration : 5986 loss: 2.021993637084961\n",
            "iteration : 5987 loss: 1.9821796417236328\n",
            "iteration : 5988 loss: 1.9563053846359253\n",
            "iteration : 5989 loss: 3.5806467533111572\n",
            "iteration : 5990 loss: 3.742053508758545\n",
            "iteration : 5991 loss: 2.194169759750366\n",
            "iteration : 5992 loss: 1.9198893308639526\n",
            "iteration : 5993 loss: 3.2172062397003174\n",
            "iteration : 5994 loss: 3.813594102859497\n",
            "iteration : 5995 loss: 3.715505838394165\n",
            "iteration : 5996 loss: 3.539722204208374\n",
            "iteration : 5997 loss: 3.3350813388824463\n",
            "iteration : 5998 loss: 2.9864771366119385\n",
            "iteration : 5999 loss: 2.9196300506591797\n",
            "iteration : 6000 loss: 2.908224105834961\n",
            "iteration : 6001 loss: 2.88543963432312\n",
            "iteration : 6002 loss: 2.7752845287323\n",
            "iteration : 6003 loss: 2.380946397781372\n",
            "iteration : 6004 loss: 3.447636604309082\n",
            "iteration : 6005 loss: 3.8553199768066406\n",
            "iteration : 6006 loss: 3.9149410724639893\n",
            "iteration : 6007 loss: 3.1750876903533936\n",
            "iteration : 6008 loss: 2.5518507957458496\n",
            "iteration : 6009 loss: 2.3315470218658447\n",
            "iteration : 6010 loss: 2.19687819480896\n",
            "iteration : 6011 loss: 2.18302583694458\n",
            "iteration : 6012 loss: 2.3875222206115723\n",
            "iteration : 6013 loss: 2.824317693710327\n",
            "iteration : 6014 loss: 2.8050730228424072\n",
            "iteration : 6015 loss: 3.1855850219726562\n",
            "iteration : 6016 loss: 2.8885419368743896\n",
            "iteration : 6017 loss: 2.750185966491699\n",
            "iteration : 6018 loss: 2.6619763374328613\n",
            "iteration : 6019 loss: 3.0152714252471924\n",
            "iteration : 6020 loss: 3.160308599472046\n",
            "iteration : 6021 loss: 3.1796443462371826\n",
            "iteration : 6022 loss: 3.428583860397339\n",
            "iteration : 6023 loss: 2.821444272994995\n",
            "iteration : 6024 loss: 2.577423334121704\n",
            "iteration : 6025 loss: 2.548362970352173\n",
            "iteration : 6026 loss: 3.24993634223938\n",
            "iteration : 6027 loss: 3.244325637817383\n",
            "iteration : 6028 loss: 2.955256938934326\n",
            "iteration : 6029 loss: 2.824477434158325\n",
            "iteration : 6030 loss: 2.765350580215454\n",
            "iteration : 6031 loss: 2.8506243228912354\n",
            "iteration : 6032 loss: 2.6603128910064697\n",
            "iteration : 6033 loss: 2.712222099304199\n",
            "iteration : 6034 loss: 2.786254644393921\n",
            "iteration : 6035 loss: 2.646106481552124\n",
            "iteration : 6036 loss: 2.9160187244415283\n",
            "iteration : 6037 loss: 2.930882453918457\n",
            "iteration : 6038 loss: 2.7839653491973877\n",
            "iteration : 6039 loss: 2.6580417156219482\n",
            "iteration : 6040 loss: 2.9921042919158936\n",
            "iteration : 6041 loss: 2.6908249855041504\n",
            "iteration : 6042 loss: 2.951942205429077\n",
            "iteration : 6043 loss: 2.7187275886535645\n",
            "iteration : 6044 loss: 2.7637083530426025\n",
            "iteration : 6045 loss: 2.7530629634857178\n",
            "iteration : 6046 loss: 2.727459669113159\n",
            "iteration : 6047 loss: 2.6984615325927734\n",
            "iteration : 6048 loss: 3.039555072784424\n",
            "iteration : 6049 loss: 2.8411529064178467\n",
            "iteration : 6050 loss: 2.7146072387695312\n",
            "iteration : 6051 loss: 2.9302914142608643\n",
            "iteration : 6052 loss: 2.7312257289886475\n",
            "iteration : 6053 loss: 2.9607326984405518\n",
            "iteration : 6054 loss: 3.0463709831237793\n",
            "iteration : 6055 loss: 3.068594217300415\n",
            "iteration : 6056 loss: 2.703622341156006\n",
            "iteration : 6057 loss: 2.8973634243011475\n",
            "iteration : 6058 loss: 2.582345485687256\n",
            "iteration : 6059 loss: 3.0330777168273926\n",
            "iteration : 6060 loss: 2.7051198482513428\n",
            "iteration : 6061 loss: 2.7881405353546143\n",
            "iteration : 6062 loss: 2.907252073287964\n",
            "iteration : 6063 loss: 2.7319047451019287\n",
            "iteration : 6064 loss: 2.856487512588501\n",
            "iteration : 6065 loss: 2.832770347595215\n",
            "iteration : 6066 loss: 3.0873444080352783\n",
            "iteration : 6067 loss: 2.806152105331421\n",
            "iteration : 6068 loss: 3.0362045764923096\n",
            "iteration : 6069 loss: 2.78129243850708\n",
            "iteration : 6070 loss: 2.889918327331543\n",
            "iteration : 6071 loss: 2.84536075592041\n",
            "iteration : 6072 loss: 2.50681471824646\n",
            "iteration : 6073 loss: 2.936922073364258\n",
            "iteration : 6074 loss: 2.7726175785064697\n",
            "iteration : 6075 loss: 2.97908091545105\n",
            "iteration : 6076 loss: 2.650177478790283\n",
            "iteration : 6077 loss: 2.5930097103118896\n",
            "iteration : 6078 loss: 2.550455093383789\n",
            "iteration : 6079 loss: 2.8910467624664307\n",
            "iteration : 6080 loss: 2.4639575481414795\n",
            "iteration : 6081 loss: 2.7327535152435303\n",
            "iteration : 6082 loss: 2.7698326110839844\n",
            "iteration : 6083 loss: 2.6871566772460938\n",
            "iteration : 6084 loss: 2.500960350036621\n",
            "iteration : 6085 loss: 2.6702682971954346\n",
            "iteration : 6086 loss: 2.766366958618164\n",
            "iteration : 6087 loss: 2.893402576446533\n",
            "iteration : 6088 loss: 2.740464210510254\n",
            "iteration : 6089 loss: 2.7498891353607178\n",
            "iteration : 6090 loss: 2.823758125305176\n",
            "iteration : 6091 loss: 2.8871169090270996\n",
            "iteration : 6092 loss: 2.738140821456909\n",
            "iteration : 6093 loss: 2.7256124019622803\n",
            "iteration : 6094 loss: 2.7011072635650635\n",
            "iteration : 6095 loss: 2.625396966934204\n",
            "iteration : 6096 loss: 2.781919002532959\n",
            "iteration : 6097 loss: 2.657200574874878\n",
            "iteration : 6098 loss: 2.842543840408325\n",
            "iteration : 6099 loss: 2.5867860317230225\n",
            "iteration : 6100 loss: 2.860997438430786\n",
            "iteration : 6101 loss: 3.3007571697235107\n",
            "iteration : 6102 loss: 2.752183675765991\n",
            "iteration : 6103 loss: 2.7752139568328857\n",
            "iteration : 6104 loss: 2.9303646087646484\n",
            "iteration : 6105 loss: 2.8369946479797363\n",
            "iteration : 6106 loss: 2.8934125900268555\n",
            "iteration : 6107 loss: 2.7773399353027344\n",
            "iteration : 6108 loss: 2.792083740234375\n",
            "iteration : 6109 loss: 2.745877742767334\n",
            "iteration : 6110 loss: 2.8367197513580322\n",
            "iteration : 6111 loss: 2.8234012126922607\n",
            "iteration : 6112 loss: 2.586351156234741\n",
            "iteration : 6113 loss: 2.8599398136138916\n",
            "iteration : 6114 loss: 2.849003553390503\n",
            "iteration : 6115 loss: 2.8280751705169678\n",
            "iteration : 6116 loss: 2.512924909591675\n",
            "iteration : 6117 loss: 2.697148323059082\n",
            "iteration : 6118 loss: 2.6591222286224365\n",
            "iteration : 6119 loss: 2.4865572452545166\n",
            "iteration : 6120 loss: 2.5447282791137695\n",
            "iteration : 6121 loss: 2.548349380493164\n",
            "iteration : 6122 loss: 2.37510085105896\n",
            "iteration : 6123 loss: 2.341874361038208\n",
            "iteration : 6124 loss: 2.4409539699554443\n",
            "iteration : 6125 loss: 2.5284640789031982\n",
            "iteration : 6126 loss: 2.430400848388672\n",
            "iteration : 6127 loss: 2.283386468887329\n",
            "iteration : 6128 loss: 2.3885865211486816\n",
            "iteration : 6129 loss: 2.1261699199676514\n",
            "iteration : 6130 loss: 2.4924867153167725\n",
            "iteration : 6131 loss: 3.043208360671997\n",
            "iteration : 6132 loss: 2.484015941619873\n",
            "iteration : 6133 loss: 2.742260217666626\n",
            "iteration : 6134 loss: 2.662492036819458\n",
            "iteration : 6135 loss: 2.2346959114074707\n",
            "iteration : 6136 loss: 2.6044442653656006\n",
            "iteration : 6137 loss: 2.0549089908599854\n",
            "iteration : 6138 loss: 2.2736711502075195\n",
            "iteration : 6139 loss: 3.3822779655456543\n",
            "iteration : 6140 loss: 3.5826542377471924\n",
            "iteration : 6141 loss: 3.4328060150146484\n",
            "iteration : 6142 loss: 4.044585227966309\n",
            "iteration : 6143 loss: 3.2982840538024902\n",
            "iteration : 6144 loss: 3.6862399578094482\n",
            "iteration : 6145 loss: 3.2991275787353516\n",
            "iteration : 6146 loss: 3.1270625591278076\n",
            "iteration : 6147 loss: 3.1587765216827393\n",
            "iteration : 6148 loss: 3.1716954708099365\n",
            "iteration : 6149 loss: 3.1125905513763428\n",
            "iteration : 6150 loss: 2.781174421310425\n",
            "iteration : 6151 loss: 2.6058168411254883\n",
            "iteration : 6152 loss: 3.257650136947632\n",
            "iteration : 6153 loss: 3.634986162185669\n",
            "iteration : 6154 loss: 2.979785442352295\n",
            "iteration : 6155 loss: 2.978421688079834\n",
            "iteration : 6156 loss: 3.151826858520508\n",
            "iteration : 6157 loss: 2.7601804733276367\n",
            "iteration : 6158 loss: 2.904633045196533\n",
            "iteration : 6159 loss: 2.8625528812408447\n",
            "iteration : 6160 loss: 3.0130765438079834\n",
            "iteration : 6161 loss: 2.9201834201812744\n",
            "iteration : 6162 loss: 2.9518308639526367\n",
            "iteration : 6163 loss: 2.7739787101745605\n",
            "iteration : 6164 loss: 2.735759735107422\n",
            "iteration : 6165 loss: 2.7463626861572266\n",
            "iteration : 6166 loss: 2.716663360595703\n",
            "iteration : 6167 loss: 2.7798941135406494\n",
            "iteration : 6168 loss: 2.9333202838897705\n",
            "iteration : 6169 loss: 2.996899127960205\n",
            "iteration : 6170 loss: 2.7966597080230713\n",
            "iteration : 6171 loss: 2.6686861515045166\n",
            "iteration : 6172 loss: 2.7022061347961426\n",
            "iteration : 6173 loss: 2.962080717086792\n",
            "iteration : 6174 loss: 2.8345577716827393\n",
            "iteration : 6175 loss: 2.8299660682678223\n",
            "iteration : 6176 loss: 2.833533763885498\n",
            "iteration : 6177 loss: 2.7373456954956055\n",
            "iteration : 6178 loss: 2.7811429500579834\n",
            "iteration : 6179 loss: 2.862647533416748\n",
            "iteration : 6180 loss: 2.9630985260009766\n",
            "iteration : 6181 loss: 2.640845775604248\n",
            "iteration : 6182 loss: 2.9544217586517334\n",
            "iteration : 6183 loss: 2.9265151023864746\n",
            "iteration : 6184 loss: 2.879810333251953\n",
            "iteration : 6185 loss: 2.8192594051361084\n",
            "iteration : 6186 loss: 2.725477457046509\n",
            "iteration : 6187 loss: 2.6303248405456543\n",
            "iteration : 6188 loss: 2.5240232944488525\n",
            "iteration : 6189 loss: 2.4145665168762207\n",
            "iteration : 6190 loss: 2.3100647926330566\n",
            "iteration : 6191 loss: 2.3431620597839355\n",
            "iteration : 6192 loss: 3.070080041885376\n",
            "iteration : 6193 loss: 3.087076187133789\n",
            "iteration : 6194 loss: 3.092388868331909\n",
            "iteration : 6195 loss: 3.0290396213531494\n",
            "iteration : 6196 loss: 3.3750455379486084\n",
            "iteration : 6197 loss: 3.2093417644500732\n",
            "iteration : 6198 loss: 2.6282620429992676\n",
            "iteration : 6199 loss: 2.6612963676452637\n",
            "iteration : 6200 loss: 3.2780330181121826\n",
            "iteration : 6201 loss: 3.244050979614258\n",
            "iteration : 6202 loss: 2.6140565872192383\n",
            "iteration : 6203 loss: 2.670076370239258\n",
            "iteration : 6204 loss: 2.983081817626953\n",
            "iteration : 6205 loss: 3.2088005542755127\n",
            "iteration : 6206 loss: 2.3681299686431885\n",
            "iteration : 6207 loss: 2.8597450256347656\n",
            "iteration : 6208 loss: 2.845860719680786\n",
            "iteration : 6209 loss: 2.767490863800049\n",
            "iteration : 6210 loss: 2.764493942260742\n",
            "iteration : 6211 loss: 2.9590468406677246\n",
            "iteration : 6212 loss: 2.9272286891937256\n",
            "iteration : 6213 loss: 2.8757715225219727\n",
            "iteration : 6214 loss: 3.1850481033325195\n",
            "iteration : 6215 loss: 2.9884884357452393\n",
            "iteration : 6216 loss: 2.7299957275390625\n",
            "iteration : 6217 loss: 2.8760225772857666\n",
            "iteration : 6218 loss: 2.916173219680786\n",
            "iteration : 6219 loss: 2.978194236755371\n",
            "iteration : 6220 loss: 2.969937324523926\n",
            "iteration : 6221 loss: 2.8339741230010986\n",
            "iteration : 6222 loss: 2.815293788909912\n",
            "iteration : 6223 loss: 3.0949742794036865\n",
            "iteration : 6224 loss: 2.711639881134033\n",
            "iteration : 6225 loss: 2.656527280807495\n",
            "iteration : 6226 loss: 2.720431327819824\n",
            "iteration : 6227 loss: 2.731423854827881\n",
            "iteration : 6228 loss: 2.825608730316162\n",
            "iteration : 6229 loss: 2.811734676361084\n",
            "iteration : 6230 loss: 2.8091819286346436\n",
            "iteration : 6231 loss: 2.670050859451294\n",
            "iteration : 6232 loss: 2.835836887359619\n",
            "iteration : 6233 loss: 2.824244499206543\n",
            "iteration : 6234 loss: 2.721367835998535\n",
            "iteration : 6235 loss: 2.7555930614471436\n",
            "iteration : 6236 loss: 2.7350258827209473\n",
            "iteration : 6237 loss: 3.0004589557647705\n",
            "iteration : 6238 loss: 2.7567951679229736\n",
            "iteration : 6239 loss: 3.1232781410217285\n",
            "iteration : 6240 loss: 2.8590621948242188\n",
            "iteration : 6241 loss: 2.6868276596069336\n",
            "iteration : 6242 loss: 2.681830644607544\n",
            "iteration : 6243 loss: 2.647813081741333\n",
            "iteration : 6244 loss: 2.6043310165405273\n",
            "iteration : 6245 loss: 2.7969882488250732\n",
            "iteration : 6246 loss: 2.597200393676758\n",
            "iteration : 6247 loss: 2.5760445594787598\n",
            "iteration : 6248 loss: 2.7298152446746826\n",
            "iteration : 6249 loss: 2.8354685306549072\n",
            "iteration : 6250 loss: 2.701669454574585\n",
            "iteration : 6251 loss: 2.7495791912078857\n",
            "iteration : 6252 loss: 2.7318809032440186\n",
            "iteration : 6253 loss: 2.8603243827819824\n",
            "iteration : 6254 loss: 2.759462833404541\n",
            "iteration : 6255 loss: 2.461902141571045\n",
            "iteration : 6256 loss: 2.6319499015808105\n",
            "iteration : 6257 loss: 3.079366683959961\n",
            "iteration : 6258 loss: 2.6744821071624756\n",
            "iteration : 6259 loss: 2.74269962310791\n",
            "iteration : 6260 loss: 2.7520246505737305\n",
            "iteration : 6261 loss: 3.0924723148345947\n",
            "iteration : 6262 loss: 2.9327690601348877\n",
            "iteration : 6263 loss: 2.9386954307556152\n",
            "iteration : 6264 loss: 2.725959062576294\n",
            "iteration : 6265 loss: 2.6991429328918457\n",
            "iteration : 6266 loss: 3.024229049682617\n",
            "iteration : 6267 loss: 3.0541441440582275\n",
            "iteration : 6268 loss: 2.7239372730255127\n",
            "iteration : 6269 loss: 2.590394973754883\n",
            "iteration : 6270 loss: 2.537616729736328\n",
            "iteration : 6271 loss: 2.549304723739624\n",
            "iteration : 6272 loss: 2.683250904083252\n",
            "iteration : 6273 loss: 2.789121150970459\n",
            "iteration : 6274 loss: 2.87990665435791\n",
            "iteration : 6275 loss: 2.7300755977630615\n",
            "iteration : 6276 loss: 2.657393455505371\n",
            "iteration : 6277 loss: 2.588517427444458\n",
            "iteration : 6278 loss: 2.7355170249938965\n",
            "iteration : 6279 loss: 2.707242488861084\n",
            "iteration : 6280 loss: 2.5081050395965576\n",
            "iteration : 6281 loss: 2.6199440956115723\n",
            "iteration : 6282 loss: 2.8580429553985596\n",
            "iteration : 6283 loss: 2.531311511993408\n",
            "iteration : 6284 loss: 2.7312076091766357\n",
            "iteration : 6285 loss: 2.5961413383483887\n",
            "iteration : 6286 loss: 2.7672953605651855\n",
            "iteration : 6287 loss: 2.768343687057495\n",
            "iteration : 6288 loss: 2.7433600425720215\n",
            "iteration : 6289 loss: 2.9567787647247314\n",
            "iteration : 6290 loss: 2.7129805088043213\n",
            "iteration : 6291 loss: 2.640362501144409\n",
            "iteration : 6292 loss: 2.635845899581909\n",
            "iteration : 6293 loss: 2.6930489540100098\n",
            "iteration : 6294 loss: 2.2025346755981445\n",
            "iteration : 6295 loss: 2.334528684616089\n",
            "iteration : 6296 loss: 2.390097141265869\n",
            "iteration : 6297 loss: 2.2308692932128906\n",
            "iteration : 6298 loss: 2.3309037685394287\n",
            "iteration : 6299 loss: 2.5265214443206787\n",
            "iteration : 6300 loss: 2.521998643875122\n",
            "iteration : 6301 loss: 2.49371075630188\n",
            "iteration : 6302 loss: 2.4528255462646484\n",
            "iteration : 6303 loss: 2.3974599838256836\n",
            "iteration : 6304 loss: 2.348784923553467\n",
            "iteration : 6305 loss: 2.311269998550415\n",
            "iteration : 6306 loss: 2.3870818614959717\n",
            "iteration : 6307 loss: 3.152294397354126\n",
            "iteration : 6308 loss: 2.680370807647705\n",
            "iteration : 6309 loss: 2.9940080642700195\n",
            "iteration : 6310 loss: 3.0276243686676025\n",
            "iteration : 6311 loss: 2.7239296436309814\n",
            "iteration : 6312 loss: 2.2129392623901367\n",
            "iteration : 6313 loss: 2.381808042526245\n",
            "iteration : 6314 loss: 2.681123971939087\n",
            "iteration : 6315 loss: 2.248399019241333\n",
            "iteration : 6316 loss: 2.8742411136627197\n",
            "iteration : 6317 loss: 2.7914535999298096\n",
            "iteration : 6318 loss: 2.5882794857025146\n",
            "iteration : 6319 loss: 2.628406286239624\n",
            "iteration : 6320 loss: 2.882316827774048\n",
            "iteration : 6321 loss: 2.898212432861328\n",
            "iteration : 6322 loss: 2.325082778930664\n",
            "iteration : 6323 loss: 2.5493242740631104\n",
            "iteration : 6324 loss: 2.593627691268921\n",
            "iteration : 6325 loss: 3.0790863037109375\n",
            "iteration : 6326 loss: 2.56298565864563\n",
            "iteration : 6327 loss: 2.7960476875305176\n",
            "iteration : 6328 loss: 3.276919364929199\n",
            "iteration : 6329 loss: 2.4353816509246826\n",
            "iteration : 6330 loss: 2.6495132446289062\n",
            "iteration : 6331 loss: 2.640091896057129\n",
            "iteration : 6332 loss: 3.074770450592041\n",
            "iteration : 6333 loss: 2.8987860679626465\n",
            "iteration : 6334 loss: 2.753382921218872\n",
            "iteration : 6335 loss: 3.1805408000946045\n",
            "iteration : 6336 loss: 2.988502264022827\n",
            "iteration : 6337 loss: 2.9810903072357178\n",
            "iteration : 6338 loss: 3.42972731590271\n",
            "iteration : 6339 loss: 2.715730905532837\n",
            "iteration : 6340 loss: 2.8034918308258057\n",
            "iteration : 6341 loss: 3.0519232749938965\n",
            "iteration : 6342 loss: 2.7194340229034424\n",
            "iteration : 6343 loss: 2.909907341003418\n",
            "iteration : 6344 loss: 3.0588021278381348\n",
            "iteration : 6345 loss: 2.8410918712615967\n",
            "iteration : 6346 loss: 2.6484932899475098\n",
            "iteration : 6347 loss: 2.6350886821746826\n",
            "iteration : 6348 loss: 2.8662383556365967\n",
            "iteration : 6349 loss: 2.68152117729187\n",
            "iteration : 6350 loss: 2.851229190826416\n",
            "iteration : 6351 loss: 2.9367456436157227\n",
            "iteration : 6352 loss: 3.0674965381622314\n",
            "iteration : 6353 loss: 2.667273998260498\n",
            "iteration : 6354 loss: 2.8194289207458496\n",
            "iteration : 6355 loss: 3.066992998123169\n",
            "iteration : 6356 loss: 2.999189853668213\n",
            "iteration : 6357 loss: 2.7397756576538086\n",
            "iteration : 6358 loss: 2.7738025188446045\n",
            "iteration : 6359 loss: 2.9354407787323\n",
            "iteration : 6360 loss: 2.908424139022827\n",
            "iteration : 6361 loss: 3.0364696979522705\n",
            "iteration : 6362 loss: 2.780601739883423\n",
            "iteration : 6363 loss: 2.7630882263183594\n",
            "iteration : 6364 loss: 2.7644524574279785\n",
            "iteration : 6365 loss: 2.7814862728118896\n",
            "iteration : 6366 loss: 2.788747549057007\n",
            "iteration : 6367 loss: 2.9638671875\n",
            "iteration : 6368 loss: 2.614741086959839\n",
            "iteration : 6369 loss: 2.7286736965179443\n",
            "iteration : 6370 loss: 2.6687984466552734\n",
            "iteration : 6371 loss: 2.69687819480896\n",
            "iteration : 6372 loss: 2.8852527141571045\n",
            "iteration : 6373 loss: 2.6432743072509766\n",
            "iteration : 6374 loss: 3.044095993041992\n",
            "iteration : 6375 loss: 2.9819817543029785\n",
            "iteration : 6376 loss: 2.8283493518829346\n",
            "iteration : 6377 loss: 2.95145320892334\n",
            "iteration : 6378 loss: 2.727349042892456\n",
            "iteration : 6379 loss: 2.8101227283477783\n",
            "iteration : 6380 loss: 2.6483237743377686\n",
            "iteration : 6381 loss: 2.9151861667633057\n",
            "iteration : 6382 loss: 2.976813793182373\n",
            "iteration : 6383 loss: 2.928560495376587\n",
            "iteration : 6384 loss: 2.8783740997314453\n",
            "iteration : 6385 loss: 2.5832157135009766\n",
            "iteration : 6386 loss: 2.743988513946533\n",
            "iteration : 6387 loss: 2.838038444519043\n",
            "iteration : 6388 loss: 2.779827833175659\n",
            "iteration : 6389 loss: 3.0408577919006348\n",
            "iteration : 6390 loss: 2.846738576889038\n",
            "iteration : 6391 loss: 3.25093674659729\n",
            "iteration : 6392 loss: 2.695357322692871\n",
            "iteration : 6393 loss: 2.762120246887207\n",
            "iteration : 6394 loss: 2.860257625579834\n",
            "iteration : 6395 loss: 3.348505973815918\n",
            "iteration : 6396 loss: 2.801170587539673\n",
            "iteration : 6397 loss: 3.0411975383758545\n",
            "iteration : 6398 loss: 2.701106309890747\n",
            "iteration : 6399 loss: 2.9054532051086426\n",
            "iteration : 6400 loss: 2.878328323364258\n",
            "iteration : 6401 loss: 2.895296096801758\n",
            "iteration : 6402 loss: 2.8745052814483643\n",
            "iteration : 6403 loss: 2.825164794921875\n",
            "iteration : 6404 loss: 2.8081343173980713\n",
            "iteration : 6405 loss: 2.6852128505706787\n",
            "iteration : 6406 loss: 2.7043280601501465\n",
            "iteration : 6407 loss: 2.7815163135528564\n",
            "iteration : 6408 loss: 2.767547369003296\n",
            "iteration : 6409 loss: 2.810073137283325\n",
            "iteration : 6410 loss: 2.5459983348846436\n",
            "iteration : 6411 loss: 2.785167694091797\n",
            "iteration : 6412 loss: 2.7571027278900146\n",
            "iteration : 6413 loss: 2.7681915760040283\n",
            "iteration : 6414 loss: 2.684981346130371\n",
            "iteration : 6415 loss: 2.50506854057312\n",
            "iteration : 6416 loss: 2.7631900310516357\n",
            "iteration : 6417 loss: 2.7324953079223633\n",
            "iteration : 6418 loss: 2.7686524391174316\n",
            "iteration : 6419 loss: 3.0026185512542725\n",
            "iteration : 6420 loss: 2.8446872234344482\n",
            "iteration : 6421 loss: 2.827472448348999\n",
            "iteration : 6422 loss: 2.7469937801361084\n",
            "iteration : 6423 loss: 2.784573793411255\n",
            "iteration : 6424 loss: 2.7341549396514893\n",
            "iteration : 6425 loss: 2.655449390411377\n",
            "iteration : 6426 loss: 2.643366813659668\n",
            "iteration : 6427 loss: 2.785177230834961\n",
            "iteration : 6428 loss: 2.8132083415985107\n",
            "iteration : 6429 loss: 3.0498158931732178\n",
            "iteration : 6430 loss: 2.7930307388305664\n",
            "iteration : 6431 loss: 2.585629940032959\n",
            "iteration : 6432 loss: 2.755169630050659\n",
            "iteration : 6433 loss: 3.355769634246826\n",
            "iteration : 6434 loss: 2.6544055938720703\n",
            "iteration : 6435 loss: 2.692519187927246\n",
            "iteration : 6436 loss: 2.5275073051452637\n",
            "iteration : 6437 loss: 2.5765790939331055\n",
            "iteration : 6438 loss: 2.5246334075927734\n",
            "iteration : 6439 loss: 2.981691360473633\n",
            "iteration : 6440 loss: 2.5230984687805176\n",
            "iteration : 6441 loss: 2.7875683307647705\n",
            "iteration : 6442 loss: 2.6387484073638916\n",
            "iteration : 6443 loss: 3.1567201614379883\n",
            "iteration : 6444 loss: 2.77421236038208\n",
            "iteration : 6445 loss: 3.4568047523498535\n",
            "iteration : 6446 loss: 3.3466250896453857\n",
            "iteration : 6447 loss: 2.5894315242767334\n",
            "iteration : 6448 loss: 2.831850290298462\n",
            "iteration : 6449 loss: 2.800166368484497\n",
            "iteration : 6450 loss: 2.5556442737579346\n",
            "iteration : 6451 loss: 2.714334011077881\n",
            "iteration : 6452 loss: 2.7296087741851807\n",
            "iteration : 6453 loss: 2.63376522064209\n",
            "iteration : 6454 loss: 2.8575470447540283\n",
            "iteration : 6455 loss: 2.8738417625427246\n",
            "iteration : 6456 loss: 2.6050994396209717\n",
            "iteration : 6457 loss: 2.666156053543091\n",
            "iteration : 6458 loss: 2.8345980644226074\n",
            "iteration : 6459 loss: 2.6615476608276367\n",
            "iteration : 6460 loss: 2.728334665298462\n",
            "iteration : 6461 loss: 2.754828691482544\n",
            "iteration : 6462 loss: 3.054694652557373\n",
            "iteration : 6463 loss: 2.5557701587677\n",
            "iteration : 6464 loss: 2.5504677295684814\n",
            "iteration : 6465 loss: 2.730701446533203\n",
            "iteration : 6466 loss: 2.4903905391693115\n",
            "iteration : 6467 loss: 2.881673574447632\n",
            "iteration : 6468 loss: 2.8152244091033936\n",
            "iteration : 6469 loss: 2.791515827178955\n",
            "iteration : 6470 loss: 2.798741579055786\n",
            "iteration : 6471 loss: 2.877260446548462\n",
            "iteration : 6472 loss: 2.8148622512817383\n",
            "iteration : 6473 loss: 2.9776217937469482\n",
            "iteration : 6474 loss: 3.0081653594970703\n",
            "iteration : 6475 loss: 2.8741469383239746\n",
            "iteration : 6476 loss: 2.733125686645508\n",
            "iteration : 6477 loss: 2.5843725204467773\n",
            "iteration : 6478 loss: 2.6933281421661377\n",
            "iteration : 6479 loss: 2.7465243339538574\n",
            "iteration : 6480 loss: 2.8569517135620117\n",
            "iteration : 6481 loss: 3.0220305919647217\n",
            "iteration : 6482 loss: 2.849623680114746\n",
            "iteration : 6483 loss: 2.697917938232422\n",
            "iteration : 6484 loss: 2.580897331237793\n",
            "iteration : 6485 loss: 2.5281331539154053\n",
            "iteration : 6486 loss: 2.8665874004364014\n",
            "iteration : 6487 loss: 3.294408082962036\n",
            "iteration : 6488 loss: 3.130723237991333\n",
            "iteration : 6489 loss: 2.71881365776062\n",
            "iteration : 6490 loss: 2.7918405532836914\n",
            "iteration : 6491 loss: 2.5446150302886963\n",
            "iteration : 6492 loss: 3.035928964614868\n",
            "iteration : 6493 loss: 2.7635111808776855\n",
            "iteration : 6494 loss: 2.7421982288360596\n",
            "iteration : 6495 loss: 2.9916694164276123\n",
            "iteration : 6496 loss: 3.0866992473602295\n",
            "iteration : 6497 loss: 3.1108860969543457\n",
            "iteration : 6498 loss: 2.6678225994110107\n",
            "iteration : 6499 loss: 2.9091975688934326\n",
            "iteration : 6500 loss: 2.805140972137451\n",
            "iteration : 6501 loss: 2.812877655029297\n",
            "iteration : 6502 loss: 2.751674175262451\n",
            "iteration : 6503 loss: 2.7608466148376465\n",
            "iteration : 6504 loss: 2.895237922668457\n",
            "iteration : 6505 loss: 2.714686870574951\n",
            "iteration : 6506 loss: 2.762420654296875\n",
            "iteration : 6507 loss: 2.8915281295776367\n",
            "iteration : 6508 loss: 2.699253797531128\n",
            "iteration : 6509 loss: 3.066506862640381\n",
            "iteration : 6510 loss: 2.878150463104248\n",
            "iteration : 6511 loss: 2.703550100326538\n",
            "iteration : 6512 loss: 2.839176893234253\n",
            "iteration : 6513 loss: 2.7891428470611572\n",
            "iteration : 6514 loss: 2.9549129009246826\n",
            "iteration : 6515 loss: 2.704813241958618\n",
            "iteration : 6516 loss: 2.8187813758850098\n",
            "iteration : 6517 loss: 2.874598503112793\n",
            "iteration : 6518 loss: 2.728949546813965\n",
            "iteration : 6519 loss: 3.0940191745758057\n",
            "iteration : 6520 loss: 2.699223518371582\n",
            "iteration : 6521 loss: 2.7351443767547607\n",
            "iteration : 6522 loss: 2.7905287742614746\n",
            "iteration : 6523 loss: 2.756593704223633\n",
            "iteration : 6524 loss: 2.7327492237091064\n",
            "iteration : 6525 loss: 2.723968267440796\n",
            "iteration : 6526 loss: 2.7272300720214844\n",
            "iteration : 6527 loss: 2.805706739425659\n",
            "iteration : 6528 loss: 2.800975799560547\n",
            "iteration : 6529 loss: 2.802736520767212\n",
            "iteration : 6530 loss: 2.724820375442505\n",
            "iteration : 6531 loss: 2.7676639556884766\n",
            "iteration : 6532 loss: 2.708563804626465\n",
            "iteration : 6533 loss: 2.6567609310150146\n",
            "iteration : 6534 loss: 2.666170835494995\n",
            "iteration : 6535 loss: 2.6775095462799072\n",
            "iteration : 6536 loss: 2.641648054122925\n",
            "iteration : 6537 loss: 2.5668528079986572\n",
            "iteration : 6538 loss: 2.613413095474243\n",
            "iteration : 6539 loss: 2.7786848545074463\n",
            "iteration : 6540 loss: 2.5831973552703857\n",
            "iteration : 6541 loss: 2.567141056060791\n",
            "iteration : 6542 loss: 2.6297597885131836\n",
            "iteration : 6543 loss: 2.657106637954712\n",
            "iteration : 6544 loss: 2.601555109024048\n",
            "iteration : 6545 loss: 2.5693631172180176\n",
            "iteration : 6546 loss: 2.5414984226226807\n",
            "iteration : 6547 loss: 2.4526875019073486\n",
            "iteration : 6548 loss: 2.617795467376709\n",
            "iteration : 6549 loss: 2.8032054901123047\n",
            "iteration : 6550 loss: 2.8373687267303467\n",
            "iteration : 6551 loss: 2.8329391479492188\n",
            "iteration : 6552 loss: 2.572612762451172\n",
            "iteration : 6553 loss: 2.9074387550354004\n",
            "iteration : 6554 loss: 2.804356098175049\n",
            "iteration : 6555 loss: 3.066426992416382\n",
            "iteration : 6556 loss: 2.7803027629852295\n",
            "iteration : 6557 loss: 3.3552277088165283\n",
            "iteration : 6558 loss: 2.827803373336792\n",
            "iteration : 6559 loss: 2.6612489223480225\n",
            "iteration : 6560 loss: 2.655305862426758\n",
            "iteration : 6561 loss: 2.710972309112549\n",
            "iteration : 6562 loss: 3.30907940864563\n",
            "iteration : 6563 loss: 3.2494685649871826\n",
            "iteration : 6564 loss: 2.9095022678375244\n",
            "iteration : 6565 loss: 2.9939117431640625\n",
            "iteration : 6566 loss: 2.824723958969116\n",
            "iteration : 6567 loss: 2.586399555206299\n",
            "iteration : 6568 loss: 2.8776133060455322\n",
            "iteration : 6569 loss: 2.8112967014312744\n",
            "iteration : 6570 loss: 2.778130531311035\n",
            "iteration : 6571 loss: 2.842172384262085\n",
            "iteration : 6572 loss: 2.7989795207977295\n",
            "iteration : 6573 loss: 3.766387701034546\n",
            "iteration : 6574 loss: 3.9351255893707275\n",
            "iteration : 6575 loss: 3.8405020236968994\n",
            "iteration : 6576 loss: 3.7144744396209717\n",
            "iteration : 6577 loss: 3.5574681758880615\n",
            "iteration : 6578 loss: 3.398819923400879\n",
            "iteration : 6579 loss: 3.2382380962371826\n",
            "iteration : 6580 loss: 3.098461866378784\n",
            "iteration : 6581 loss: 2.945249557495117\n",
            "iteration : 6582 loss: 2.7977418899536133\n",
            "iteration : 6583 loss: 2.73779296875\n",
            "iteration : 6584 loss: 2.547022581100464\n",
            "iteration : 6585 loss: 2.4234440326690674\n",
            "iteration : 6586 loss: 2.298004388809204\n",
            "iteration : 6587 loss: 2.181105852127075\n",
            "iteration : 6588 loss: 2.0901224613189697\n",
            "iteration : 6589 loss: 2.128659248352051\n",
            "iteration : 6590 loss: 1.9142249822616577\n",
            "iteration : 6591 loss: 1.8542733192443848\n",
            "iteration : 6592 loss: 1.8151602745056152\n",
            "iteration : 6593 loss: 1.7839250564575195\n",
            "iteration : 6594 loss: 1.7697542905807495\n",
            "iteration : 6595 loss: 1.7515429258346558\n",
            "iteration : 6596 loss: 1.734131932258606\n",
            "iteration : 6597 loss: 1.7197548151016235\n",
            "iteration : 6598 loss: 1.6978073120117188\n",
            "iteration : 6599 loss: 4.059803009033203\n",
            "iteration : 6600 loss: 4.747603893280029\n",
            "iteration : 6601 loss: 4.669747352600098\n",
            "iteration : 6602 loss: 4.73709774017334\n",
            "iteration : 6603 loss: 4.4080023765563965\n",
            "iteration : 6604 loss: 4.693309783935547\n",
            "iteration : 6605 loss: 4.251325607299805\n",
            "iteration : 6606 loss: 4.150665760040283\n",
            "iteration : 6607 loss: 4.0147199630737305\n",
            "iteration : 6608 loss: 3.757873773574829\n",
            "iteration : 6609 loss: 3.3121752738952637\n",
            "iteration : 6610 loss: 3.3989620208740234\n",
            "iteration : 6611 loss: 3.0456488132476807\n",
            "iteration : 6612 loss: 3.3133935928344727\n",
            "iteration : 6613 loss: 3.2682769298553467\n",
            "iteration : 6614 loss: 3.0063061714172363\n",
            "iteration : 6615 loss: 3.1093273162841797\n",
            "iteration : 6616 loss: 3.2629597187042236\n",
            "iteration : 6617 loss: 3.145724058151245\n",
            "iteration : 6618 loss: 2.7711563110351562\n",
            "iteration : 6619 loss: 2.938642978668213\n",
            "iteration : 6620 loss: 2.8099417686462402\n",
            "iteration : 6621 loss: 3.145758628845215\n",
            "iteration : 6622 loss: 3.056793212890625\n",
            "iteration : 6623 loss: 2.9515769481658936\n",
            "iteration : 6624 loss: 2.933342933654785\n",
            "iteration : 6625 loss: 2.8458919525146484\n",
            "iteration : 6626 loss: 2.9258718490600586\n",
            "iteration : 6627 loss: 2.941513776779175\n",
            "iteration : 6628 loss: 2.9005656242370605\n",
            "iteration : 6629 loss: 2.8694217205047607\n",
            "iteration : 6630 loss: 2.615967273712158\n",
            "iteration : 6631 loss: 3.091778516769409\n",
            "iteration : 6632 loss: 2.8041961193084717\n",
            "iteration : 6633 loss: 2.6915009021759033\n",
            "iteration : 6634 loss: 2.6952552795410156\n",
            "iteration : 6635 loss: 2.911625862121582\n",
            "iteration : 6636 loss: 2.551053285598755\n",
            "iteration : 6637 loss: 2.920220136642456\n",
            "iteration : 6638 loss: 2.9797098636627197\n",
            "iteration : 6639 loss: 2.749539852142334\n",
            "iteration : 6640 loss: 2.5529661178588867\n",
            "iteration : 6641 loss: 2.972975730895996\n",
            "iteration : 6642 loss: 2.789355993270874\n",
            "iteration : 6643 loss: 2.715395450592041\n",
            "iteration : 6644 loss: 2.9810075759887695\n",
            "iteration : 6645 loss: 2.9435653686523438\n",
            "iteration : 6646 loss: 2.5982279777526855\n",
            "iteration : 6647 loss: 2.593616485595703\n",
            "iteration : 6648 loss: 2.544586658477783\n",
            "iteration : 6649 loss: 2.6500155925750732\n",
            "iteration : 6650 loss: 2.8533170223236084\n",
            "iteration : 6651 loss: 2.893095016479492\n",
            "iteration : 6652 loss: 2.771284818649292\n",
            "iteration : 6653 loss: 2.764193534851074\n",
            "iteration : 6654 loss: 2.7831687927246094\n",
            "iteration : 6655 loss: 2.7007572650909424\n",
            "iteration : 6656 loss: 2.744598865509033\n",
            "iteration : 6657 loss: 2.7307252883911133\n",
            "iteration : 6658 loss: 2.611764669418335\n",
            "iteration : 6659 loss: 2.503692388534546\n",
            "iteration : 6660 loss: 2.680807113647461\n",
            "iteration : 6661 loss: 2.7332637310028076\n",
            "iteration : 6662 loss: 2.714158773422241\n",
            "iteration : 6663 loss: 2.4889578819274902\n",
            "iteration : 6664 loss: 2.5041303634643555\n",
            "iteration : 6665 loss: 2.6553685665130615\n",
            "iteration : 6666 loss: 2.5622518062591553\n",
            "iteration : 6667 loss: 2.615346908569336\n",
            "iteration : 6668 loss: 2.487517833709717\n",
            "iteration : 6669 loss: 2.6083834171295166\n",
            "iteration : 6670 loss: 2.65724778175354\n",
            "iteration : 6671 loss: 2.757805347442627\n",
            "iteration : 6672 loss: 2.8434183597564697\n",
            "iteration : 6673 loss: 2.646998882293701\n",
            "iteration : 6674 loss: 2.5804905891418457\n",
            "iteration : 6675 loss: 2.672196388244629\n",
            "iteration : 6676 loss: 2.688631296157837\n",
            "iteration : 6677 loss: 2.494433879852295\n",
            "iteration : 6678 loss: 2.6468708515167236\n",
            "iteration : 6679 loss: 2.6205742359161377\n",
            "iteration : 6680 loss: 2.7492899894714355\n",
            "iteration : 6681 loss: 2.9562764167785645\n",
            "iteration : 6682 loss: 2.917219638824463\n",
            "iteration : 6683 loss: 2.8606951236724854\n",
            "iteration : 6684 loss: 2.613212823867798\n",
            "iteration : 6685 loss: 2.5855560302734375\n",
            "iteration : 6686 loss: 2.707540512084961\n",
            "iteration : 6687 loss: 2.7837586402893066\n",
            "iteration : 6688 loss: 2.6908464431762695\n",
            "iteration : 6689 loss: 2.5781304836273193\n",
            "iteration : 6690 loss: 2.5362517833709717\n",
            "iteration : 6691 loss: 2.5402872562408447\n",
            "iteration : 6692 loss: 2.445737838745117\n",
            "iteration : 6693 loss: 2.658238649368286\n",
            "iteration : 6694 loss: 2.7646188735961914\n",
            "iteration : 6695 loss: 2.5441994667053223\n",
            "iteration : 6696 loss: 2.6113500595092773\n",
            "iteration : 6697 loss: 2.701165199279785\n",
            "iteration : 6698 loss: 2.7207753658294678\n",
            "iteration : 6699 loss: 2.6904704570770264\n",
            "iteration : 6700 loss: 2.5890309810638428\n",
            "iteration : 6701 loss: 2.6853668689727783\n",
            "iteration : 6702 loss: 3.025712728500366\n",
            "iteration : 6703 loss: 2.6431217193603516\n",
            "iteration : 6704 loss: 2.5170910358428955\n",
            "iteration : 6705 loss: 2.5052647590637207\n",
            "iteration : 6706 loss: 2.3843142986297607\n",
            "iteration : 6707 loss: 2.6987791061401367\n",
            "iteration : 6708 loss: 2.5708742141723633\n",
            "iteration : 6709 loss: 2.429741144180298\n",
            "iteration : 6710 loss: 2.485799551010132\n",
            "iteration : 6711 loss: 2.4804296493530273\n",
            "iteration : 6712 loss: 2.9328770637512207\n",
            "iteration : 6713 loss: 2.358949661254883\n",
            "iteration : 6714 loss: 2.481640577316284\n",
            "iteration : 6715 loss: 2.393667697906494\n",
            "iteration : 6716 loss: 3.0307984352111816\n",
            "iteration : 6717 loss: 2.6464357376098633\n",
            "iteration : 6718 loss: 2.6390328407287598\n",
            "iteration : 6719 loss: 2.4910244941711426\n",
            "iteration : 6720 loss: 2.448831081390381\n",
            "iteration : 6721 loss: 2.527646064758301\n",
            "iteration : 6722 loss: 2.5315675735473633\n",
            "iteration : 6723 loss: 2.7504687309265137\n",
            "iteration : 6724 loss: 2.913301706314087\n",
            "iteration : 6725 loss: 2.7914116382598877\n",
            "iteration : 6726 loss: 2.6388165950775146\n",
            "iteration : 6727 loss: 2.381061315536499\n",
            "iteration : 6728 loss: 2.4173171520233154\n",
            "iteration : 6729 loss: 2.660580635070801\n",
            "iteration : 6730 loss: 2.6030356884002686\n",
            "iteration : 6731 loss: 2.577270269393921\n",
            "iteration : 6732 loss: 2.3427350521087646\n",
            "iteration : 6733 loss: 2.6449596881866455\n",
            "iteration : 6734 loss: 2.899543285369873\n",
            "iteration : 6735 loss: 2.4666764736175537\n",
            "iteration : 6736 loss: 2.6144721508026123\n",
            "iteration : 6737 loss: 2.692906379699707\n",
            "iteration : 6738 loss: 2.595494031906128\n",
            "iteration : 6739 loss: 2.5701541900634766\n",
            "iteration : 6740 loss: 3.184328317642212\n",
            "iteration : 6741 loss: 2.4495797157287598\n",
            "iteration : 6742 loss: 2.7401864528656006\n",
            "iteration : 6743 loss: 2.5957789421081543\n",
            "iteration : 6744 loss: 2.3892908096313477\n",
            "iteration : 6745 loss: 2.744842290878296\n",
            "iteration : 6746 loss: 2.7198562622070312\n",
            "iteration : 6747 loss: 2.694182872772217\n",
            "iteration : 6748 loss: 3.4235947132110596\n",
            "iteration : 6749 loss: 3.5931432247161865\n",
            "iteration : 6750 loss: 2.6913440227508545\n",
            "iteration : 6751 loss: 2.3091628551483154\n",
            "iteration : 6752 loss: 2.3481080532073975\n",
            "iteration : 6753 loss: 2.2837443351745605\n",
            "iteration : 6754 loss: 2.639692783355713\n",
            "iteration : 6755 loss: 2.5014488697052\n",
            "iteration : 6756 loss: 2.708660125732422\n",
            "iteration : 6757 loss: 2.930985689163208\n",
            "iteration : 6758 loss: 2.7128729820251465\n",
            "iteration : 6759 loss: 2.618767499923706\n",
            "iteration : 6760 loss: 2.7249631881713867\n",
            "iteration : 6761 loss: 2.744954824447632\n",
            "iteration : 6762 loss: 2.4502451419830322\n",
            "iteration : 6763 loss: 3.535735845565796\n",
            "iteration : 6764 loss: 2.7810218334198\n",
            "iteration : 6765 loss: 2.4576849937438965\n",
            "iteration : 6766 loss: 2.8691532611846924\n",
            "iteration : 6767 loss: 3.065319299697876\n",
            "iteration : 6768 loss: 2.5869545936584473\n",
            "iteration : 6769 loss: 2.713618278503418\n",
            "iteration : 6770 loss: 2.7715582847595215\n",
            "iteration : 6771 loss: 2.73940110206604\n",
            "iteration : 6772 loss: 2.9062092304229736\n",
            "iteration : 6773 loss: 2.867324113845825\n",
            "iteration : 6774 loss: 3.0179553031921387\n",
            "iteration : 6775 loss: 2.975270986557007\n",
            "iteration : 6776 loss: 3.122849941253662\n",
            "iteration : 6777 loss: 2.7941370010375977\n",
            "iteration : 6778 loss: 2.648059129714966\n",
            "iteration : 6779 loss: 2.474792957305908\n",
            "iteration : 6780 loss: 3.101470470428467\n",
            "iteration : 6781 loss: 2.7219743728637695\n",
            "iteration : 6782 loss: 2.9738471508026123\n",
            "iteration : 6783 loss: 2.8709604740142822\n",
            "iteration : 6784 loss: 3.3755714893341064\n",
            "iteration : 6785 loss: 3.2091972827911377\n",
            "iteration : 6786 loss: 2.7421276569366455\n",
            "iteration : 6787 loss: 2.852956771850586\n",
            "iteration : 6788 loss: 2.7134294509887695\n",
            "iteration : 6789 loss: 2.630767583847046\n",
            "iteration : 6790 loss: 3.277022123336792\n",
            "iteration : 6791 loss: 2.829732656478882\n",
            "iteration : 6792 loss: 2.7275071144104004\n",
            "iteration : 6793 loss: 2.737278461456299\n",
            "iteration : 6794 loss: 2.7132256031036377\n",
            "iteration : 6795 loss: 2.6491880416870117\n",
            "iteration : 6796 loss: 2.748762607574463\n",
            "iteration : 6797 loss: 2.7125539779663086\n",
            "iteration : 6798 loss: 2.874803066253662\n",
            "iteration : 6799 loss: 2.746405601501465\n",
            "iteration : 6800 loss: 2.813596248626709\n",
            "iteration : 6801 loss: 2.7150251865386963\n",
            "iteration : 6802 loss: 2.6970739364624023\n",
            "iteration : 6803 loss: 2.6993398666381836\n",
            "iteration : 6804 loss: 2.5218095779418945\n",
            "iteration : 6805 loss: 2.579157590866089\n",
            "iteration : 6806 loss: 2.6877713203430176\n",
            "iteration : 6807 loss: 2.7377829551696777\n",
            "iteration : 6808 loss: 2.5644514560699463\n",
            "iteration : 6809 loss: 2.8681297302246094\n",
            "iteration : 6810 loss: 2.894660472869873\n",
            "iteration : 6811 loss: 2.81853985786438\n",
            "iteration : 6812 loss: 2.8319473266601562\n",
            "iteration : 6813 loss: 2.7741754055023193\n",
            "iteration : 6814 loss: 2.7209219932556152\n",
            "iteration : 6815 loss: 2.748731851577759\n",
            "iteration : 6816 loss: 2.86576771736145\n",
            "iteration : 6817 loss: 2.604044198989868\n",
            "iteration : 6818 loss: 2.6067891120910645\n",
            "iteration : 6819 loss: 2.822723388671875\n",
            "iteration : 6820 loss: 2.861788749694824\n",
            "iteration : 6821 loss: 2.7537097930908203\n",
            "iteration : 6822 loss: 2.670128345489502\n",
            "iteration : 6823 loss: 2.7103755474090576\n",
            "iteration : 6824 loss: 2.7447516918182373\n",
            "iteration : 6825 loss: 2.716571807861328\n",
            "iteration : 6826 loss: 2.608778238296509\n",
            "iteration : 6827 loss: 2.7689671516418457\n",
            "iteration : 6828 loss: 2.7024483680725098\n",
            "iteration : 6829 loss: 2.6585235595703125\n",
            "iteration : 6830 loss: 2.680060625076294\n",
            "iteration : 6831 loss: 2.815279960632324\n",
            "iteration : 6832 loss: 2.7112677097320557\n",
            "iteration : 6833 loss: 2.743366003036499\n",
            "iteration : 6834 loss: 2.8922040462493896\n",
            "iteration : 6835 loss: 2.851578950881958\n",
            "iteration : 6836 loss: 2.7648489475250244\n",
            "iteration : 6837 loss: 2.865333080291748\n",
            "iteration : 6838 loss: 2.622570037841797\n",
            "iteration : 6839 loss: 2.6247620582580566\n",
            "iteration : 6840 loss: 2.685645818710327\n",
            "iteration : 6841 loss: 3.035750150680542\n",
            "iteration : 6842 loss: 3.529451608657837\n",
            "iteration : 6843 loss: 2.7844033241271973\n",
            "iteration : 6844 loss: 2.6040613651275635\n",
            "iteration : 6845 loss: 2.8032333850860596\n",
            "iteration : 6846 loss: 2.715413808822632\n",
            "iteration : 6847 loss: 2.6866185665130615\n",
            "iteration : 6848 loss: 2.7560436725616455\n",
            "iteration : 6849 loss: 2.8537750244140625\n",
            "iteration : 6850 loss: 2.651886224746704\n",
            "iteration : 6851 loss: 2.8320815563201904\n",
            "iteration : 6852 loss: 2.6630749702453613\n",
            "iteration : 6853 loss: 2.800445556640625\n",
            "iteration : 6854 loss: 2.9350998401641846\n",
            "iteration : 6855 loss: 2.927031993865967\n",
            "iteration : 6856 loss: 2.6608896255493164\n",
            "iteration : 6857 loss: 2.9897706508636475\n",
            "iteration : 6858 loss: 3.091470718383789\n",
            "iteration : 6859 loss: 2.718919277191162\n",
            "iteration : 6860 loss: 2.882148265838623\n",
            "iteration : 6861 loss: 2.8734912872314453\n",
            "iteration : 6862 loss: 2.8205912113189697\n",
            "iteration : 6863 loss: 2.763073444366455\n",
            "iteration : 6864 loss: 2.678499698638916\n",
            "iteration : 6865 loss: 2.654758930206299\n",
            "iteration : 6866 loss: 2.6622421741485596\n",
            "iteration : 6867 loss: 2.8647639751434326\n",
            "iteration : 6868 loss: 2.902022123336792\n",
            "iteration : 6869 loss: 3.3883392810821533\n",
            "iteration : 6870 loss: 3.1505722999572754\n",
            "iteration : 6871 loss: 2.6403048038482666\n",
            "iteration : 6872 loss: 2.63163161277771\n",
            "iteration : 6873 loss: 2.604835271835327\n",
            "iteration : 6874 loss: 2.756298065185547\n",
            "iteration : 6875 loss: 3.4836783409118652\n",
            "iteration : 6876 loss: 2.879549503326416\n",
            "iteration : 6877 loss: 2.5153534412384033\n",
            "iteration : 6878 loss: 2.950981616973877\n",
            "iteration : 6879 loss: 2.813321352005005\n",
            "iteration : 6880 loss: 2.7724897861480713\n",
            "iteration : 6881 loss: 2.7892706394195557\n",
            "iteration : 6882 loss: 2.766413927078247\n",
            "iteration : 6883 loss: 2.657541036605835\n",
            "iteration : 6884 loss: 2.971796751022339\n",
            "iteration : 6885 loss: 2.625253438949585\n",
            "iteration : 6886 loss: 2.543506622314453\n",
            "iteration : 6887 loss: 3.0662477016448975\n",
            "iteration : 6888 loss: 2.658306837081909\n",
            "iteration : 6889 loss: 2.6444461345672607\n",
            "iteration : 6890 loss: 2.4194352626800537\n",
            "iteration : 6891 loss: 3.1504132747650146\n",
            "iteration : 6892 loss: 2.947565793991089\n",
            "iteration : 6893 loss: 3.571411609649658\n",
            "iteration : 6894 loss: 2.9484918117523193\n",
            "iteration : 6895 loss: 2.9214413166046143\n",
            "iteration : 6896 loss: 2.463745594024658\n",
            "iteration : 6897 loss: 2.7536964416503906\n",
            "iteration : 6898 loss: 2.5816650390625\n",
            "iteration : 6899 loss: 2.34187388420105\n",
            "iteration : 6900 loss: 2.3329975605010986\n",
            "iteration : 6901 loss: 2.953037738800049\n",
            "iteration : 6902 loss: 3.387604236602783\n",
            "iteration : 6903 loss: 2.6896467208862305\n",
            "iteration : 6904 loss: 2.7631781101226807\n",
            "iteration : 6905 loss: 2.602912187576294\n",
            "iteration : 6906 loss: 2.8121449947357178\n",
            "iteration : 6907 loss: 3.26755690574646\n",
            "iteration : 6908 loss: 3.129615068435669\n",
            "iteration : 6909 loss: 3.1600825786590576\n",
            "iteration : 6910 loss: 2.9311132431030273\n",
            "iteration : 6911 loss: 2.825223922729492\n",
            "iteration : 6912 loss: 2.8715639114379883\n",
            "iteration : 6913 loss: 2.7727432250976562\n",
            "iteration : 6914 loss: 2.6722569465637207\n",
            "iteration : 6915 loss: 2.713895082473755\n",
            "iteration : 6916 loss: 2.6639373302459717\n",
            "iteration : 6917 loss: 2.845294713973999\n",
            "iteration : 6918 loss: 2.578688383102417\n",
            "iteration : 6919 loss: 2.657639265060425\n",
            "iteration : 6920 loss: 2.5491487979888916\n",
            "iteration : 6921 loss: 2.548292398452759\n",
            "iteration : 6922 loss: 2.5291450023651123\n",
            "iteration : 6923 loss: 2.837756872177124\n",
            "iteration : 6924 loss: 2.886823892593384\n",
            "iteration : 6925 loss: 2.7824325561523438\n",
            "iteration : 6926 loss: 2.5577471256256104\n",
            "iteration : 6927 loss: 2.5435612201690674\n",
            "iteration : 6928 loss: 2.665544271469116\n",
            "iteration : 6929 loss: 2.5095927715301514\n",
            "iteration : 6930 loss: 2.5777652263641357\n",
            "iteration : 6931 loss: 2.7241246700286865\n",
            "iteration : 6932 loss: 2.4931747913360596\n",
            "iteration : 6933 loss: 2.4581034183502197\n",
            "iteration : 6934 loss: 2.618799924850464\n",
            "iteration : 6935 loss: 2.5574309825897217\n",
            "iteration : 6936 loss: 3.0070574283599854\n",
            "iteration : 6937 loss: 2.9126713275909424\n",
            "iteration : 6938 loss: 2.61865234375\n",
            "iteration : 6939 loss: 3.219238042831421\n",
            "iteration : 6940 loss: 2.8339734077453613\n",
            "iteration : 6941 loss: 2.7070887088775635\n",
            "iteration : 6942 loss: 3.0225019454956055\n",
            "iteration : 6943 loss: 2.612844467163086\n",
            "iteration : 6944 loss: 2.5853958129882812\n",
            "iteration : 6945 loss: 2.636260747909546\n",
            "iteration : 6946 loss: 3.358069896697998\n",
            "iteration : 6947 loss: 2.52602481842041\n",
            "iteration : 6948 loss: 2.834657907485962\n",
            "iteration : 6949 loss: 3.0658934116363525\n",
            "iteration : 6950 loss: 2.7113237380981445\n",
            "iteration : 6951 loss: 2.48874568939209\n",
            "iteration : 6952 loss: 2.712050199508667\n",
            "iteration : 6953 loss: 2.5205929279327393\n",
            "iteration : 6954 loss: 2.9409220218658447\n",
            "iteration : 6955 loss: 2.9757027626037598\n",
            "iteration : 6956 loss: 2.9905662536621094\n",
            "iteration : 6957 loss: 2.76251482963562\n",
            "iteration : 6958 loss: 2.575251340866089\n",
            "iteration : 6959 loss: 2.8090734481811523\n",
            "iteration : 6960 loss: 2.7538657188415527\n",
            "iteration : 6961 loss: 2.6990389823913574\n",
            "iteration : 6962 loss: 2.936734676361084\n",
            "iteration : 6963 loss: 2.553865432739258\n",
            "iteration : 6964 loss: 2.671618700027466\n",
            "iteration : 6965 loss: 2.47268009185791\n",
            "iteration : 6966 loss: 2.418844699859619\n",
            "iteration : 6967 loss: 2.4138033390045166\n",
            "iteration : 6968 loss: 2.695108413696289\n",
            "iteration : 6969 loss: 2.546294927597046\n",
            "iteration : 6970 loss: 2.417466402053833\n",
            "iteration : 6971 loss: 2.55602765083313\n",
            "iteration : 6972 loss: 2.6377034187316895\n",
            "iteration : 6973 loss: 2.4995274543762207\n",
            "iteration : 6974 loss: 2.587618827819824\n",
            "iteration : 6975 loss: 2.764174699783325\n",
            "iteration : 6976 loss: 3.000922441482544\n",
            "iteration : 6977 loss: 2.866953134536743\n",
            "iteration : 6978 loss: 2.481140375137329\n",
            "iteration : 6979 loss: 2.339405059814453\n",
            "iteration : 6980 loss: 2.6968116760253906\n",
            "iteration : 6981 loss: 2.6670703887939453\n",
            "iteration : 6982 loss: 2.6727445125579834\n",
            "iteration : 6983 loss: 2.7249557971954346\n",
            "iteration : 6984 loss: 2.715913772583008\n",
            "iteration : 6985 loss: 2.6540284156799316\n",
            "iteration : 6986 loss: 2.358988046646118\n",
            "iteration : 6987 loss: 2.3888649940490723\n",
            "iteration : 6988 loss: 2.6154823303222656\n",
            "iteration : 6989 loss: 2.5950770378112793\n",
            "iteration : 6990 loss: 2.5656065940856934\n",
            "iteration : 6991 loss: 2.6907355785369873\n",
            "iteration : 6992 loss: 2.678565740585327\n",
            "iteration : 6993 loss: 2.797422170639038\n",
            "iteration : 6994 loss: 2.6430671215057373\n",
            "iteration : 6995 loss: 2.450714349746704\n",
            "iteration : 6996 loss: 2.7463152408599854\n",
            "iteration : 6997 loss: 2.9217641353607178\n",
            "iteration : 6998 loss: 2.790973663330078\n",
            "iteration : 6999 loss: 2.4215590953826904\n",
            "iteration : 7000 loss: 2.776289224624634\n",
            "iteration : 7001 loss: 2.990938186645508\n",
            "iteration : 7002 loss: 2.83986496925354\n",
            "iteration : 7003 loss: 2.9566922187805176\n",
            "iteration : 7004 loss: 2.6687722206115723\n",
            "iteration : 7005 loss: 2.8665931224823\n",
            "iteration : 7006 loss: 2.6562588214874268\n",
            "iteration : 7007 loss: 2.8316805362701416\n",
            "iteration : 7008 loss: 2.7789697647094727\n",
            "iteration : 7009 loss: 2.820282459259033\n",
            "iteration : 7010 loss: 2.6364381313323975\n",
            "iteration : 7011 loss: 2.6678812503814697\n",
            "iteration : 7012 loss: 2.6694648265838623\n",
            "iteration : 7013 loss: 2.8364105224609375\n",
            "iteration : 7014 loss: 2.693403959274292\n",
            "iteration : 7015 loss: 2.6702117919921875\n",
            "iteration : 7016 loss: 3.0208945274353027\n",
            "iteration : 7017 loss: 2.683821201324463\n",
            "iteration : 7018 loss: 2.7029035091400146\n",
            "iteration : 7019 loss: 2.5850861072540283\n",
            "iteration : 7020 loss: 2.5383520126342773\n",
            "iteration : 7021 loss: 2.5016863346099854\n",
            "iteration : 7022 loss: 2.72541880607605\n",
            "iteration : 7023 loss: 2.5728278160095215\n",
            "iteration : 7024 loss: 2.7125823497772217\n",
            "iteration : 7025 loss: 2.6705808639526367\n",
            "iteration : 7026 loss: 2.64713191986084\n",
            "iteration : 7027 loss: 2.7390100955963135\n",
            "iteration : 7028 loss: 2.6149353981018066\n",
            "iteration : 7029 loss: 2.5803635120391846\n",
            "iteration : 7030 loss: 2.6637187004089355\n",
            "iteration : 7031 loss: 3.0364911556243896\n",
            "iteration : 7032 loss: 2.6703364849090576\n",
            "iteration : 7033 loss: 2.4604313373565674\n",
            "iteration : 7034 loss: 2.593951940536499\n",
            "iteration : 7035 loss: 2.712116003036499\n",
            "iteration : 7036 loss: 3.0424625873565674\n",
            "iteration : 7037 loss: 2.696702241897583\n",
            "iteration : 7038 loss: 2.741370677947998\n",
            "iteration : 7039 loss: 2.8386666774749756\n",
            "iteration : 7040 loss: 2.633209228515625\n",
            "iteration : 7041 loss: 2.6500866413116455\n",
            "iteration : 7042 loss: 2.5213706493377686\n",
            "iteration : 7043 loss: 2.5916550159454346\n",
            "iteration : 7044 loss: 2.5237765312194824\n",
            "iteration : 7045 loss: 2.5471794605255127\n",
            "iteration : 7046 loss: 2.4477035999298096\n",
            "iteration : 7047 loss: 2.5510637760162354\n",
            "iteration : 7048 loss: 2.6439170837402344\n",
            "iteration : 7049 loss: 2.9428257942199707\n",
            "iteration : 7050 loss: 2.9826042652130127\n",
            "iteration : 7051 loss: 2.5516645908355713\n",
            "iteration : 7052 loss: 2.623904228210449\n",
            "iteration : 7053 loss: 2.6727240085601807\n",
            "iteration : 7054 loss: 2.611093282699585\n",
            "iteration : 7055 loss: 2.861565113067627\n",
            "iteration : 7056 loss: 2.401388645172119\n",
            "iteration : 7057 loss: 2.3261210918426514\n",
            "iteration : 7058 loss: 2.3270246982574463\n",
            "iteration : 7059 loss: 2.3119704723358154\n",
            "iteration : 7060 loss: 2.7842113971710205\n",
            "iteration : 7061 loss: 2.7686352729797363\n",
            "iteration : 7062 loss: 2.838887929916382\n",
            "iteration : 7063 loss: 2.486953020095825\n",
            "iteration : 7064 loss: 2.3114802837371826\n",
            "iteration : 7065 loss: 3.0330376625061035\n",
            "iteration : 7066 loss: 2.694694757461548\n",
            "iteration : 7067 loss: 2.85982346534729\n",
            "iteration : 7068 loss: 2.5063209533691406\n",
            "iteration : 7069 loss: 2.5286877155303955\n",
            "iteration : 7070 loss: 2.3309974670410156\n",
            "iteration : 7071 loss: 2.8942627906799316\n",
            "iteration : 7072 loss: 2.699751853942871\n",
            "iteration : 7073 loss: 2.638228178024292\n",
            "iteration : 7074 loss: 2.517514944076538\n",
            "iteration : 7075 loss: 2.4647631645202637\n",
            "iteration : 7076 loss: 2.6350016593933105\n",
            "iteration : 7077 loss: 2.7008614540100098\n",
            "iteration : 7078 loss: 2.517075538635254\n",
            "iteration : 7079 loss: 2.237827777862549\n",
            "iteration : 7080 loss: 2.7482712268829346\n",
            "iteration : 7081 loss: 2.3562591075897217\n",
            "iteration : 7082 loss: 2.7774574756622314\n",
            "iteration : 7083 loss: 2.5310206413269043\n",
            "iteration : 7084 loss: 2.3872225284576416\n",
            "iteration : 7085 loss: 2.6207737922668457\n",
            "iteration : 7086 loss: 2.611764907836914\n",
            "iteration : 7087 loss: 3.010814905166626\n",
            "iteration : 7088 loss: 2.788445472717285\n",
            "iteration : 7089 loss: 2.808950185775757\n",
            "iteration : 7090 loss: 2.9858453273773193\n",
            "iteration : 7091 loss: 2.984400749206543\n",
            "iteration : 7092 loss: 3.078850269317627\n",
            "iteration : 7093 loss: 2.940317153930664\n",
            "iteration : 7094 loss: 3.033442497253418\n",
            "iteration : 7095 loss: 2.7956366539001465\n",
            "iteration : 7096 loss: 2.5665359497070312\n",
            "iteration : 7097 loss: 2.410202741622925\n",
            "iteration : 7098 loss: 3.1114461421966553\n",
            "iteration : 7099 loss: 2.5032153129577637\n",
            "iteration : 7100 loss: 2.7318215370178223\n",
            "iteration : 7101 loss: 2.695338249206543\n",
            "iteration : 7102 loss: 2.668165683746338\n",
            "iteration : 7103 loss: 2.5743751525878906\n",
            "iteration : 7104 loss: 2.6402881145477295\n",
            "iteration : 7105 loss: 3.2958052158355713\n",
            "iteration : 7106 loss: 2.7480218410491943\n",
            "iteration : 7107 loss: 2.626781463623047\n",
            "iteration : 7108 loss: 2.9324233531951904\n",
            "iteration : 7109 loss: 2.979557514190674\n",
            "iteration : 7110 loss: 2.6678600311279297\n",
            "iteration : 7111 loss: 2.6614296436309814\n",
            "iteration : 7112 loss: 2.541785717010498\n",
            "iteration : 7113 loss: 2.7052133083343506\n",
            "iteration : 7114 loss: 2.7263808250427246\n",
            "iteration : 7115 loss: 2.5851235389709473\n",
            "iteration : 7116 loss: 2.8470067977905273\n",
            "iteration : 7117 loss: 2.6115896701812744\n",
            "iteration : 7118 loss: 2.9342665672302246\n",
            "iteration : 7119 loss: 2.6950795650482178\n",
            "iteration : 7120 loss: 2.6928553581237793\n",
            "iteration : 7121 loss: 2.6725921630859375\n",
            "iteration : 7122 loss: 2.758199453353882\n",
            "iteration : 7123 loss: 2.533328056335449\n",
            "iteration : 7124 loss: 2.7533504962921143\n",
            "iteration : 7125 loss: 2.891784191131592\n",
            "iteration : 7126 loss: 2.737375497817993\n",
            "iteration : 7127 loss: 2.5918543338775635\n",
            "iteration : 7128 loss: 2.6752631664276123\n",
            "iteration : 7129 loss: 2.8630452156066895\n",
            "iteration : 7130 loss: 2.8446404933929443\n",
            "iteration : 7131 loss: 2.7733988761901855\n",
            "iteration : 7132 loss: 2.79931640625\n",
            "iteration : 7133 loss: 2.6354377269744873\n",
            "iteration : 7134 loss: 2.7428231239318848\n",
            "iteration : 7135 loss: 2.627368211746216\n",
            "iteration : 7136 loss: 2.652310848236084\n",
            "iteration : 7137 loss: 2.5026400089263916\n",
            "iteration : 7138 loss: 2.5119876861572266\n",
            "iteration : 7139 loss: 2.7550225257873535\n",
            "iteration : 7140 loss: 2.658583641052246\n",
            "iteration : 7141 loss: 2.6413822174072266\n",
            "iteration : 7142 loss: 2.934440851211548\n",
            "iteration : 7143 loss: 2.743351697921753\n",
            "iteration : 7144 loss: 3.5332489013671875\n",
            "iteration : 7145 loss: 2.8102941513061523\n",
            "iteration : 7146 loss: 2.777000904083252\n",
            "iteration : 7147 loss: 2.453249454498291\n",
            "iteration : 7148 loss: 2.474926710128784\n",
            "iteration : 7149 loss: 2.636986255645752\n",
            "iteration : 7150 loss: 2.653564691543579\n",
            "iteration : 7151 loss: 3.4440386295318604\n",
            "iteration : 7152 loss: 2.916276454925537\n",
            "iteration : 7153 loss: 2.5784363746643066\n",
            "iteration : 7154 loss: 2.9354991912841797\n",
            "iteration : 7155 loss: 2.7497739791870117\n",
            "iteration : 7156 loss: 2.773308038711548\n",
            "iteration : 7157 loss: 2.5297491550445557\n",
            "iteration : 7158 loss: 2.477400302886963\n",
            "iteration : 7159 loss: 2.6600053310394287\n",
            "iteration : 7160 loss: 2.5199103355407715\n",
            "iteration : 7161 loss: 2.4184648990631104\n",
            "iteration : 7162 loss: 2.723508834838867\n",
            "iteration : 7163 loss: 2.6696808338165283\n",
            "iteration : 7164 loss: 2.8863189220428467\n",
            "iteration : 7165 loss: 2.9026713371276855\n",
            "iteration : 7166 loss: 2.800917387008667\n",
            "iteration : 7167 loss: 2.8112683296203613\n",
            "iteration : 7168 loss: 2.805133104324341\n",
            "iteration : 7169 loss: 2.7334096431732178\n",
            "iteration : 7170 loss: 2.742520332336426\n",
            "iteration : 7171 loss: 2.7566068172454834\n",
            "iteration : 7172 loss: 2.5490047931671143\n",
            "iteration : 7173 loss: 2.6692988872528076\n",
            "iteration : 7174 loss: 2.6833655834198\n",
            "iteration : 7175 loss: 2.616788148880005\n",
            "iteration : 7176 loss: 2.6740214824676514\n",
            "iteration : 7177 loss: 2.7563910484313965\n",
            "iteration : 7178 loss: 2.8617422580718994\n",
            "iteration : 7179 loss: 2.5698747634887695\n",
            "iteration : 7180 loss: 2.62276291847229\n",
            "iteration : 7181 loss: 2.964843511581421\n",
            "iteration : 7182 loss: 2.8203768730163574\n",
            "iteration : 7183 loss: 2.6763267517089844\n",
            "iteration : 7184 loss: 2.733720064163208\n",
            "iteration : 7185 loss: 2.691011428833008\n",
            "iteration : 7186 loss: 2.6621463298797607\n",
            "iteration : 7187 loss: 2.5184714794158936\n",
            "iteration : 7188 loss: 2.5918068885803223\n",
            "iteration : 7189 loss: 2.869216203689575\n",
            "iteration : 7190 loss: 3.2218122482299805\n",
            "iteration : 7191 loss: 3.0196645259857178\n",
            "iteration : 7192 loss: 3.1688430309295654\n",
            "iteration : 7193 loss: 3.084418535232544\n",
            "iteration : 7194 loss: 3.183364152908325\n",
            "iteration : 7195 loss: 3.0926923751831055\n",
            "iteration : 7196 loss: 2.947883367538452\n",
            "iteration : 7197 loss: 2.8363091945648193\n",
            "iteration : 7198 loss: 2.9047534465789795\n",
            "iteration : 7199 loss: 2.6832964420318604\n",
            "iteration : 7200 loss: 2.583888292312622\n",
            "iteration : 7201 loss: 2.5205976963043213\n",
            "iteration : 7202 loss: 3.083448648452759\n",
            "iteration : 7203 loss: 2.4134249687194824\n",
            "iteration : 7204 loss: 2.2527945041656494\n",
            "iteration : 7205 loss: 2.199657440185547\n",
            "iteration : 7206 loss: 2.451230525970459\n",
            "iteration : 7207 loss: 2.273197889328003\n",
            "iteration : 7208 loss: 2.034653663635254\n",
            "iteration : 7209 loss: 1.9898961782455444\n",
            "iteration : 7210 loss: 2.1355671882629395\n",
            "iteration : 7211 loss: 1.9230364561080933\n",
            "iteration : 7212 loss: 2.267474889755249\n",
            "iteration : 7213 loss: 1.8626800775527954\n",
            "iteration : 7214 loss: 2.1334333419799805\n",
            "iteration : 7215 loss: 2.218750238418579\n",
            "iteration : 7216 loss: 1.9209736585617065\n",
            "iteration : 7217 loss: 2.4362680912017822\n",
            "iteration : 7218 loss: 3.5864017009735107\n",
            "iteration : 7219 loss: 3.463578701019287\n",
            "iteration : 7220 loss: 3.739720582962036\n",
            "iteration : 7221 loss: 3.679236888885498\n",
            "iteration : 7222 loss: 3.7241427898406982\n",
            "iteration : 7223 loss: 3.740375280380249\n",
            "iteration : 7224 loss: 3.6677894592285156\n",
            "iteration : 7225 loss: 3.4036619663238525\n",
            "iteration : 7226 loss: 2.9808387756347656\n",
            "iteration : 7227 loss: 3.429054021835327\n",
            "iteration : 7228 loss: 3.263031244277954\n",
            "iteration : 7229 loss: 3.2195427417755127\n",
            "iteration : 7230 loss: 3.007845878601074\n",
            "iteration : 7231 loss: 3.1130788326263428\n",
            "iteration : 7232 loss: 3.0209386348724365\n",
            "iteration : 7233 loss: 2.9248623847961426\n",
            "iteration : 7234 loss: 2.941601514816284\n",
            "iteration : 7235 loss: 2.9569649696350098\n",
            "iteration : 7236 loss: 3.0538649559020996\n",
            "iteration : 7237 loss: 2.9443633556365967\n",
            "iteration : 7238 loss: 3.0173721313476562\n",
            "iteration : 7239 loss: 2.927877187728882\n",
            "iteration : 7240 loss: 2.7157793045043945\n",
            "iteration : 7241 loss: 2.8898894786834717\n",
            "iteration : 7242 loss: 3.0192935466766357\n",
            "iteration : 7243 loss: 2.782792806625366\n",
            "iteration : 7244 loss: 2.57584547996521\n",
            "iteration : 7245 loss: 2.950467109680176\n",
            "iteration : 7246 loss: 2.717601776123047\n",
            "iteration : 7247 loss: 2.6677627563476562\n",
            "iteration : 7248 loss: 2.603898525238037\n",
            "iteration : 7249 loss: 2.7524967193603516\n",
            "iteration : 7250 loss: 2.416905164718628\n",
            "iteration : 7251 loss: 2.6841208934783936\n",
            "iteration : 7252 loss: 3.1438004970550537\n",
            "iteration : 7253 loss: 3.653404951095581\n",
            "iteration : 7254 loss: 3.560724973678589\n",
            "iteration : 7255 loss: 3.4520328044891357\n",
            "iteration : 7256 loss: 3.5570099353790283\n",
            "iteration : 7257 loss: 3.4841067790985107\n",
            "iteration : 7258 loss: 3.0220868587493896\n",
            "iteration : 7259 loss: 2.9166951179504395\n",
            "iteration : 7260 loss: 2.8174452781677246\n",
            "iteration : 7261 loss: 2.714715003967285\n",
            "iteration : 7262 loss: 3.113677740097046\n",
            "iteration : 7263 loss: 2.699087142944336\n",
            "iteration : 7264 loss: 2.532510995864868\n",
            "iteration : 7265 loss: 2.5732808113098145\n",
            "iteration : 7266 loss: 2.691176414489746\n",
            "iteration : 7267 loss: 2.967902660369873\n",
            "iteration : 7268 loss: 3.0784168243408203\n",
            "iteration : 7269 loss: 2.7947921752929688\n",
            "iteration : 7270 loss: 2.6955976486206055\n",
            "iteration : 7271 loss: 2.7610037326812744\n",
            "iteration : 7272 loss: 2.654602289199829\n",
            "iteration : 7273 loss: 2.7963905334472656\n",
            "iteration : 7274 loss: 2.6275241374969482\n",
            "iteration : 7275 loss: 2.879672050476074\n",
            "iteration : 7276 loss: 2.879096269607544\n",
            "iteration : 7277 loss: 2.7110414505004883\n",
            "iteration : 7278 loss: 3.113532781600952\n",
            "iteration : 7279 loss: 2.784893274307251\n",
            "iteration : 7280 loss: 2.960875988006592\n",
            "iteration : 7281 loss: 3.0814669132232666\n",
            "iteration : 7282 loss: 3.068007469177246\n",
            "iteration : 7283 loss: 2.878631830215454\n",
            "iteration : 7284 loss: 2.9348442554473877\n",
            "iteration : 7285 loss: 3.0100066661834717\n",
            "iteration : 7286 loss: 3.180579423904419\n",
            "iteration : 7287 loss: 2.919165849685669\n",
            "iteration : 7288 loss: 2.947556972503662\n",
            "iteration : 7289 loss: 2.7518115043640137\n",
            "iteration : 7290 loss: 3.0262503623962402\n",
            "iteration : 7291 loss: 2.9507339000701904\n",
            "iteration : 7292 loss: 2.9120078086853027\n",
            "iteration : 7293 loss: 2.958770513534546\n",
            "iteration : 7294 loss: 2.9663071632385254\n",
            "iteration : 7295 loss: 3.0960662364959717\n",
            "iteration : 7296 loss: 2.8846023082733154\n",
            "iteration : 7297 loss: 2.796112298965454\n",
            "iteration : 7298 loss: 2.9044392108917236\n",
            "iteration : 7299 loss: 2.8041739463806152\n",
            "iteration : 7300 loss: 3.0068321228027344\n",
            "iteration : 7301 loss: 2.975355863571167\n",
            "iteration : 7302 loss: 2.9312796592712402\n",
            "iteration : 7303 loss: 2.944282293319702\n",
            "iteration : 7304 loss: 2.898036241531372\n",
            "iteration : 7305 loss: 2.8689255714416504\n",
            "iteration : 7306 loss: 2.848395824432373\n",
            "iteration : 7307 loss: 2.890303611755371\n",
            "iteration : 7308 loss: 2.8594937324523926\n",
            "iteration : 7309 loss: 2.969663619995117\n",
            "iteration : 7310 loss: 2.8314120769500732\n",
            "iteration : 7311 loss: 2.8093044757843018\n",
            "iteration : 7312 loss: 2.793383836746216\n",
            "iteration : 7313 loss: 2.890578508377075\n",
            "iteration : 7314 loss: 2.8624160289764404\n",
            "iteration : 7315 loss: 2.8729608058929443\n",
            "iteration : 7316 loss: 2.784137725830078\n",
            "iteration : 7317 loss: 2.7599947452545166\n",
            "iteration : 7318 loss: 2.778411626815796\n",
            "iteration : 7319 loss: 2.735194444656372\n",
            "iteration : 7320 loss: 2.770275592803955\n",
            "iteration : 7321 loss: 2.84477162361145\n",
            "iteration : 7322 loss: 2.7429392337799072\n",
            "iteration : 7323 loss: 2.729180335998535\n",
            "iteration : 7324 loss: 2.8478634357452393\n",
            "iteration : 7325 loss: 2.672466993331909\n",
            "iteration : 7326 loss: 2.752741813659668\n",
            "iteration : 7327 loss: 2.722423553466797\n",
            "iteration : 7328 loss: 2.7009074687957764\n",
            "iteration : 7329 loss: 2.706098794937134\n",
            "iteration : 7330 loss: 2.670854091644287\n",
            "iteration : 7331 loss: 2.814621925354004\n",
            "iteration : 7332 loss: 2.75117564201355\n",
            "iteration : 7333 loss: 2.7430074214935303\n",
            "iteration : 7334 loss: 2.6903626918792725\n",
            "iteration : 7335 loss: 2.8335094451904297\n",
            "iteration : 7336 loss: 2.661031723022461\n",
            "iteration : 7337 loss: 2.6431593894958496\n",
            "iteration : 7338 loss: 2.811204195022583\n",
            "iteration : 7339 loss: 2.864525318145752\n",
            "iteration : 7340 loss: 2.843729019165039\n",
            "iteration : 7341 loss: 2.75597882270813\n",
            "iteration : 7342 loss: 2.811427593231201\n",
            "iteration : 7343 loss: 2.900239944458008\n",
            "iteration : 7344 loss: 2.6545114517211914\n",
            "iteration : 7345 loss: 2.60027813911438\n",
            "iteration : 7346 loss: 2.6049723625183105\n",
            "iteration : 7347 loss: 2.9202327728271484\n",
            "iteration : 7348 loss: 2.729304075241089\n",
            "iteration : 7349 loss: 2.819254159927368\n",
            "iteration : 7350 loss: 2.7005794048309326\n",
            "iteration : 7351 loss: 2.6297128200531006\n",
            "iteration : 7352 loss: 2.8954334259033203\n",
            "iteration : 7353 loss: 2.720017194747925\n",
            "iteration : 7354 loss: 2.661733627319336\n",
            "iteration : 7355 loss: 2.904160261154175\n",
            "iteration : 7356 loss: 2.4905459880828857\n",
            "iteration : 7357 loss: 2.520616292953491\n",
            "iteration : 7358 loss: 2.9017601013183594\n",
            "iteration : 7359 loss: 3.2050037384033203\n",
            "iteration : 7360 loss: 2.808450698852539\n",
            "iteration : 7361 loss: 2.4655518531799316\n",
            "iteration : 7362 loss: 2.5123298168182373\n",
            "iteration : 7363 loss: 2.9342498779296875\n",
            "iteration : 7364 loss: 2.7987923622131348\n",
            "iteration : 7365 loss: 2.7199018001556396\n",
            "iteration : 7366 loss: 2.919494867324829\n",
            "iteration : 7367 loss: 2.792814254760742\n",
            "iteration : 7368 loss: 2.7838134765625\n",
            "iteration : 7369 loss: 2.8535876274108887\n",
            "iteration : 7370 loss: 2.7315304279327393\n",
            "iteration : 7371 loss: 2.755124092102051\n",
            "iteration : 7372 loss: 3.0284109115600586\n",
            "iteration : 7373 loss: 2.5764596462249756\n",
            "iteration : 7374 loss: 2.598672866821289\n",
            "iteration : 7375 loss: 2.8815853595733643\n",
            "iteration : 7376 loss: 2.917858839035034\n",
            "iteration : 7377 loss: 2.71709942817688\n",
            "iteration : 7378 loss: 2.870682954788208\n",
            "iteration : 7379 loss: 2.641042947769165\n",
            "iteration : 7380 loss: 2.5756001472473145\n",
            "iteration : 7381 loss: 2.6477794647216797\n",
            "iteration : 7382 loss: 3.0016160011291504\n",
            "iteration : 7383 loss: 2.7066571712493896\n",
            "iteration : 7384 loss: 3.2272346019744873\n",
            "iteration : 7385 loss: 2.830414295196533\n",
            "iteration : 7386 loss: 2.6761205196380615\n",
            "iteration : 7387 loss: 2.6868157386779785\n",
            "iteration : 7388 loss: 2.588958263397217\n",
            "iteration : 7389 loss: 2.529764175415039\n",
            "iteration : 7390 loss: 2.5826849937438965\n",
            "iteration : 7391 loss: 2.8381028175354004\n",
            "iteration : 7392 loss: 2.761655807495117\n",
            "iteration : 7393 loss: 2.6176843643188477\n",
            "iteration : 7394 loss: 2.8095309734344482\n",
            "iteration : 7395 loss: 2.7208645343780518\n",
            "iteration : 7396 loss: 2.74745512008667\n",
            "iteration : 7397 loss: 2.7546398639678955\n",
            "iteration : 7398 loss: 2.678332805633545\n",
            "iteration : 7399 loss: 2.6885721683502197\n",
            "iteration : 7400 loss: 2.6437888145446777\n",
            "iteration : 7401 loss: 2.663071393966675\n",
            "iteration : 7402 loss: 2.715679407119751\n",
            "iteration : 7403 loss: 2.6966421604156494\n",
            "iteration : 7404 loss: 2.7327394485473633\n",
            "iteration : 7405 loss: 2.6583657264709473\n",
            "iteration : 7406 loss: 2.60866117477417\n",
            "iteration : 7407 loss: 2.6337010860443115\n",
            "iteration : 7408 loss: 2.6580824851989746\n",
            "iteration : 7409 loss: 2.65702748298645\n",
            "iteration : 7410 loss: 2.632437229156494\n",
            "iteration : 7411 loss: 2.6309964656829834\n",
            "iteration : 7412 loss: 2.6024911403656006\n",
            "iteration : 7413 loss: 2.7846314907073975\n",
            "iteration : 7414 loss: 2.7673885822296143\n",
            "iteration : 7415 loss: 2.6930196285247803\n",
            "iteration : 7416 loss: 2.6498987674713135\n",
            "iteration : 7417 loss: 2.8357717990875244\n",
            "iteration : 7418 loss: 2.502641201019287\n",
            "iteration : 7419 loss: 2.6897335052490234\n",
            "iteration : 7420 loss: 2.8877646923065186\n",
            "iteration : 7421 loss: 2.703620672225952\n",
            "iteration : 7422 loss: 2.5281434059143066\n",
            "iteration : 7423 loss: 2.852684736251831\n",
            "iteration : 7424 loss: 2.7250726222991943\n",
            "iteration : 7425 loss: 2.7382824420928955\n",
            "iteration : 7426 loss: 2.5586228370666504\n",
            "iteration : 7427 loss: 2.5616962909698486\n",
            "iteration : 7428 loss: 2.556178092956543\n",
            "iteration : 7429 loss: 2.8897078037261963\n",
            "iteration : 7430 loss: 2.8166778087615967\n",
            "iteration : 7431 loss: 2.6039040088653564\n",
            "iteration : 7432 loss: 2.8338329792022705\n",
            "iteration : 7433 loss: 2.8059420585632324\n",
            "iteration : 7434 loss: 2.5183732509613037\n",
            "iteration : 7435 loss: 2.8226230144500732\n",
            "iteration : 7436 loss: 2.910396099090576\n",
            "iteration : 7437 loss: 3.408702850341797\n",
            "iteration : 7438 loss: 3.3826844692230225\n",
            "iteration : 7439 loss: 3.3308629989624023\n",
            "iteration : 7440 loss: 2.957573175430298\n",
            "iteration : 7441 loss: 2.6613736152648926\n",
            "iteration : 7442 loss: 2.6427690982818604\n",
            "iteration : 7443 loss: 2.7401034832000732\n",
            "iteration : 7444 loss: 2.6730587482452393\n",
            "iteration : 7445 loss: 2.6287457942962646\n",
            "iteration : 7446 loss: 2.7104599475860596\n",
            "iteration : 7447 loss: 2.746116876602173\n",
            "iteration : 7448 loss: 2.681184768676758\n",
            "iteration : 7449 loss: 2.567134380340576\n",
            "iteration : 7450 loss: 2.573296070098877\n",
            "iteration : 7451 loss: 2.679222345352173\n",
            "iteration : 7452 loss: 2.630932331085205\n",
            "iteration : 7453 loss: 2.5522077083587646\n",
            "iteration : 7454 loss: 2.5886168479919434\n",
            "iteration : 7455 loss: 2.7843968868255615\n",
            "iteration : 7456 loss: 3.1203808784484863\n",
            "iteration : 7457 loss: 2.6692652702331543\n",
            "iteration : 7458 loss: 2.4743988513946533\n",
            "iteration : 7459 loss: 2.6378798484802246\n",
            "iteration : 7460 loss: 2.6603477001190186\n",
            "iteration : 7461 loss: 2.5810482501983643\n",
            "iteration : 7462 loss: 2.5938894748687744\n",
            "iteration : 7463 loss: 2.5669913291931152\n",
            "iteration : 7464 loss: 2.803459405899048\n",
            "iteration : 7465 loss: 2.655240774154663\n",
            "iteration : 7466 loss: 2.6223440170288086\n",
            "iteration : 7467 loss: 2.733067750930786\n",
            "iteration : 7468 loss: 2.5087430477142334\n",
            "iteration : 7469 loss: 3.1118056774139404\n",
            "iteration : 7470 loss: 2.6306955814361572\n",
            "iteration : 7471 loss: 2.738668918609619\n",
            "iteration : 7472 loss: 2.6605465412139893\n",
            "iteration : 7473 loss: 2.657186269760132\n",
            "iteration : 7474 loss: 2.7151994705200195\n",
            "iteration : 7475 loss: 2.7565670013427734\n",
            "iteration : 7476 loss: 2.614785671234131\n",
            "iteration : 7477 loss: 2.7891011238098145\n",
            "iteration : 7478 loss: 2.8246231079101562\n",
            "iteration : 7479 loss: 2.738297939300537\n",
            "iteration : 7480 loss: 2.7152936458587646\n",
            "iteration : 7481 loss: 2.6633949279785156\n",
            "iteration : 7482 loss: 2.7324962615966797\n",
            "iteration : 7483 loss: 2.712266206741333\n",
            "iteration : 7484 loss: 2.693586826324463\n",
            "iteration : 7485 loss: 2.621068239212036\n",
            "iteration : 7486 loss: 2.700003147125244\n",
            "iteration : 7487 loss: 2.692253351211548\n",
            "iteration : 7488 loss: 2.8371286392211914\n",
            "iteration : 7489 loss: 2.8532962799072266\n",
            "iteration : 7490 loss: 2.661172866821289\n",
            "iteration : 7491 loss: 2.6082234382629395\n",
            "iteration : 7492 loss: 3.221409797668457\n",
            "iteration : 7493 loss: 2.938673734664917\n",
            "iteration : 7494 loss: 2.6778488159179688\n",
            "iteration : 7495 loss: 3.0409350395202637\n",
            "iteration : 7496 loss: 3.3016936779022217\n",
            "iteration : 7497 loss: 2.511653184890747\n",
            "iteration : 7498 loss: 2.6799933910369873\n",
            "iteration : 7499 loss: 2.9018566608428955\n",
            "iteration : 7500 loss: 2.5790998935699463\n",
            "iteration : 7501 loss: 2.4670932292938232\n",
            "iteration : 7502 loss: 2.9310390949249268\n",
            "iteration : 7503 loss: 2.975257396697998\n",
            "iteration : 7504 loss: 2.9730122089385986\n",
            "iteration : 7505 loss: 2.770691156387329\n",
            "iteration : 7506 loss: 2.7532312870025635\n",
            "iteration : 7507 loss: 3.1011550426483154\n",
            "iteration : 7508 loss: 2.8008501529693604\n",
            "iteration : 7509 loss: 2.688776731491089\n",
            "iteration : 7510 loss: 2.7188806533813477\n",
            "iteration : 7511 loss: 2.943338632583618\n",
            "iteration : 7512 loss: 2.7023885250091553\n",
            "iteration : 7513 loss: 2.944387435913086\n",
            "iteration : 7514 loss: 3.0238397121429443\n",
            "iteration : 7515 loss: 2.8156373500823975\n",
            "iteration : 7516 loss: 2.801118850708008\n",
            "iteration : 7517 loss: 2.853754758834839\n",
            "iteration : 7518 loss: 2.3720738887786865\n",
            "iteration : 7519 loss: 2.366431474685669\n",
            "iteration : 7520 loss: 2.408172369003296\n",
            "iteration : 7521 loss: 2.830848455429077\n",
            "iteration : 7522 loss: 2.5946640968322754\n",
            "iteration : 7523 loss: 2.6534342765808105\n",
            "iteration : 7524 loss: 2.712266206741333\n",
            "iteration : 7525 loss: 2.610050916671753\n",
            "iteration : 7526 loss: 2.617884635925293\n",
            "iteration : 7527 loss: 2.8068408966064453\n",
            "iteration : 7528 loss: 2.5690741539001465\n",
            "iteration : 7529 loss: 2.752314329147339\n",
            "iteration : 7530 loss: 2.8482136726379395\n",
            "iteration : 7531 loss: 3.069664239883423\n",
            "iteration : 7532 loss: 2.676854133605957\n",
            "iteration : 7533 loss: 2.690032482147217\n",
            "iteration : 7534 loss: 2.554091691970825\n",
            "iteration : 7535 loss: 3.3296890258789062\n",
            "iteration : 7536 loss: 3.2089719772338867\n",
            "iteration : 7537 loss: 3.003009796142578\n",
            "iteration : 7538 loss: 2.6288914680480957\n",
            "iteration : 7539 loss: 2.5833146572113037\n",
            "iteration : 7540 loss: 2.970733642578125\n",
            "iteration : 7541 loss: 2.534820318222046\n",
            "iteration : 7542 loss: 2.47965145111084\n",
            "iteration : 7543 loss: 2.7766993045806885\n",
            "iteration : 7544 loss: 2.804626941680908\n",
            "iteration : 7545 loss: 2.601874589920044\n",
            "iteration : 7546 loss: 2.7280428409576416\n",
            "iteration : 7547 loss: 2.8879332542419434\n",
            "iteration : 7548 loss: 2.8559162616729736\n",
            "iteration : 7549 loss: 2.96527361869812\n",
            "iteration : 7550 loss: 3.0149073600769043\n",
            "iteration : 7551 loss: 2.4768104553222656\n",
            "iteration : 7552 loss: 3.077312707901001\n",
            "iteration : 7553 loss: 3.282917022705078\n",
            "iteration : 7554 loss: 2.9641003608703613\n",
            "iteration : 7555 loss: 2.654282331466675\n",
            "iteration : 7556 loss: 3.2086286544799805\n",
            "iteration : 7557 loss: 2.725573778152466\n",
            "iteration : 7558 loss: 2.725679636001587\n",
            "iteration : 7559 loss: 2.6660618782043457\n",
            "iteration : 7560 loss: 2.976738452911377\n",
            "iteration : 7561 loss: 2.8060524463653564\n",
            "iteration : 7562 loss: 2.6717758178710938\n",
            "iteration : 7563 loss: 2.810235023498535\n",
            "iteration : 7564 loss: 2.778881072998047\n",
            "iteration : 7565 loss: 2.707217216491699\n",
            "iteration : 7566 loss: 2.6549112796783447\n",
            "iteration : 7567 loss: 2.6169049739837646\n",
            "iteration : 7568 loss: 2.5824954509735107\n",
            "iteration : 7569 loss: 2.5529065132141113\n",
            "iteration : 7570 loss: 2.7430927753448486\n",
            "iteration : 7571 loss: 2.6118345260620117\n",
            "iteration : 7572 loss: 2.5643465518951416\n",
            "iteration : 7573 loss: 2.6648635864257812\n",
            "iteration : 7574 loss: 2.772904396057129\n",
            "iteration : 7575 loss: 2.696385383605957\n",
            "iteration : 7576 loss: 2.679678440093994\n",
            "iteration : 7577 loss: 2.5991387367248535\n",
            "iteration : 7578 loss: 2.563567638397217\n",
            "iteration : 7579 loss: 2.688239336013794\n",
            "iteration : 7580 loss: 2.735196352005005\n",
            "iteration : 7581 loss: 2.636587619781494\n",
            "iteration : 7582 loss: 2.6611475944519043\n",
            "iteration : 7583 loss: 2.799470901489258\n",
            "iteration : 7584 loss: 2.5758702754974365\n",
            "iteration : 7585 loss: 2.5557169914245605\n",
            "iteration : 7586 loss: 2.5198171138763428\n",
            "iteration : 7587 loss: 2.5089633464813232\n",
            "iteration : 7588 loss: 2.628718137741089\n",
            "iteration : 7589 loss: 2.529162645339966\n",
            "iteration : 7590 loss: 2.4762275218963623\n",
            "iteration : 7591 loss: 2.5614418983459473\n",
            "iteration : 7592 loss: 2.504875421524048\n",
            "iteration : 7593 loss: 2.5748696327209473\n",
            "iteration : 7594 loss: 2.534026861190796\n",
            "iteration : 7595 loss: 2.5434839725494385\n",
            "iteration : 7596 loss: 2.889991044998169\n",
            "iteration : 7597 loss: 2.520454168319702\n",
            "iteration : 7598 loss: 2.452260971069336\n",
            "iteration : 7599 loss: 2.4492311477661133\n",
            "iteration : 7600 loss: 2.7035093307495117\n",
            "iteration : 7601 loss: 2.5624420642852783\n",
            "iteration : 7602 loss: 2.393831968307495\n",
            "iteration : 7603 loss: 2.386331558227539\n",
            "iteration : 7604 loss: 2.60360050201416\n",
            "iteration : 7605 loss: 2.46295428276062\n",
            "iteration : 7606 loss: 2.529280185699463\n",
            "iteration : 7607 loss: 2.5025558471679688\n",
            "iteration : 7608 loss: 2.584273338317871\n",
            "iteration : 7609 loss: 2.563981294631958\n",
            "iteration : 7610 loss: 2.380589485168457\n",
            "iteration : 7611 loss: 2.5684216022491455\n",
            "iteration : 7612 loss: 2.896237373352051\n",
            "iteration : 7613 loss: 2.6846237182617188\n",
            "iteration : 7614 loss: 2.463191509246826\n",
            "iteration : 7615 loss: 2.594254970550537\n",
            "iteration : 7616 loss: 2.690730333328247\n",
            "iteration : 7617 loss: 2.713883638381958\n",
            "iteration : 7618 loss: 3.1242477893829346\n",
            "iteration : 7619 loss: 2.403573513031006\n",
            "iteration : 7620 loss: 2.4160592555999756\n",
            "iteration : 7621 loss: 3.005791425704956\n",
            "iteration : 7622 loss: 3.2926173210144043\n",
            "iteration : 7623 loss: 2.598139762878418\n",
            "iteration : 7624 loss: 2.9184091091156006\n",
            "iteration : 7625 loss: 2.6096742153167725\n",
            "iteration : 7626 loss: 2.5516631603240967\n",
            "iteration : 7627 loss: 2.793041229248047\n",
            "iteration : 7628 loss: 2.748201608657837\n",
            "iteration : 7629 loss: 2.5693390369415283\n",
            "iteration : 7630 loss: 2.7827441692352295\n",
            "iteration : 7631 loss: 2.933199644088745\n",
            "iteration : 7632 loss: 2.806302547454834\n",
            "iteration : 7633 loss: 2.620814800262451\n",
            "iteration : 7634 loss: 2.9566943645477295\n",
            "iteration : 7635 loss: 2.567600727081299\n",
            "iteration : 7636 loss: 2.6009743213653564\n",
            "iteration : 7637 loss: 2.452975273132324\n",
            "iteration : 7638 loss: 3.279641628265381\n",
            "iteration : 7639 loss: 2.700838804244995\n",
            "iteration : 7640 loss: 2.716008186340332\n",
            "iteration : 7641 loss: 2.6913461685180664\n",
            "iteration : 7642 loss: 2.71921443939209\n",
            "iteration : 7643 loss: 2.4173552989959717\n",
            "iteration : 7644 loss: 2.6670215129852295\n",
            "iteration : 7645 loss: 2.714751958847046\n",
            "iteration : 7646 loss: 2.5333030223846436\n",
            "iteration : 7647 loss: 2.820887804031372\n",
            "iteration : 7648 loss: 2.5465633869171143\n",
            "iteration : 7649 loss: 2.6945488452911377\n",
            "iteration : 7650 loss: 2.844428300857544\n",
            "iteration : 7651 loss: 2.7706642150878906\n",
            "iteration : 7652 loss: 3.177506685256958\n",
            "iteration : 7653 loss: 2.596038341522217\n",
            "iteration : 7654 loss: 2.665771484375\n",
            "iteration : 7655 loss: 2.369960069656372\n",
            "iteration : 7656 loss: 2.639376640319824\n",
            "iteration : 7657 loss: 2.5130741596221924\n",
            "iteration : 7658 loss: 2.648585796356201\n",
            "iteration : 7659 loss: 2.5597360134124756\n",
            "iteration : 7660 loss: 2.524679660797119\n",
            "iteration : 7661 loss: 2.5799100399017334\n",
            "iteration : 7662 loss: 2.501811981201172\n",
            "iteration : 7663 loss: 2.4360475540161133\n",
            "iteration : 7664 loss: 2.5616519451141357\n",
            "iteration : 7665 loss: 2.547811985015869\n",
            "iteration : 7666 loss: 2.480391025543213\n",
            "iteration : 7667 loss: 2.5456619262695312\n",
            "iteration : 7668 loss: 2.512403964996338\n",
            "iteration : 7669 loss: 2.365583658218384\n",
            "iteration : 7670 loss: 2.375363826751709\n",
            "iteration : 7671 loss: 3.9144508838653564\n",
            "iteration : 7672 loss: 3.9006519317626953\n",
            "iteration : 7673 loss: 3.848170042037964\n",
            "iteration : 7674 loss: 3.531902313232422\n",
            "iteration : 7675 loss: 2.812232732772827\n",
            "iteration : 7676 loss: 2.866046190261841\n",
            "iteration : 7677 loss: 2.30792498588562\n",
            "iteration : 7678 loss: 2.804389238357544\n",
            "iteration : 7679 loss: 2.338874578475952\n",
            "iteration : 7680 loss: 2.531003475189209\n",
            "iteration : 7681 loss: 2.5789406299591064\n",
            "iteration : 7682 loss: 2.914503335952759\n",
            "iteration : 7683 loss: 2.6391608715057373\n",
            "iteration : 7684 loss: 2.831962823867798\n",
            "iteration : 7685 loss: 3.1314797401428223\n",
            "iteration : 7686 loss: 3.065803289413452\n",
            "iteration : 7687 loss: 2.505019426345825\n",
            "iteration : 7688 loss: 2.2658143043518066\n",
            "iteration : 7689 loss: 2.2666983604431152\n",
            "iteration : 7690 loss: 2.260359525680542\n",
            "iteration : 7691 loss: 2.2512781620025635\n",
            "iteration : 7692 loss: 2.237776517868042\n",
            "iteration : 7693 loss: 2.2271740436553955\n",
            "iteration : 7694 loss: 2.2160184383392334\n",
            "iteration : 7695 loss: 2.1991028785705566\n",
            "iteration : 7696 loss: 2.1768739223480225\n",
            "iteration : 7697 loss: 2.1622912883758545\n",
            "iteration : 7698 loss: 2.149064064025879\n",
            "iteration : 7699 loss: 2.7106568813323975\n",
            "iteration : 7700 loss: 2.725111246109009\n",
            "iteration : 7701 loss: 2.6522183418273926\n",
            "iteration : 7702 loss: 2.7765204906463623\n",
            "iteration : 7703 loss: 2.958333969116211\n",
            "iteration : 7704 loss: 2.9317626953125\n",
            "iteration : 7705 loss: 2.672726631164551\n",
            "iteration : 7706 loss: 2.940286874771118\n",
            "iteration : 7707 loss: 2.9633615016937256\n",
            "iteration : 7708 loss: 2.885512113571167\n",
            "iteration : 7709 loss: 2.858370304107666\n",
            "iteration : 7710 loss: 2.7137601375579834\n",
            "iteration : 7711 loss: 2.8328850269317627\n",
            "iteration : 7712 loss: 2.7364861965179443\n",
            "iteration : 7713 loss: 2.4880714416503906\n",
            "iteration : 7714 loss: 2.624901533126831\n",
            "iteration : 7715 loss: 2.4672508239746094\n",
            "iteration : 7716 loss: 2.7217581272125244\n",
            "iteration : 7717 loss: 3.1606907844543457\n",
            "iteration : 7718 loss: 2.96287202835083\n",
            "iteration : 7719 loss: 2.8752095699310303\n",
            "iteration : 7720 loss: 3.001152276992798\n",
            "iteration : 7721 loss: 2.7733328342437744\n",
            "iteration : 7722 loss: 2.90563702583313\n",
            "iteration : 7723 loss: 2.8613221645355225\n",
            "iteration : 7724 loss: 3.2114689350128174\n",
            "iteration : 7725 loss: 3.208500385284424\n",
            "iteration : 7726 loss: 2.94795560836792\n",
            "iteration : 7727 loss: 2.9135382175445557\n",
            "iteration : 7728 loss: 2.8677420616149902\n",
            "iteration : 7729 loss: 2.774240255355835\n",
            "iteration : 7730 loss: 2.74851393699646\n",
            "iteration : 7731 loss: 2.7032968997955322\n",
            "iteration : 7732 loss: 2.829683780670166\n",
            "iteration : 7733 loss: 2.745497465133667\n",
            "iteration : 7734 loss: 3.2086329460144043\n",
            "iteration : 7735 loss: 3.2382633686065674\n",
            "iteration : 7736 loss: 3.2114665508270264\n",
            "iteration : 7737 loss: 3.1644506454467773\n",
            "iteration : 7738 loss: 2.8402373790740967\n",
            "iteration : 7739 loss: 2.5715620517730713\n",
            "iteration : 7740 loss: 2.610037088394165\n",
            "iteration : 7741 loss: 2.6293675899505615\n",
            "iteration : 7742 loss: 3.1726677417755127\n",
            "iteration : 7743 loss: 2.8257570266723633\n",
            "iteration : 7744 loss: 3.069406032562256\n",
            "iteration : 7745 loss: 2.710913896560669\n",
            "iteration : 7746 loss: 2.6196377277374268\n",
            "iteration : 7747 loss: 2.618319272994995\n",
            "iteration : 7748 loss: 2.613229513168335\n",
            "iteration : 7749 loss: 2.5948917865753174\n",
            "iteration : 7750 loss: 2.574838638305664\n",
            "iteration : 7751 loss: 2.5524442195892334\n",
            "iteration : 7752 loss: 2.5222723484039307\n",
            "iteration : 7753 loss: 2.4934985637664795\n",
            "iteration : 7754 loss: 2.4705889225006104\n",
            "iteration : 7755 loss: 2.450162649154663\n",
            "iteration : 7756 loss: 2.845447063446045\n",
            "iteration : 7757 loss: 2.939175605773926\n",
            "iteration : 7758 loss: 2.7133607864379883\n",
            "iteration : 7759 loss: 3.0880250930786133\n",
            "iteration : 7760 loss: 2.9377381801605225\n",
            "iteration : 7761 loss: 2.8712892532348633\n",
            "iteration : 7762 loss: 2.9085803031921387\n",
            "iteration : 7763 loss: 2.813789129257202\n",
            "iteration : 7764 loss: 2.465959310531616\n",
            "iteration : 7765 loss: 2.8935673236846924\n",
            "iteration : 7766 loss: 2.556586503982544\n",
            "iteration : 7767 loss: 2.704132080078125\n",
            "iteration : 7768 loss: 2.834955930709839\n",
            "iteration : 7769 loss: 2.819310188293457\n",
            "iteration : 7770 loss: 2.7428789138793945\n",
            "iteration : 7771 loss: 2.775270700454712\n",
            "iteration : 7772 loss: 2.6994528770446777\n",
            "iteration : 7773 loss: 2.816934108734131\n",
            "iteration : 7774 loss: 3.2199912071228027\n",
            "iteration : 7775 loss: 3.009932279586792\n",
            "iteration : 7776 loss: 3.0456600189208984\n",
            "iteration : 7777 loss: 3.117549419403076\n",
            "iteration : 7778 loss: 3.105825185775757\n",
            "iteration : 7779 loss: 2.6709041595458984\n",
            "iteration : 7780 loss: 2.6733057498931885\n",
            "iteration : 7781 loss: 2.6816461086273193\n",
            "iteration : 7782 loss: 2.635422945022583\n",
            "iteration : 7783 loss: 2.7773189544677734\n",
            "iteration : 7784 loss: 2.8377366065979004\n",
            "iteration : 7785 loss: 2.812570095062256\n",
            "iteration : 7786 loss: 2.7915210723876953\n",
            "iteration : 7787 loss: 2.8774189949035645\n",
            "iteration : 7788 loss: 2.711414337158203\n",
            "iteration : 7789 loss: 2.752279281616211\n",
            "iteration : 7790 loss: 2.882946491241455\n",
            "iteration : 7791 loss: 2.8916172981262207\n",
            "iteration : 7792 loss: 2.723721742630005\n",
            "iteration : 7793 loss: 2.5352718830108643\n",
            "iteration : 7794 loss: 2.8534188270568848\n",
            "iteration : 7795 loss: 3.07010817527771\n",
            "iteration : 7796 loss: 3.0575993061065674\n",
            "iteration : 7797 loss: 3.0261173248291016\n",
            "iteration : 7798 loss: 2.9970743656158447\n",
            "iteration : 7799 loss: 2.9408857822418213\n",
            "iteration : 7800 loss: 2.737318277359009\n",
            "iteration : 7801 loss: 2.7444114685058594\n",
            "iteration : 7802 loss: 2.834749937057495\n",
            "iteration : 7803 loss: 2.908869743347168\n",
            "iteration : 7804 loss: 2.7572453022003174\n",
            "iteration : 7805 loss: 2.6761744022369385\n",
            "iteration : 7806 loss: 2.682319402694702\n",
            "iteration : 7807 loss: 2.7694549560546875\n",
            "iteration : 7808 loss: 2.8313920497894287\n",
            "iteration : 7809 loss: 3.027193546295166\n",
            "iteration : 7810 loss: 2.7433969974517822\n",
            "iteration : 7811 loss: 2.7077362537384033\n",
            "iteration : 7812 loss: 2.7778446674346924\n"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "model = Transformer(config)\n",
        "model.to(get_device())\n",
        "#model = torch.compile(model)\n",
        "\n",
        "batchify = create_batch(shadr,meters,config.batch_size,config.max_seq_len)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "num_batchs = len(shadr) // config.batch_size\n",
        "print(num_batchs)\n",
        "for i in range(num_batchs):\n",
        "    x,y = batchify.next_batch()\n",
        "    x = torch.tensor(x).to(get_device())\n",
        "    y = torch.tensor(y).to(get_device())\n",
        "    optimizer.zero_grad()\n",
        "    out, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"iteration : {i+1} loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzDmHn3zJPA3",
        "outputId": "fec5e85c-d574-43c4-b027-2a0480a03e3f"
      },
      "id": "PzDmHn3zJPA3",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'drive/MyDrive/model_weights.pth')"
      ],
      "metadata": {
        "id": "aXbHb8EnJUhs"
      },
      "id": "aXbHb8EnJUhs",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "BPw7HMX_JbNS"
      },
      "id": "BPw7HMX_JbNS",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(text):\n",
        "    return tiktoken.get_encoding('gpt2').decode(text)"
      ],
      "metadata": {
        "id": "e1yjoJBsLKmh"
      },
      "id": "e1yjoJBsLKmh",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "a81de73b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a81de73b",
        "outputId": "cce0a072-a262-4e6c-b111-05e678ea45d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المجتث                                                 , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المنسرح                                                 , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال�الالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المديد                                                   , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الكامل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: المتقارب                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: البسيط                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الخفيف                                                , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الرمل                                                    , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الوافر                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n",
            "target: الطويل                                                  , predicted: الالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالالال\n"
          ]
        }
      ],
      "source": [
        "batchify = create_batch(shadr,meters,config.batch_size,config.max_seq_len)\n",
        "# Sampling loop\n",
        "sample_rng = torch.Generator(device=get_device())\n",
        "sample_rng.manual_seed(42)\n",
        "for i in range(100):\n",
        "    with torch.no_grad():\n",
        "      x,y = batchify.next_batch()\n",
        "      x = torch.tensor(x).to(get_device())\n",
        "      y = torch.tensor(y).to(get_device())\n",
        "      out, loss = model(x, y)\n",
        "      out_ = torch.argmax(out[0], axis=1)\n",
        "      print(f\"target: {decoder(list(y[0]))}, predicted: {decoder(list(out_))}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}